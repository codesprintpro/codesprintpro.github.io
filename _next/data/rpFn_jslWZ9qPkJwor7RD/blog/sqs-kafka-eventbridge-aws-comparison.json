{"pageProps":{"post":{"title":"SQS vs Kafka vs EventBridge: Choosing the Right Messaging System on AWS","description":"A senior engineer's guide to selecting between Amazon SQS, Apache Kafka on AWS, and EventBridge. Throughput benchmarks, cost breakdowns, ordering guarantees, and real production trade-offs.","date":"2025-04-02","category":"Messaging","tags":["aws","sqs","kafka","eventbridge","distributed systems","messaging","msk"],"featured":false,"affiliateSection":"aws-resources","slug":"sqs-kafka-eventbridge-aws-comparison","readingTime":"10 min read","excerpt":"Every AWS backend team eventually faces the same decision: you need asynchronous messaging. SQS is right there in the console. Your architect says you need Kafka. Someone from DevOps mentions EventBridge. Each option has…","contentHtml":"<p>Every AWS backend team eventually faces the same decision: you need asynchronous messaging. SQS is right there in the console. Your architect says you need Kafka. Someone from DevOps mentions EventBridge. Each option has a vocal fan base, and they are all wrong about different things.</p>\n<p>This article cuts through the advocacy and gives you a decision framework based on throughput characteristics, cost at scale, operational burden, and failure behavior — the things that actually matter in production.</p>\n<h2>The Three Systems in One Paragraph Each</h2>\n<p><strong>Amazon SQS</strong> is a fully managed queue service. Producers enqueue messages, consumers poll and delete them. Standard queues offer at-least-once delivery with best-effort ordering. FIFO queues offer exactly-once delivery with strict ordering within a message group, capped at 300 messages/second per queue (3000 with batching). SQS has no concept of replay — a deleted message is gone.</p>\n<p><strong>Apache Kafka on AWS (Amazon MSK)</strong> is a distributed log. Messages are appended to ordered, immutable partitions and retained for a configurable period (default 7 days). Any consumer can read from any offset at any time. Kafka decouples producer throughput from consumer lag — a slow consumer doesn't backpressure the producer. MSK Serverless removes cluster management at the cost of throughput limits and higher per-unit cost.</p>\n<p><strong>Amazon EventBridge</strong> is a serverless event bus. Producers publish events; EventBridge routes them to targets (Lambda, SQS, Kinesis, HTTP endpoints) based on content-based rules. It is optimized for event-driven architectures where routing logic is complex and throughput is modest. Maximum throughput is 10,000 events/second per bus, with no replay (unless you enable the event archive, which adds cost and latency).</p>\n<h2>Throughput Comparison</h2>\n<pre><code>System              | Sustained Throughput         | Burst Behavior\n--------------------|------------------------------|---------------------------\nSQS Standard        | Unlimited (AWS-managed)      | Auto-scales, no config\nSQS FIFO            | 300 msg/s (3000 w/ batching) | Hard limit per queue\nEventBridge         | 10,000 events/s per bus      | Soft limit, raiseable\nMSK (Kafka)         | 1 GB/s+ per broker           | Add brokers/partitions\nMSK Serverless      | 200 MB/s ingress             | Automatically scales\n</code></pre>\n<p>SQS Standard is genuinely unlimited — AWS manages the infrastructure horizontally. In practice, the bottleneck becomes your consumer fleet, not SQS itself.</p>\n<p>Kafka throughput is bounded by broker count × partition count × disk I/O. A 3-broker MSK cluster with <code>r5.2xlarge</code> instances can sustain 500–800 MB/s. Add 3 more brokers and you scale linearly. This is the key advantage: Kafka throughput is predictable and tunable.</p>\n<p>EventBridge's 10K events/s feels generous until you're doing analytics ingestion or log streaming — at which point it's a hard architectural wall.</p>\n<h2>Ordering Guarantees</h2>\n<p>This is where teams make expensive mistakes.</p>\n<p><strong>SQS Standard</strong> provides best-effort ordering. Across distributed consumers, messages can arrive out of order. For fire-and-forget notifications or task queues where order is irrelevant, this is fine.</p>\n<p><strong>SQS FIFO</strong> guarantees order within a <code>MessageGroupId</code>. If you use a single group ID, you get FIFO across the entire queue — but throughput drops to 300 msg/s. The practical pattern is to partition by entity ID: use <code>customerId</code> as the group ID to get ordered processing per customer while maintaining parallelism across customers.</p>\n<p><strong>Kafka</strong> partitions are strictly ordered. Within a partition, consumers see messages in write order, guaranteed. Across partitions, there is no global ordering — this is a fundamental property of the distributed log. Design around it: put messages that must be ordered relative to each other in the same partition using the same partition key.</p>\n<p><strong>EventBridge</strong> provides no ordering guarantees. It is designed for event routing, not ordered processing.</p>\n<h2>Cost Breakdown at Scale</h2>\n<p>Let's be concrete. Assume 100 million messages/day at 1 KB average size.</p>\n<p><strong>SQS Standard:</strong></p>\n<ul>\n<li>100M requests/day × $0.40 per million = <strong>$40/day = $1,200/month</strong></li>\n<li>Add data transfer costs if consumers are outside the same region</li>\n<li>No infrastructure to manage</li>\n</ul>\n<p><strong>SQS FIFO:</strong></p>\n<ul>\n<li>Same request pricing + $0.05 per 10K deduplication checks</li>\n<li>At 100M messages: $40 + $500 dedup = <strong>~$1,500/month</strong></li>\n</ul>\n<p><strong>EventBridge:</strong></p>\n<ul>\n<li>$1.00 per million custom events = <strong>$100/day = $3,000/month</strong></li>\n<li>Plus $0.10 per GB for event archive if replay is needed</li>\n<li>Expensive at volume — EventBridge is not designed for high-throughput streaming</li>\n</ul>\n<p><strong>MSK (Amazon Managed Kafka):</strong></p>\n<ul>\n<li>3× <code>kafka.m5.large</code> brokers: ~$0.21/hr each = <strong>$450/month</strong> cluster cost</li>\n<li>EBS storage: 100M × 1KB × 7-day retention = 700 GB × $0.10/GB = <strong>$70/month</strong></li>\n<li>MSK data transfer + data out costs</li>\n<li>Total: <strong>~$600–900/month</strong> for this workload, but flat-rate regardless of message volume</li>\n<li>Operational cost: cluster monitoring, lag alerting, schema registry, Kafka client config</li>\n</ul>\n<p>The crossover point is typically around 50–100 million messages/day where MSK becomes cheaper than SQS, assuming you have the engineering capacity to operate it.</p>\n<p><strong>MSK Serverless:</strong></p>\n<ul>\n<li>$0.75/VCU-hour + $0.10/GB-hour storage — often 2–3× the cost of provisioned MSK at sustained throughput, but zero operational overhead.</li>\n</ul>\n<h2>Multi-Region Support</h2>\n<p><strong>SQS:</strong> Single-region by default. For multi-region, you replicate at the application layer — consumer reads from us-east-1 queue, writes to eu-west-1 queue. No native cross-region replication.</p>\n<p><strong>EventBridge:</strong> Global buses support cross-account event routing. EventBridge Event Bus can forward events to buses in other regions via rules. This is the simplest cross-region event routing available in AWS, and it's first-class.</p>\n<p><strong>Kafka/MSK:</strong> MSK Replication (MirrorMaker 2) replicates topics across clusters in different regions with configurable lag. Active-active multi-region Kafka is operationally complex — topic offsets diverge and merging is non-trivial. Most teams do active-passive: one region produces, MirrorMaker2 replicates to the DR region, consumers fail over to the replica cluster manually.</p>\n<pre><code>Multi-Region Kafka Architecture (Active-Passive):\n\nus-east-1                          eu-west-1\n┌─────────────────┐               ┌─────────────────┐\n│  MSK Cluster A  │──MirrorMaker──│  MSK Cluster B  │\n│  (Primary)      │               │  (Replica)       │\n│                 │               │                  │\n│  Producers ──►  │               │  ◄── Failover    │\n│  Consumers ──►  │               │      Consumers   │\n└─────────────────┘               └─────────────────┘\n</code></pre>\n<h2>Retry Behavior and Dead Letter Queues</h2>\n<p><strong>SQS:</strong> Messages have a configurable <code>VisibilityTimeout</code>. When a consumer reads a message, it becomes invisible to other consumers. If the consumer doesn't delete it before the timeout, it becomes visible again and another consumer picks it up. After <code>maxReceiveCount</code> failures, SQS moves the message to a Dead Letter Queue (DLQ). This is fully managed and requires zero code.</p>\n<pre><code>SQS Retry Flow:\nMessage → Consumer → (processing fails) → Visibility timeout expires\n→ Message re-visible → Re-consumed → ... → maxReceiveCount reached\n→ Message moved to DLQ\n</code></pre>\n<p><strong>EventBridge:</strong> Failed deliveries are retried with exponential backoff for up to 24 hours. If all retries fail, the event is sent to a DLQ (SQS) or dropped. The retry window is configurable but you have limited visibility into retry state.</p>\n<p><strong>Kafka:</strong> Kafka has no built-in retry concept at the broker level. Retry is the consumer's responsibility. The production pattern is retry topics:</p>\n<pre><code>Retry Topic Pattern:\norders-topic → Consumer (fails) → orders-retry-1 (wait 30s)\n→ Consumer (fails) → orders-retry-2 (wait 5min)\n→ Consumer (fails) → orders-retry-3 (wait 30min)\n→ Consumer (fails) → orders-dlq\n</code></pre>\n<p>This requires explicit implementation but gives you complete control over retry semantics, delay scheduling, and DLQ routing.</p>\n<h2>Scaling Strategy</h2>\n<p><strong>SQS:</strong> Consumer scaling is driven by queue depth. AWS SQS → CloudWatch → Auto Scaling Group scales consumer EC2 instances or Lambda concurrency based on <code>ApproximateNumberOfMessagesVisible</code>. This is mature and well-understood.</p>\n<p><strong>EventBridge:</strong> Targets scale automatically (Lambda, Fargate). You don't manage consumers. This is the point — EventBridge handles fan-out and routing so you don't have to.</p>\n<p><strong>Kafka:</strong> Consumer scaling is partition-bound. You cannot have more active consumers in a consumer group than partitions in a topic. Plan partition count at topic creation: <code>partitions = max_expected_consumers × headroom_factor</code>. Kafka's <code>kafka.admin.client</code> lets you expand partition count after creation, but redistributing partitions causes a brief rebalance. Pre-partition aggressively.</p>\n<h2>Operational Overhead</h2>\n<p>This is where honest conversations get uncomfortable.</p>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>SQS</th>\n<th>EventBridge</th>\n<th>MSK</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cluster provisioning</td>\n<td>None</td>\n<td>None</td>\n<td>Broker sizing, AZ config</td>\n</tr>\n<tr>\n<td>Schema management</td>\n<td>None</td>\n<td>JSON schema registry (optional)</td>\n<td>Confluent Schema Registry or Glue</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Basic CloudWatch</td>\n<td>Basic CloudWatch</td>\n<td>Custom dashboards, consumer lag</td>\n</tr>\n<tr>\n<td>Security</td>\n<td>IAM, VPC</td>\n<td>IAM, resource policies</td>\n<td>mTLS, SASL, ACLs</td>\n</tr>\n<tr>\n<td>Upgrade management</td>\n<td>Automatic</td>\n<td>Automatic</td>\n<td>MSK broker version upgrades</td>\n</tr>\n<tr>\n<td>Consumer lag tracking</td>\n<td>Queue depth metrics</td>\n<td>None</td>\n<td>Kafka consumer group offsets</td>\n</tr>\n</tbody>\n</table>\n<p>MSK is the Kafka product of choice on AWS, but \"managed\" is relative. You still provision brokers, choose instance types, manage broker storage, configure retention, set up Schema Registry, build consumer lag dashboards, and handle rebalance storms.</p>\n<p>MSK Serverless offloads most of this at a cost premium. For teams without dedicated platform engineering, MSK Serverless or Confluent Cloud are worth the price.</p>\n<h2>Latency Characteristics</h2>\n<p><strong>SQS</strong> short-polling returns immediately (empty or not). Long-polling waits up to 20 seconds, which reduces empty receives and cost. End-to-end latency (produce → consume) is typically 50–200ms under normal load, with occasional spikes to seconds under high retry load.</p>\n<p><strong>EventBridge</strong> typically delivers in under 500ms. For Lambda targets, add cold start time. Not appropriate for sub-100ms requirements.</p>\n<p><strong>Kafka</strong> end-to-end latency depends on producer acknowledgment settings:</p>\n<ul>\n<li><code>acks=0</code>: fire and forget, ~5ms produce latency, data loss risk</li>\n<li><code>acks=1</code>: leader acknowledges, ~10–20ms</li>\n<li><code>acks=all</code>: all ISR replicas acknowledge, 20–50ms typically</li>\n</ul>\n<p>Consumer-side, with <code>fetch.min.bytes=1</code> and <code>fetch.max.wait.ms=500</code> defaults, a new message is consumed within 1–500ms after it's committed. For low-latency streaming (&#x3C; 100ms end-to-end), tune <code>fetch.max.wait.ms=0</code>.</p>\n<h2>When Kafka is Overkill</h2>\n<p>Use SQS when:</p>\n<ul>\n<li>You need a simple task queue with retry and DLQ</li>\n<li>Throughput is under 10K messages/second</li>\n<li>You don't need message replay</li>\n<li>Your team has no Kafka operational experience</li>\n<li>You're a startup without a platform team</li>\n</ul>\n<p>Kafka's power comes from replayability, strict ordering, and high throughput. If your use case doesn't need these properties, you're paying operational overhead for nothing.</p>\n<h2>When SQS Fails at Scale</h2>\n<p>SQS FIFO breaks at ordering + throughput intersection. At 300 msg/s per queue (3,000 with batching), you hit the hard limit. The workaround is sharding: deploy 10 FIFO queues, partition by entity ID, route producers accordingly. This works, but you've now built a partition routing layer — which is exactly what Kafka does natively.</p>\n<p>SQS Standard's lack of replay means you cannot re-process a stream of events. If your downstream system has a bug that corrupts two hours of data, you cannot replay from two hours ago. You need a separate audit log — at which point, you should have just used Kafka.</p>\n<h2>Real Production Case Study</h2>\n<p>At a fintech company processing 5 million payment events per day, the team started with SQS FIFO for per-customer ordered processing. After 18 months:</p>\n<ul>\n<li>FIFO throughput limit triggered scaling issues during month-end batch processing</li>\n<li>Zero replay capability meant a buggy consumer silently dropped 30,000 events before detection — requiring a full re-run from an S3 audit backup</li>\n<li>Adding a new downstream consumer required modifying the producer to enqueue to a second queue</li>\n</ul>\n<p>Migration to MSK:</p>\n<ul>\n<li>Topic: <code>payment-events</code> with 48 partitions (keyed by <code>customerId</code>)</li>\n<li>Consumer groups per downstream system — each independently maintains its offset</li>\n<li>7-day retention enabled replay of any incident window</li>\n<li>MirrorMaker2 replicates to a DR region</li>\n</ul>\n<p>Operational cost increased by $800/month. Engineering productivity increased significantly — onboarding new consumers went from code changes to a new consumer group.</p>\n<h2>Decision Framework</h2>\n<pre><code>START\n│\n├── Do you need message replay?\n│   └── YES → Kafka (MSK)\n│\n├── Do you need cross-service event routing with content-based rules?\n│   └── YES → EventBridge\n│\n├── Do you need strict ordering + throughput > 3000 msg/s?\n│   └── YES → Kafka (MSK)\n│\n├── Do you have a platform team to operate Kafka?\n│   └── NO + need Kafka features → MSK Serverless or Confluent Cloud\n│\n└── Simple task queue, retry + DLQ, &#x3C; 3000 ordered msg/s?\n    └── YES → SQS (FIFO if ordering matters, Standard otherwise)\n</code></pre>\n<p>The default should be SQS. Introduce Kafka when you hit the specific limitations that Kafka solves. EventBridge shines in event-driven microservice architectures where routing logic is the primary challenge.</p>\n","tableOfContents":[{"id":"the-three-systems-in-one-paragraph-each","text":"The Three Systems in One Paragraph Each","level":2},{"id":"throughput-comparison","text":"Throughput Comparison","level":2},{"id":"ordering-guarantees","text":"Ordering Guarantees","level":2},{"id":"cost-breakdown-at-scale","text":"Cost Breakdown at Scale","level":2},{"id":"multi-region-support","text":"Multi-Region Support","level":2},{"id":"retry-behavior-and-dead-letter-queues","text":"Retry Behavior and Dead Letter Queues","level":2},{"id":"scaling-strategy","text":"Scaling Strategy","level":2},{"id":"operational-overhead","text":"Operational Overhead","level":2},{"id":"latency-characteristics","text":"Latency Characteristics","level":2},{"id":"when-kafka-is-overkill","text":"When Kafka is Overkill","level":2},{"id":"when-sqs-fails-at-scale","text":"When SQS Fails at Scale","level":2},{"id":"real-production-case-study","text":"Real Production Case Study","level":2},{"id":"decision-framework","text":"Decision Framework","level":2}]},"relatedPosts":[{"title":"Kafka Exactly-Once Semantics: Myth vs Production Reality","description":"What Kafka's exactly-once guarantee actually covers, where duplicates still happen in practice, and how to design genuinely idempotent consumers with Spring Kafka. Real production mistakes and their fixes.","date":"2025-04-20","category":"Messaging","tags":["kafka","exactly-once","spring kafka","distributed systems","transactions","java"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"kafka-exactly-once-semantics","readingTime":"9 min read","excerpt":"Kafka 0.11 introduced exactly-once semantics (EOS), and every architecture diagram since then has confidently placed a checkbox next to \"exactly once delivery.\" In practice, most teams deploying Kafka with EOS still see …"},{"title":"Kafka Internals Deep Dive: Partitions, Offsets, and Consumer Groups","description":"Understand how Apache Kafka achieves high throughput through log-based storage, how offsets enable reliable consumption, and how consumer groups scale processing horizontally.","date":"2025-01-15","category":"Messaging","tags":["kafka","distributed systems","streaming","java"],"featured":true,"affiliateSection":"distributed-systems-books","slug":"kafka-internals-deep-dive","readingTime":"10 min read","excerpt":"Apache Kafka is the de facto standard for event streaming in distributed systems, but most developers treat it as a black box — a durable message queue with a fancy name. Understanding Kafka's internals unlocks its true …"}]},"__N_SSG":true}