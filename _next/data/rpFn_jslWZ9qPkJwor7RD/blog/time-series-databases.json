{"pageProps":{"post":{"title":"Time-Series Databases: InfluxDB vs TimescaleDB vs Prometheus","description":"Choose the right time-series database for metrics, IoT, and observability workloads. Deep comparison of InfluxDB, TimescaleDB, and Prometheus with retention policies, downsampling, and query patterns.","date":"2025-03-29","category":"Databases","tags":["time-series","influxdb","timescaledb","prometheus","observability","iot","databases"],"featured":false,"affiliateSection":"database-resources","slug":"time-series-databases","readingTime":"11 min read","excerpt":"Time-series data is fundamentally different from general-purpose data: it arrives in time order, is queried by time ranges, has predictable decay in value, and has write patterns that overwhelm traditional relational dat…","contentHtml":"<p>Time-series data is fundamentally different from general-purpose data: it arrives in time order, is queried by time ranges, has predictable decay in value, and has write patterns that overwhelm traditional relational databases. InfluxDB, TimescaleDB, and Prometheus each solve this problem differently. Picking the wrong one means data loss, query timeouts, or a rewrite 6 months later.</p>\n<h2>Why Regular Databases Fail for Time-Series</h2>\n<p>Before looking at the solutions, it is worth understanding exactly why a general-purpose database like PostgreSQL struggles here. The problem is not just volume — it is the combination of write velocity, append-only patterns, and the fact that most data becomes cold and rarely queried after a short window.</p>\n<pre><code>IoT sensor data: 10,000 sensors × 1 reading/second = 10,000 inserts/second\n\nPostgreSQL (without time-series extension):\n  - B-tree index insertion: O(log n) per row\n  - At 10K/sec: 1 billion rows/day = 864GB raw data\n  - Index grows unbounded: 200GB+ index for 864GB data\n  - Query: \"average temperature last hour\" → full index scan of 36M rows\n  - Write amplification: each insert touches multiple B-tree pages\n\nVacuum and bloat:\n  - Sensor readings are append-only → vacuum can't reclaim space well\n  - Table bloat after months = 3× actual data size\n\nTimescaleDB solves this by:\n  - Chunking data by time window (1-day chunks by default)\n  - Old chunks become immutable → no vacuum overhead\n  - Query planner prunes chunks by time → only scan relevant chunks\n  - Chunk-level compression: 90-95% size reduction\n</code></pre>\n<p>The core insight is that time-series data has a natural expiry: you care deeply about the last hour, somewhat about the last week, and almost never about data from 18 months ago. Time-series databases exploit this by automatically tiering, compressing, and eventually dropping old data — something a general-purpose database makes you implement yourself.</p>\n<h2>InfluxDB: Purpose-Built TSDB</h2>\n<p>InfluxDB is designed exclusively for time-series data with its own query language (Flux) and data model.</p>\n<p>InfluxDB's data model separates metadata (tags) from measurements (fields), which is a deliberate design choice: tags are indexed and meant for filtering and grouping, while fields are just stored values. If you mistakenly put a high-cardinality value (like a user ID) in a tag, InfluxDB's index grows unbounded and performance degrades dramatically. Get this distinction right and InfluxDB is extremely fast.</p>\n<pre><code>Data model:\n  Measurement: cpu_usage          (like a table)\n  Tags: host=web-1, region=us-east (indexed metadata — filtering)\n  Fields: cpu_percent=89.2, load=1.23 (values — not indexed)\n  Timestamp: 2025-03-29T10:00:00Z (nanosecond precision)\n\nWritten as line protocol:\n  cpu_usage,host=web-1,region=us-east cpu_percent=89.2,load=1.23 1711699200000000000\n</code></pre>\n<p>The Python client below shows the two most important write patterns: single-point writes for low-frequency data, and batch writes for high-frequency sensors. Always prefer batch writes for anything above a few points per second — sending one HTTP request per data point at high volume is the fastest way to overwhelm both the client and the server.</p>\n<pre><code class=\"language-python\">from influxdb_client import InfluxDBClient, Point\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nfrom datetime import datetime, timedelta\n\nclient = InfluxDBClient(\n    url=\"http://localhost:8086\",\n    token=\"your-admin-token\",\n    org=\"your-org\"\n)\n\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\n\n# Write a point\npoint = (\n    Point(\"cpu_usage\")\n    .tag(\"host\", \"web-1\")\n    .tag(\"region\", \"us-east\")\n    .field(\"cpu_percent\", 89.2)\n    .field(\"load_1m\", 1.23)\n    .time(datetime.utcnow())\n)\nwrite_api.write(bucket=\"metrics\", record=point)\n\n# Batch write (efficient for high-frequency data)\npoints = [\n    Point(\"cpu_usage\")\n        .tag(\"host\", f\"web-{i}\")\n        .field(\"cpu_percent\", 40 + i * 2.5)\n        .time(datetime.utcnow())\n    for i in range(100)\n]\nwrite_api.write(bucket=\"metrics\", record=points)\n\n# Query with Flux\nquery_api = client.query_api()\n\n# Average CPU per host, last 1 hour\nquery = '''\nfrom(bucket: \"metrics\")\n  |> range(start: -1h)\n  |> filter(fn: (r) => r._measurement == \"cpu_usage\")\n  |> filter(fn: (r) => r._field == \"cpu_percent\")\n  |> group(columns: [\"host\"])\n  |> mean()\n  |> sort(columns: [\"_value\"], desc: true)\n'''\n\nresult = query_api.query(query=query, org=\"your-org\")\nfor table in result:\n    for record in table.records:\n        print(f\"Host: {record['host']}, Avg CPU: {record['_value']:.1f}%\")\n\n# Downsampled aggregation: 5-minute averages over last 7 days\nquery = '''\nfrom(bucket: \"metrics\")\n  |> range(start: -7d)\n  |> filter(fn: (r) => r._measurement == \"cpu_usage\")\n  |> filter(fn: (r) => r._field == \"cpu_percent\")\n  |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)\n'''\n</code></pre>\n<p>Retention policies and automatic downsampling are where InfluxDB really shines for IoT workloads. The tier structure below keeps raw data for just 7 days, then retains progressively coarser aggregates for months or years. This gives you fast recent queries on raw data and affordable long-term trend analysis on pre-aggregated data — without any manual data management.</p>\n<pre><code>InfluxDB Retention Policies:\n  Bucket: metrics_raw        → retention: 7 days   (high resolution)\n  Bucket: metrics_hourly     → retention: 90 days  (1-hour aggregates)\n  Bucket: metrics_daily      → retention: 2 years  (daily aggregates)\n\nInfluxDB Task (continuous downsampling):\n  option task = {name: \"Downsample to hourly\", every: 1h}\n\n  from(bucket: \"metrics_raw\")\n    |> range(start: -1h)\n    |> filter(fn: (r) => r._measurement == \"cpu_usage\")\n    |> aggregateWindow(every: 1h, fn: mean)\n    |> to(bucket: \"metrics_hourly\")\n</code></pre>\n<h2>TimescaleDB: PostgreSQL for Time-Series</h2>\n<p>TimescaleDB extends PostgreSQL with time-series superpowers while remaining fully PostgreSQL-compatible.</p>\n<p>The biggest advantage of TimescaleDB is not performance — it is familiarity. If your team already knows SQL, already operates PostgreSQL, and already has tooling around it, TimescaleDB adds time-series capability without introducing a new database engine to learn, operate, and monitor.</p>\n<p>The setup process below is intentionally familiar: you create a normal PostgreSQL table, then call <code>create_hypertable</code> to activate TimescaleDB's automatic partitioning. Existing applications that query this table via standard SQL continue to work unchanged — TimescaleDB is transparent to the query layer.</p>\n<pre><code class=\"language-sql\">-- Enable extension\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Create a regular table first\nCREATE TABLE sensor_data (\n    time        TIMESTAMPTZ NOT NULL,\n    sensor_id   VARCHAR(50) NOT NULL,\n    location    VARCHAR(100),\n    temperature DOUBLE PRECISION,\n    humidity    DOUBLE PRECISION,\n    pressure    DOUBLE PRECISION\n);\n\n-- Convert to hypertable (TimescaleDB magic)\n-- Creates automatic partitioning by time (7-day chunks by default)\nSELECT create_hypertable('sensor_data', 'time');\n\n-- Optional: partition by space dimension too (for IoT: partition by sensor_id)\nSELECT create_hypertable('sensor_data', 'time',\n    partitioning_column => 'sensor_id',\n    number_partitions => 8\n);\n\n-- Indexes (TimescaleDB creates them per chunk → much more efficient)\nCREATE INDEX idx_sensor_data_sensor_time ON sensor_data (sensor_id, time DESC);\n</code></pre>\n<p>TimescaleDB's <code>time_bucket</code> function is the key abstraction for time-series aggregation. It divides the time axis into equal windows (5 minutes, 1 hour, 1 day) and lets you aggregate within each window using standard SQL aggregates. Gap filling is equally valuable — real sensor networks have missing data, and <code>LOCF</code> (Last Observation Carry Forward) lets you produce clean, uniform time series for dashboards without preprocessing the data in your application.</p>\n<pre><code class=\"language-sql\">-- Queries: full PostgreSQL SQL + time-series functions\n-- Last hour of readings for a sensor\nSELECT time, temperature, humidity\nFROM sensor_data\nWHERE sensor_id = 'sensor-42'\n  AND time > NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n\n-- TimescaleDB time_bucket: aggregate by time window\nSELECT\n    time_bucket('5 minutes', time) AS bucket,\n    sensor_id,\n    AVG(temperature)   AS avg_temp,\n    MIN(temperature)   AS min_temp,\n    MAX(temperature)   AS max_temp,\n    COUNT(*)           AS readings\nFROM sensor_data\nWHERE time > NOW() - INTERVAL '24 hours'\nGROUP BY bucket, sensor_id\nORDER BY bucket DESC, sensor_id;\n\n-- Gap filling: fill missing intervals with NULL or forward-fill\nSELECT\n    time_bucket_gapfill('5 minutes', time) AS bucket,\n    sensor_id,\n    LOCF(AVG(temperature)) AS temperature  -- Last observation carry forward\nFROM sensor_data\nWHERE time BETWEEN NOW() - INTERVAL '24h' AND NOW()\nGROUP BY bucket, sensor_id\nORDER BY bucket;\n</code></pre>\n<p>Compression and continuous aggregates are the features that make TimescaleDB viable for long-running IoT deployments. The SQL below enables columnar compression on chunks older than 7 days — yielding 90%+ storage reduction for typical sensor data — and creates a materialized view that is automatically refreshed. Queries against <code>sensor_hourly</code> return pre-aggregated data instantly, rather than scanning millions of raw readings.</p>\n<pre><code class=\"language-sql\">-- Compression (90%+ reduction for IoT data)\n-- Enable compression with 7-day delay (keep last 7 days uncompressed for fast inserts)\nALTER TABLE sensor_data SET (\n    timescaledb.compress,\n    timescaledb.compress_orderby = 'time DESC',\n    timescaledb.compress_segmentby = 'sensor_id'\n);\n\nSELECT add_compression_policy('sensor_data', INTERVAL '7 days');\n\n-- Automatic retention policy\nSELECT add_retention_policy('sensor_data', INTERVAL '1 year');\n\n-- Continuous aggregates (materialized, automatically updated)\nCREATE MATERIALIZED VIEW sensor_hourly\nWITH (timescaledb.continuous) AS\nSELECT\n    time_bucket('1 hour', time) AS hour,\n    sensor_id,\n    AVG(temperature) AS avg_temp,\n    MIN(temperature) AS min_temp,\n    MAX(temperature) AS max_temp\nFROM sensor_data\nGROUP BY hour, sensor_id\nWITH NO DATA;\n\n-- Policy: refresh aggregate every hour for last 3 hours\nSELECT add_continuous_aggregate_policy('sensor_hourly',\n    start_offset => INTERVAL '3 hours',\n    end_offset   => INTERVAL '1 hour',\n    schedule_interval => INTERVAL '1 hour'\n);\n\n-- Query the materialized aggregate (fast, pre-computed)\nSELECT * FROM sensor_hourly\nWHERE sensor_id = 'sensor-42'\n  AND hour > NOW() - INTERVAL '7 days'\nORDER BY hour DESC;\n</code></pre>\n<h2>Prometheus: Metrics-First</h2>\n<p>Prometheus is a pull-based metrics system — services expose metrics, Prometheus scrapes them.</p>\n<p>Prometheus works on an inverted model from the other two databases: instead of your application pushing data to Prometheus, Prometheus reaches out and pulls from your services on a schedule. This pull model makes it easy to see when a service is down (it stops being scrapeable), and it keeps the metrics pipeline decoupled from application code — your service just needs to expose an HTTP endpoint.</p>\n<p>The configuration below sets up Prometheus to scrape three Spring Boot microservices and a node exporter for server-level metrics. Services must expose a <code>/actuator/prometheus</code> endpoint, which Spring Boot's Micrometer integration provides automatically when you add the <code>micrometer-registry-prometheus</code> dependency.</p>\n<pre><code class=\"language-yaml\"># prometheus.yml\nglobal:\n  scrape_interval: 15s      # How often to scrape\n\nscrape_configs:\n  - job_name: spring-boot-services\n    metrics_path: /actuator/prometheus\n    static_configs:\n      - targets:\n        - order-service:8080\n        - payment-service:8080\n        - inventory-service:8080\n\n  - job_name: node-exporter    # Server metrics (CPU, memory, disk)\n    static_configs:\n      - targets: ['node-exporter:9100']\n\n# Retention\nstorage:\n  tsdb:\n    retention.time: 15d\n    retention.size: 50GB\n\n# For long-term storage: Thanos or VictoriaMetrics sidecar\n</code></pre>\n<p>Beyond the built-in HTTP metrics that Micrometer auto-instruments (request rate, latency histograms, JVM stats), you often want custom business metrics. The three metric types below cover the most common use cases: counters for things that only go up (orders created), histograms for distributions (order value), and gauges for current state (pending order count). Defining these in a dedicated metrics class keeps instrumentation organized and testable.</p>\n<pre><code class=\"language-java\">// Spring Boot Prometheus metrics (auto-instrumented)\n// Add dependency: micrometer-registry-prometheus\n\n// Custom business metrics\n@Service\npublic class OrderMetrics {\n\n    @Autowired\n    private MeterRegistry registry;\n\n    // Counter: total orders created\n    private Counter ordersCreated = Counter.builder(\"orders.created.total\")\n        .tag(\"payment_method\", \"credit_card\")\n        .description(\"Total orders created\")\n        .register(registry);\n\n    // Histogram: order value distribution\n    private DistributionSummary orderValue = DistributionSummary\n        .builder(\"orders.value.cents\")\n        .scale(0.01)  // Convert cents to dollars for display\n        .publishPercentiles(0.5, 0.95, 0.99)\n        .register(registry);\n\n    // Gauge: current pending orders\n    private AtomicInteger pendingOrders = registry.gauge(\n        \"orders.pending.current\", new AtomicInteger(0)\n    );\n}\n</code></pre>\n<p>PromQL is the query language you use to turn raw metric samples into actionable signals in Grafana dashboards and alerting rules. The three queries below cover the most important patterns: computing a rate from a counter, calculating a high-percentile latency from a histogram, and expressing an alert threshold as a ratio. The <code>rate()</code> function is the workhorse of PromQL — it handles counter resets (service restarts) automatically and computes a per-second rate over the specified window.</p>\n<pre><code class=\"language-promql\"># PromQL queries (used in Grafana dashboards)\n\n# Request rate per second (5-minute window)\nrate(http_server_requests_seconds_count[5m])\n\n# P99 latency by service\nhistogram_quantile(0.99,\n  sum by (service, le) (\n    rate(http_server_requests_seconds_bucket[5m])\n  )\n)\n\n# Alert: error rate > 5%\n(\n  sum(rate(http_server_requests_seconds_count{status=~\"5..\"}[5m]))\n  /\n  sum(rate(http_server_requests_seconds_count[5m]))\n) > 0.05\n</code></pre>\n<h2>Comparison and Decision Guide</h2>\n<table>\n<thead>\n<tr>\n<th>Factor</th>\n<th>InfluxDB</th>\n<th>TimescaleDB</th>\n<th>Prometheus</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Query language</strong></td>\n<td>Flux (custom)</td>\n<td>SQL</td>\n<td>PromQL</td>\n</tr>\n<tr>\n<td><strong>Write throughput</strong></td>\n<td>Very high (500K/sec)</td>\n<td>High (100K/sec)</td>\n<td>Low (pull-based)</td>\n</tr>\n<tr>\n<td><strong>Cardinality limit</strong></td>\n<td>Medium (millions)</td>\n<td>High (billions)</td>\n<td>Low (millions)</td>\n</tr>\n<tr>\n<td><strong>Long-term storage</strong></td>\n<td>Native buckets</td>\n<td>Native + compression</td>\n<td>Needs Thanos/VictoriaMetrics</td>\n</tr>\n<tr>\n<td><strong>Joins/analytics</strong></td>\n<td>Limited</td>\n<td>Full SQL</td>\n<td>No</td>\n</tr>\n<tr>\n<td><strong>Use case</strong></td>\n<td>IoT, telemetry</td>\n<td>General time-series</td>\n<td>Service metrics</td>\n</tr>\n<tr>\n<td><strong>Operational complexity</strong></td>\n<td>Medium</td>\n<td>Low (just PostgreSQL)</td>\n<td>Low</td>\n</tr>\n</tbody>\n</table>\n<p>The comparison above shows that no single tool is best across all dimensions. Use the decision guide below to match your specific requirements to the right choice, keeping in mind that many production setups combine two of these tools — typically Prometheus for short-term alerting and TimescaleDB or InfluxDB for long-term trend analysis.</p>\n<pre><code>Choose InfluxDB when:\n  - IoT data at high velocity (100K+ writes/sec)\n  - Need built-in downsampling and retention policies\n  - Time-series is your only data model\n\nChoose TimescaleDB when:\n  - Already using PostgreSQL (zero new infrastructure)\n  - Need SQL joins (time-series + relational data together)\n  - Want compression + retention + SQL in one system\n  - Application data and metrics in same database\n\nChoose Prometheus when:\n  - Monitoring and alerting is the primary use case\n  - Service metrics (latency, error rate, saturation)\n  - Grafana dashboards\n  - Already in Kubernetes ecosystem\n  - Short retention (15-30 days) is acceptable\n\nCommon architecture:\n  Prometheus (short-term metrics) + Thanos (long-term storage)\n  OR\n  Prometheus (alerting/dashboards) + TimescaleDB (long-term analytics)\n</code></pre>\n<p>TimescaleDB is the pragmatic choice for most teams: if you already have PostgreSQL, it's zero additional infrastructure, the SQL compatibility means no new query language to learn, and the performance improvements over vanilla PostgreSQL are substantial. For high-velocity IoT data or when you need a purpose-built TSDB, InfluxDB shines. For monitoring and alerting in a Kubernetes environment, Prometheus is the de facto standard.</p>\n","tableOfContents":[{"id":"why-regular-databases-fail-for-time-series","text":"Why Regular Databases Fail for Time-Series","level":2},{"id":"influxdb-purpose-built-tsdb","text":"InfluxDB: Purpose-Built TSDB","level":2},{"id":"timescaledb-postgresql-for-time-series","text":"TimescaleDB: PostgreSQL for Time-Series","level":2},{"id":"prometheus-metrics-first","text":"Prometheus: Metrics-First","level":2},{"id":"comparison-and-decision-guide","text":"Comparison and Decision Guide","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring — every node is equal, there's no primary, and data placement is determined by partit…"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you…"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible — rollback a bad deploy and your code is back to the previous version. Database schema changes are st…"}]},"__N_SSG":true}