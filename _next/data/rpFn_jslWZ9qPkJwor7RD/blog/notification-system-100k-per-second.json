{"pageProps":{"post":{"title":"Designing a High-Throughput Notification System for 100K Events per Second","description":"End-to-end architecture for a notification system handling 100,000 events per second: capacity planning, Kafka partition sizing, fan-out strategy, rate limiting, idempotency, and incident simulation.","date":"2025-05-10","category":"System Design","tags":["system design","notifications","kafka","throughput","capacity planning","distributed systems"],"featured":false,"affiliateSection":"system-design-courses","slug":"notification-system-100k-per-second","readingTime":"10 min read","excerpt":"100,000 events per second is not a notification system problem — it's a data pipeline problem that happens to produce notifications. Teams that approach it as a features problem (\"we just need push, email, and SMS\") buil…","contentHtml":"<p>100,000 events per second is not a notification system problem — it's a data pipeline problem that happens to produce notifications. Teams that approach it as a features problem (\"we just need push, email, and SMS\") build systems that collapse under load. The engineering challenge is the fan-out at scale: one event triggers notifications to potentially thousands of users, each notification routed through different channels, rate-limited per user, deduplicated, and delivered with retry guarantees.</p>\n<h2>Functional and Non-Functional Requirements</h2>\n<p><strong>Functional:</strong></p>\n<ul>\n<li>Ingest raw events from producer services (user actions, system events)</li>\n<li>Enrich events with user preferences and notification templates</li>\n<li>Route to appropriate channels: push (FCM/APNs), email, SMS, in-app</li>\n<li>Respect per-user preferences and quiet hours</li>\n<li>Track delivery status per notification</li>\n</ul>\n<p><strong>Non-Functional:</strong></p>\n<ul>\n<li>Ingest throughput: 100,000 events/second (peak)</li>\n<li>Delivery latency: &#x3C; 5 seconds end-to-end for push notifications</li>\n<li>Delivery latency: &#x3C; 60 seconds for email/SMS</li>\n<li>Availability: 99.9% (&#x3C; 8.7 hours downtime/year)</li>\n<li>At-least-once delivery with idempotent consumers</li>\n<li>Rate limiting: max 10 push notifications/user/hour</li>\n<li>Multi-region: active-active for ingestion, active-passive for delivery</li>\n</ul>\n<h2>Throughput Calculation and Capacity Planning</h2>\n<p>Raw event rate: 100,000 events/second</p>\n<p>Fan-out ratio: on average, each event triggers notifications for 5 users (social platform: a viral post triggers notifications to followers). Burst: 50× fan-out (celebrity post, flash sale).</p>\n<pre><code>Peak notification volume:\n100,000 events/s × 50 fan-out = 5,000,000 notifications/second (peak burst)\n100,000 events/s × 5 fan-out  = 500,000 notifications/second (average)\n\nChannel distribution:\n- Push: 70% → 350,000 push/second\n- In-app: 20% → 100,000 in-app/second\n- Email: 8%  → 40,000 email/second\n- SMS: 2%    → 10,000 SMS/second\n</code></pre>\n<p><strong>Storage sizing:</strong></p>\n<ul>\n<li>Each notification record: ~2KB</li>\n<li>Retention: 30 days</li>\n<li>Volume: 500,000/s × 86,400s × 30 days × 2KB = <strong>2.59 TB/day</strong></li>\n</ul>\n<p>This is a write-heavy storage problem. Cassandra or DynamoDB, not PostgreSQL.</p>\n<h2>Kafka Partition Planning</h2>\n<p>Kafka partitions determine parallelism. Rule: partition count ≥ peak consumer count × 1.5.</p>\n<p>For the event ingestion topic (<code>raw-events</code>):</p>\n<pre><code>Target throughput: 100,000 events/second × 1KB avg = 100 MB/s\nKafka broker write throughput: ~200 MB/s per broker (practical limit)\nMinimum brokers: 100/200 = 0.5 → use 3 brokers for HA\n\nTarget consumer parallelism: 200 consumer threads\nPartition count: 200 × 1.5 = 300 partitions\n</code></pre>\n<p>For the fan-out output topic (<code>notifications-to-send</code>):</p>\n<pre><code>Peak output: 5,000,000 notifications/second × 500B = 2,500 MB/s\nBrokers needed: 2,500/200 = 12.5 → 15 brokers (safety margin)\nPartitions: 1,500 (15 brokers × 100 partitions/broker)\n</code></pre>\n<pre><code>Topic Configuration:\nraw-events:\n  partitions: 300\n  replication-factor: 3\n  min.insync.replicas: 2\n  retention.ms: 3600000  # 1 hour (events are processed quickly)\n\nnotifications-to-send:\n  partitions: 1500\n  replication-factor: 3\n  retention.ms: 86400000  # 24 hours (for replay on downstream failure)\n\nnotification-status:\n  partitions: 300\n  replication-factor: 3\n  retention.ms: 604800000  # 7 days\n</code></pre>\n<h2>System Architecture</h2>\n<pre><code>High-Throughput Notification Architecture:\n\nProducer Services (100K events/s)\n[Order Service] [Social Service] [Marketing Service]\n        │              │                │\n        └──────────────┴────────────────┘\n                       │\n                       ▼ (Kafka, 100K msg/s)\n              ┌─────────────────┐\n              │   raw-events    │  300 partitions\n              │   topic         │\n              └────────┬────────┘\n                       │\n                       ▼\n          ┌────────────────────────┐\n          │   Event Processor      │  200 instances\n          │   - Validate           │  (Kafka consumer group)\n          │   - Deduplicate        │\n          │   - Enrich with        │\n          │     user prefs         │\n          └────────┬───────────────┘\n                   │ Fan-out (1 event → N notifications)\n                   ▼ (Kafka, 500K-5M msg/s peak)\n        ┌──────────────────────┐\n        │ notifications-to-    │  1500 partitions\n        │ send topic           │\n        └───┬──────────┬───────┘\n            │          │\n    ┌───────┘          └──────────┐\n    ▼                             ▼\n┌─────────────┐           ┌─────────────┐\n│  Push       │           │  Email/SMS  │\n│  Dispatcher │           │  Dispatcher │\n│  (FCM/APNs) │           │  (SES/SNS)  │\n│  500 inst   │           │  100 inst   │\n└──────┬──────┘           └──────┬──────┘\n       │                         │\n       ▼                         ▼\n┌──────────────────────────────────────┐\n│   Notification Status Store          │\n│   (Cassandra/DynamoDB)               │\n│   + notification-status Kafka topic  │\n└──────────────────────────────────────┘\n</code></pre>\n<h2>Database Schema</h2>\n<pre><code class=\"language-sql\">-- Cassandra schema for notification storage\n-- Partition key = user_id for co-located user notification history\n-- Clustering key = created_at DESC for reverse-chronological reads\n\nCREATE TABLE notifications (\n    user_id         UUID,\n    notification_id UUID,\n    type            TEXT,          -- push | email | sms | in_app\n    title           TEXT,\n    body            TEXT,\n    status          TEXT,          -- pending | sent | delivered | failed\n    channel_msg_id  TEXT,          -- FCM message_id, SES message_id, etc.\n    idempotency_key TEXT,\n    metadata        MAP&#x3C;TEXT, TEXT>,\n    created_at      TIMESTAMP,\n    delivered_at    TIMESTAMP,\n    PRIMARY KEY ((user_id), created_at, notification_id)\n) WITH CLUSTERING ORDER BY (created_at DESC)\n  AND compaction = {'class': 'TimeWindowCompactionStrategy',\n                    'compaction_window_size': '1',\n                    'compaction_window_unit': 'DAYS'};\n\n-- TTL: auto-expire after 90 days\nALTER TABLE notifications WITH default_time_to_live = 7776000;\n</code></pre>\n<h2>Fan-Out Problem and Solution</h2>\n<p>Fan-out is the amplification problem. One event → many notifications. Two strategies:</p>\n<p><strong>Push fan-out (eager):</strong> Expand the fan-out immediately when the event is ingested. For a post liked by 1M followers: generate 1M notification records instantly.</p>\n<pre><code>Pros: Delivery latency is predictable (no fan-out latency at read time)\nCons: Hot events cause traffic spikes; wasteful for inactive users\n</code></pre>\n<p><strong>Pull fan-out (lazy):</strong> Store the event once; fan-out happens when the user opens the app.</p>\n<pre><code>Pros: Low write amplification; inactive users don't receive unnecessary processing\nCons: First read after event is slow (fan-out happens on read)\n</code></pre>\n<p><strong>Hybrid (what large platforms use):</strong> Push fan-out for regular users (&#x3C; 10K followers). Pull fan-out for celebrity/high-follower accounts. Threshold: if sender follower count > 100K, use pull fan-out.</p>\n<pre><code class=\"language-java\">@Service\npublic class FanOutRouter {\n\n    private static final int PUSH_FANOUT_THRESHOLD = 100_000;\n\n    public void route(NotificationEvent event) {\n        long followerCount = userService.getFollowerCount(event.getSenderId());\n\n        if (followerCount &#x3C;= PUSH_FANOUT_THRESHOLD) {\n            // Eager fan-out: write a notification for each follower now\n            fanOutService.pushFanOut(event);\n        } else {\n            // Lazy fan-out: write event once, expand on read\n            fanOutService.lazyFanOut(event);\n        }\n    }\n}\n</code></pre>\n<h2>Rate Limiting</h2>\n<p>Rate limiting protects users from notification spam and protects downstream channels from overload.</p>\n<p><strong>Per-user rate limits:</strong> Redis sliding window counter:</p>\n<pre><code class=\"language-java\">public boolean isAllowed(String userId, String channel) {\n    String key = \"ratelimit:\" + channel + \":\" + userId;\n    long windowSeconds = 3600L; // 1 hour window\n    int maxAllowed = switch (channel) {\n        case \"push\"  -> 10;\n        case \"email\" -> 3;\n        case \"sms\"   -> 2;\n        default      -> 20;\n    };\n\n    // Lua script: atomic sliding window check\n    Long count = redisTemplate.execute(\n        SLIDING_WINDOW_SCRIPT,\n        Collections.singletonList(key),\n        String.valueOf(System.currentTimeMillis()),\n        String.valueOf(windowSeconds * 1000),\n        String.valueOf(maxAllowed)\n    );\n\n    return count != null &#x26;&#x26; count &#x3C;= maxAllowed;\n}\n</code></pre>\n<p><strong>Channel-level rate limits:</strong> FCM allows 600 notifications/second per project, SES defaults to 14 emails/second. Use a token bucket per channel:</p>\n<pre><code class=\"language-java\">@Component\npublic class ChannelRateLimiter {\n    // RateLimiter from Guava: token bucket implementation\n    private final Map&#x3C;String, RateLimiter> channelLimiters = Map.of(\n        \"fcm\",  RateLimiter.create(500),  // 500/s, below FCM limit\n        \"apns\", RateLimiter.create(1000), // 1000/s\n        \"ses\",  RateLimiter.create(100),  // 100/s, limit for burst safety\n        \"sns\",  RateLimiter.create(200)\n    );\n\n    public void acquire(String channel) {\n        channelLimiters.get(channel).acquire(); // Blocks until permit available\n    }\n}\n</code></pre>\n<h2>Backpressure Handling</h2>\n<p>When downstream channels are slow (FCM is degraded, SES is throttling), Kafka consumer lag grows. Handle it explicitly:</p>\n<pre><code class=\"language-java\">@KafkaListener(topics = \"notifications-to-send\", groupId = \"push-dispatcher\")\npublic void dispatch(ConsumerRecord&#x3C;String, NotificationMessage> record,\n                     Acknowledgment ack) {\n    NotificationMessage msg = record.value();\n\n    // Check channel health before consuming more\n    if (channelHealthMonitor.isUnhealthy(\"fcm\")) {\n        // Pause this partition — let lag build in Kafka\n        consumer.pause(Collections.singleton(record.topicPartition()));\n        scheduledExecutor.schedule(() -> {\n            consumer.resume(Collections.singleton(record.topicPartition()));\n        }, 5, TimeUnit.SECONDS);\n        return; // Don't ack — will be redelivered\n    }\n\n    try {\n        channelRateLimiter.acquire(\"fcm\");\n        FcmResult result = fcmClient.send(msg);\n        notificationStore.updateStatus(msg.getNotificationId(), \"sent\", result.getMessageId());\n        ack.acknowledge();\n    } catch (FcmThrottledException e) {\n        // Don't ack — will retry\n        Thread.sleep(1000);\n    }\n}\n</code></pre>\n<h2>Idempotency Design</h2>\n<p>The event processor may process the same event twice (Kafka at-least-once). Fan-out must be idempotent:</p>\n<pre><code class=\"language-java\">@Transactional\npublic void fanOut(NotificationEvent event) {\n    String dedupKey = \"fanout:\" + event.getEventId();\n\n    // Atomic insert — skip if already processed\n    boolean inserted = cassandraOps.insert(\n        new FanOutRecord(event.getEventId(), Instant.now())\n    ).wasApplied(); // Cassandra lightweight transaction\n\n    if (!inserted) {\n        log.info(\"Fan-out already processed for event: {}\", event.getEventId());\n        return;\n    }\n\n    // Process fan-out only once\n    List&#x3C;Notification> notifications = generateNotifications(event);\n    kafkaTemplate.send(\"notifications-to-send\", notifications);\n}\n</code></pre>\n<h2>Retry Strategy</h2>\n<pre><code>Retry topology:\nnotifications-to-send\n        │\n        ├── FCM success → notification-status (delivered)\n        │\n        ├── FCM retryable error (5xx, timeout)\n        │       └── notifications-retry-30s (wait 30s)\n        │               └── notifications-retry-5m\n        │                       └── notifications-retry-30m\n        │                               └── notifications-dlq\n        │\n        └── FCM non-retryable (invalid token, app uninstalled)\n                └── Update user record: push token invalid\n                └── notifications-dlq (for audit)\n</code></pre>\n<h2>Horizontal Scaling</h2>\n<p>Each component scales independently:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Scale trigger</th>\n<th>Scale mechanism</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Event Processor</td>\n<td>Kafka consumer lag > 10K</td>\n<td>Add consumer instances</td>\n</tr>\n<tr>\n<td>Push Dispatcher</td>\n<td>Kafka consumer lag + FCM latency</td>\n<td>Add consumer instances</td>\n</tr>\n<tr>\n<td>Email Dispatcher</td>\n<td>SES send rate limit</td>\n<td>Add SES sending identities</td>\n</tr>\n<tr>\n<td>Cassandra</td>\n<td>Disk > 70% or read latency > 5ms</td>\n<td>Add Cassandra nodes</td>\n</tr>\n</tbody>\n</table>\n<p>Kafka consumers scale horizontally up to the partition count. With 1,500 partitions on <code>notifications-to-send</code>, you can run 1,500 consumer threads — spread across ~100 pods × 15 threads each.</p>\n<h2>Monitoring and Alerting</h2>\n<pre><code>Critical alerts (page immediately):\n- Kafka consumer lag on notifications-to-send > 1,000,000 messages\n- Push delivery success rate &#x3C; 95% over 5 minutes\n- DLQ topic lag growing > 1000/minute\n- Cassandra write latency P99 > 100ms\n\nWarning alerts (ticket/slack):\n- Fan-out throughput > 80% of capacity (approaching limit)\n- Per-user rate limit hit rate > 5% (users getting suppressed at high rate)\n- Email bounce rate > 2%\n</code></pre>\n<p>Grafana dashboard panels:</p>\n<ul>\n<li>Events ingested/second (with 24h comparison)</li>\n<li>Fan-out ratio (events → notifications) — spike indicates viral content</li>\n<li>Notification delivery success rate by channel</li>\n<li>Kafka consumer lag by consumer group</li>\n<li>Channel latency P50/P95/P99</li>\n</ul>\n<h2>Incident Simulation</h2>\n<p><strong>Scenario:</strong> FCM has a 30-minute partial outage (50% error rate).</p>\n<p><strong>Impact without proper design:</strong></p>\n<ul>\n<li>Push dispatcher retries immediately</li>\n<li>2× traffic to FCM</li>\n<li>FCM rate-limits our project</li>\n<li>All push notifications backed up</li>\n<li>Downstream timeout cascade</li>\n</ul>\n<p><strong>Impact with proper design:</strong></p>\n<ul>\n<li>Circuit breaker opens after 20% FCM error rate</li>\n<li>Push Dispatcher pauses Kafka consumption on FCM topic</li>\n<li>Kafka lag builds (Kafka as buffer — this is the right behavior)</li>\n<li>Backpressure propagates cleanly — no retry storm</li>\n<li>Email/SMS/in-app continue unaffected (separate consumer groups)</li>\n<li>When FCM recovers: circuit breaker half-opens, dispatcher resumes, lag drains over 10 minutes</li>\n</ul>\n<p>The system degrades gracefully. Push notifications are delayed, not lost.</p>\n<h2>Trade-offs Discussion</h2>\n<p><strong>Consistency vs availability for rate limits:</strong> Using Redis for per-user rate limits means Redis failure bypasses rate limiting. Decision: accept this. Redis failure is transient; brief notification overshoot is acceptable. Alternative (DB-based rate limits) would cause rate limit checks to become a bottleneck under load.</p>\n<p><strong>Fan-out at write vs read:</strong> Push fan-out guarantees low delivery latency but wastes compute for inactive users. For a 100M user platform where 20% are active, push fan-out wastes 80% of fan-out compute. Hybrid strategy based on sender follower count is the right balance.</p>\n<p><strong>Kafka retention:</strong> 24-hour retention on the notification topic is long enough for downstream failures to recover and replay. 7-day retention would require much larger storage. 1-hour retention would not cover extended outages.</p>\n<p>The architecture scales to 100K events/second because every component is stateless and horizontally scalable, and Kafka absorbs all the burst capacity between components. The hard part is the fan-out math — design your Kafka partition count around your peak fan-out ratio, not your ingestion rate.</p>\n","tableOfContents":[{"id":"functional-and-non-functional-requirements","text":"Functional and Non-Functional Requirements","level":2},{"id":"throughput-calculation-and-capacity-planning","text":"Throughput Calculation and Capacity Planning","level":2},{"id":"kafka-partition-planning","text":"Kafka Partition Planning","level":2},{"id":"system-architecture","text":"System Architecture","level":2},{"id":"database-schema","text":"Database Schema","level":2},{"id":"fan-out-problem-and-solution","text":"Fan-Out Problem and Solution","level":2},{"id":"rate-limiting","text":"Rate Limiting","level":2},{"id":"backpressure-handling","text":"Backpressure Handling","level":2},{"id":"idempotency-design","text":"Idempotency Design","level":2},{"id":"retry-strategy","text":"Retry Strategy","level":2},{"id":"horizontal-scaling","text":"Horizontal Scaling","level":2},{"id":"monitoring-and-alerting","text":"Monitoring and Alerting","level":2},{"id":"incident-simulation","text":"Incident Simulation","level":2},{"id":"trade-offs-discussion","text":"Trade-offs Discussion","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}