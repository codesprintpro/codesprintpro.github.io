{"pageProps":{"post":{"title":"Building a Production RAG System: Embeddings, Vector DBs, and Retrieval","description":"A practical guide to building a Retrieval-Augmented Generation system — from chunking strategies and embedding models to vector databases, retrieval optimization, and avoiding hallucinations.","date":"2025-02-12","category":"AI/ML","tags":["ai","llm","rag","langchain","vector database","embeddings"],"featured":true,"affiliateSection":"ai-ml-books","slug":"building-rag-system-langchain","readingTime":"12 min read","excerpt":"Retrieval-Augmented Generation (RAG) is the most practical technique for making LLMs useful on your private data. Instead of hoping the model memorizes your documents during training (it doesn't), RAG retrieves relevant …","contentHtml":"<p>Retrieval-Augmented Generation (RAG) is the most practical technique for making LLMs useful on your private data. Instead of hoping the model memorizes your documents during training (it doesn't), RAG retrieves relevant context at query time and injects it into the prompt. The model reasons over retrieved facts rather than hallucinated ones.</p>\n<p>Getting RAG to work in a notebook demo is easy. Getting it to work reliably in production — with accurate retrieval, consistent quality, and measurable performance — requires understanding every component in the pipeline.</p>\n<h2>Why RAG, Not Fine-Tuning?</h2>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>When to Use</th>\n<th>Cost</th>\n<th>Freshness</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>RAG</strong></td>\n<td>Dynamic data, factual Q&#x26;A, large corpora</td>\n<td>Low compute, storage cost</td>\n<td>Real-time updates</td>\n</tr>\n<tr>\n<td><strong>Fine-tuning</strong></td>\n<td>Style/tone transfer, format adherence, domain jargon</td>\n<td>High GPU cost, retraining</td>\n<td>Snapshot in time</td>\n</tr>\n<tr>\n<td><strong>Context stuffing</strong></td>\n<td>&#x3C;128K tokens, structured data</td>\n<td>API cost (tokens)</td>\n<td>Real-time</td>\n</tr>\n<tr>\n<td><strong>RAG + Fine-tuning</strong></td>\n<td>Best factual recall + domain style</td>\n<td>High</td>\n<td>Real-time</td>\n</tr>\n</tbody>\n</table>\n<p>For most enterprise use cases — internal knowledge bases, documentation Q&#x26;A, customer support — RAG is the right tool.</p>\n<h2>The RAG Pipeline</h2>\n<p>Think of the RAG pipeline as two separate workflows: an offline ingestion phase where you prepare and index your documents, and an online query phase where you look up relevant information to answer each user question. Understanding this separation helps you optimize each phase independently.</p>\n<pre><code>Ingestion Pipeline (offline):\n  Documents → Chunker → Embedder → Vector DB\n\nQuery Pipeline (online):\n  Question → Embedder → Vector DB → [Top-K chunks] → LLM → Answer\n\nFull flow:\n  ┌──────────┐   ┌──────────┐   ┌────────────┐   ┌──────────────┐\n  │ Document │──►│  Chunk   │──►│  Embed     │──►│  Vector DB   │\n  │ (PDF,    │   │  Split   │   │  (OpenAI,  │   │  (Pinecone,  │\n  │  DOCX,   │   │          │   │  Cohere)   │   │  ChromaDB,   │\n  │  HTML)   │   └──────────┘   └────────────┘   │  pgvector)   │\n  └──────────┘                                    └──────┬───────┘\n                                                         │ similarity\n  ┌──────────┐   ┌──────────┐   ┌────────────┐         │ search\n  │  Answer  │◄──│   LLM    │◄──│  Prompt    │◄────────┘\n  │          │   │ (GPT-4,  │   │  Template  │  Top-K chunks\n  └──────────┘   │  Claude) │   └────────────┘\n                 └──────────┘\n</code></pre>\n<h2>Step 1: Document Loading and Chunking</h2>\n<p>Chunking is the most underappreciated step. Bad chunking breaks context across meaningful boundaries, and no retrieval algorithm can recover from that.</p>\n<p>Your goal with chunking is to create pieces of text that are self-contained enough to answer a question on their own, while staying small enough to be precise when retrieved. The code below demonstrates three strategies — from the simplest recursive split to the most sophisticated semantic approach — so you can pick the right tool for your document type.</p>\n<pre><code class=\"language-python\">from langchain.document_loaders import PyPDFLoader, DirectoryLoader\nfrom langchain.text_splitter import (\n    RecursiveCharacterTextSplitter,\n    MarkdownHeaderTextSplitter,\n)\n\n# Load documents\nloader = DirectoryLoader(\"./docs\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\nraw_docs = loader.load()\n\n# Strategy 1: Recursive character splitting (most robust for mixed content)\n# Tries to split on: paragraphs → sentences → words → characters\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,      # Characters per chunk\n    chunk_overlap=200,    # Overlap to preserve context across boundaries\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n)\nchunks = text_splitter.split_documents(raw_docs)\n\n# Strategy 2: Markdown-aware splitting (for structured docs)\n# Respects heading hierarchy — chunks stay within sections\nmd_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on=[\n        (\"#\", \"h1\"),\n        (\"##\", \"h2\"),\n        (\"###\", \"h3\"),\n    ],\n    strip_headers=False,  # Keep headers in chunk for context\n)\n\n# Strategy 3: Semantic chunking (most accurate, slower)\n# Groups sentences by embedding similarity — no arbitrary character limits\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai import OpenAIEmbeddings\n\nsemantic_splitter = SemanticChunker(\n    OpenAIEmbeddings(),\n    breakpoint_threshold_type=\"percentile\",  # Split at 95th percentile of similarity drop\n    breakpoint_threshold_amount=95,\n)\n</code></pre>\n<p>Notice that the <code>chunk_overlap=200</code> parameter is doing important work here: it ensures that a sentence split across two chunks appears in both, so neither chunk loses critical context at its boundary.</p>\n<p><strong>Chunking guidelines from production experience:</strong></p>\n<ul>\n<li><strong>500-1000 characters</strong> for Q&#x26;A over prose documents</li>\n<li><strong>1500-2000 characters</strong> for code documentation (preserve function context)</li>\n<li><strong>200 character overlap</strong> to prevent context loss at boundaries</li>\n<li><strong>Metadata preservation</strong> is critical: always keep source URL, page number, section header</li>\n</ul>\n<h2>Step 2: Embedding Models</h2>\n<p>Embeddings convert text to vectors — numerical representations where semantically similar text clusters together in high-dimensional space.</p>\n<p>Think of an embedding as a sophisticated \"fingerprint\" for meaning: two sentences that say the same thing in different words will have nearly identical fingerprints, while two sentences about completely different topics will have fingerprints that bear no resemblance to each other. The code below shows how to generate these fingerprints using three different providers, each with different cost and performance tradeoffs.</p>\n<pre><code class=\"language-python\">from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Option 1: OpenAI text-embedding-3-small (recommended for most use cases)\n# Dimensions: 1536, Cost: $0.02/1M tokens\n# Strong multilingual support, easy integration\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# Option 2: Open-source via HuggingFace (no API cost, self-hosted)\n# BAAI/bge-large-en-v1.5: Strong English performance, competitive with OpenAI\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"BAAI/bge-large-en-v1.5\",\n    model_kwargs={\"device\": \"cpu\"},\n    encode_kwargs={\"normalize_embeddings\": True},\n)\n\n# Option 3: Cohere embed-english-v3.0\n# Better at distinguishing search queries from documents (trained with input_type)\nfrom langchain_cohere import CohereEmbeddings\nembeddings = CohereEmbeddings(\n    model=\"embed-english-v3.0\",\n    input_type=\"search_query\",  # Or \"search_document\" for indexing\n)\n</code></pre>\n<p>Pay attention to the <code>input_type</code> parameter in the Cohere example — this is a subtle but powerful feature. Cohere trains the model to understand that a short user question and a long document passage are answering the same semantic question from different sides, which improves retrieval quality compared to treating both identically.</p>\n<p><strong>Embedding model comparison (MTEB benchmark, 2024):</strong></p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>MTEB Score</th>\n<th>Dimensions</th>\n<th>Cost</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>OpenAI text-embedding-3-large</td>\n<td>64.6</td>\n<td>3072</td>\n<td>$0.13/1M tokens</td>\n</tr>\n<tr>\n<td>Cohere embed-english-v3.0</td>\n<td>64.5</td>\n<td>1024</td>\n<td>$0.10/1M tokens</td>\n</tr>\n<tr>\n<td>BAAI/bge-large-en-v1.5</td>\n<td>63.5</td>\n<td>1024</td>\n<td>Free (self-hosted)</td>\n</tr>\n<tr>\n<td>OpenAI text-embedding-3-small</td>\n<td>62.3</td>\n<td>1536</td>\n<td>$0.02/1M tokens</td>\n</tr>\n</tbody>\n</table>\n<p>For most production RAG systems, <code>text-embedding-3-small</code> or <code>bge-large-en-v1.5</code> provides the best cost/performance tradeoff.</p>\n<h2>Step 3: Vector Databases</h2>\n<p>Once your documents are embedded, you need somewhere to store and efficiently search those vectors. A vector database is purpose-built for one operation: given a query vector, find the N most similar stored vectors as fast as possible. The choice of database mainly comes down to your scale and infrastructure constraints — the code patterns look nearly identical across all three options shown here.</p>\n<pre><code class=\"language-python\"># Option 1: ChromaDB (local development, small-medium scale)\nimport chromadb\nfrom langchain_chroma import Chroma\n\nchroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\nvectorstore = Chroma(\n    collection_name=\"docs\",\n    embedding_function=embeddings,\n    client=chroma_client,\n)\n\n# Ingest documents\nvectorstore.add_documents(chunks)\n\n# Option 2: pgvector (PostgreSQL extension — great for existing PG users)\nfrom langchain_postgres import PGVector\n\nvectorstore = PGVector(\n    embeddings=embeddings,\n    collection_name=\"docs\",\n    connection=\"postgresql+psycopg://user:pass@localhost:5432/mydb\",\n)\n\n# Option 3: Pinecone (managed, production-grade, serverless)\nfrom langchain_pinecone import PineconeVectorStore\nimport pinecone\n\npc = pinecone.Pinecone(api_key=\"YOUR_API_KEY\")\nindex = pc.Index(\"docs-index\")\n\nvectorstore = PineconeVectorStore(\n    index=index,\n    embedding=embeddings,\n    text_key=\"text\",\n)\n</code></pre>\n<p><strong>Vector DB selection criteria:</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ChromaDB</th>\n<th>pgvector</th>\n<th>Pinecone</th>\n<th>Weaviate</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Setup</td>\n<td>Trivial</td>\n<td>Easy (extension)</td>\n<td>Managed</td>\n<td>Self-hosted/managed</td>\n</tr>\n<tr>\n<td>Scale</td>\n<td>&#x3C;1M vectors</td>\n<td>&#x3C;100M vectors</td>\n<td>Hundreds of millions</td>\n<td>Billions</td>\n</tr>\n<tr>\n<td>Filtering</td>\n<td>Basic</td>\n<td>Full SQL</td>\n<td>Metadata filters</td>\n<td>GraphQL</td>\n</tr>\n<tr>\n<td>Cost</td>\n<td>Free</td>\n<td>PG cost</td>\n<td>Pay per vector</td>\n<td>Free/managed</td>\n</tr>\n<tr>\n<td>Best for</td>\n<td>Dev/prototype</td>\n<td>Existing PG</td>\n<td>Production SaaS</td>\n<td>Complex filtering</td>\n</tr>\n</tbody>\n</table>\n<h2>Step 4: Building the Retrieval Chain</h2>\n<p>Now that you have indexed documents, you can wire together the full question-answering chain. The prompt template here is doing a critical job: it instructs the model to stay within the retrieved context and explicitly say when it doesn't know something, which is what prevents hallucination.</p>\n<p>Basic retrieval is just similarity search. Production retrieval combines dense search, sparse (BM25) search, and reranking.</p>\n<pre><code class=\"language-python\">from langchain_openai import ChatOpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\n# Basic RAG chain\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\nprompt_template = \"\"\"Use the following context to answer the question.\nIf the answer is not in the context, say \"I don't have enough information to answer this question.\"\nDo not make up information.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\nprompt = PromptTemplate(\n    template=prompt_template,\n    input_variables=[\"context\", \"question\"],\n)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",                       # stuff: concat all chunks into one prompt\n    retriever=vectorstore.as_retriever(\n        search_type=\"similarity\",\n        search_kwargs={\"k\": 5}                # Retrieve top 5 chunks\n    ),\n    chain_type_kwargs={\"prompt\": prompt},\n    return_source_documents=True,             # For citation support\n)\n\nresult = qa_chain.invoke({\"query\": \"What is the refund policy?\"})\nprint(result[\"result\"])\nprint(\"Sources:\", [doc.metadata for doc in result[\"source_documents\"]])\n</code></pre>\n<p>The <code>return_source_documents=True</code> flag is important for production use: it lets you display citations alongside the answer, so users can verify the information and you can debug retrieval failures.</p>\n<h3>Advanced: Hybrid Search with Reranking</h3>\n<p>Pure vector similarity misses keyword-heavy queries. Hybrid search combines dense (embedding) and sparse (BM25/TF-IDF) retrieval. Imagine a user searching for a product code like \"SKU-7829\" — pure semantic search will struggle with this exact string, but BM25 keyword search handles it perfectly. By blending both approaches, you get the best of both worlds.</p>\n<pre><code class=\"language-python\">from langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\n\n# BM25: keyword-based retrieval (great for exact terms, product codes, names)\nbm25_retriever = BM25Retriever.from_documents(chunks)\nbm25_retriever.k = 10\n\n# Dense: semantic retrieval\ndense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n\n# Ensemble: weighted hybrid (60% dense, 40% sparse)\nhybrid_retriever = EnsembleRetriever(\n    retrievers=[dense_retriever, bm25_retriever],\n    weights=[0.6, 0.4],\n)\n\n# Reranker: re-scores top-20 results using a cross-encoder model\n# Cross-encoders compare query+document jointly — much more accurate than bi-encoder similarity\nreranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\nreranker = CrossEncoderReranker(model=reranker_model, top_n=5)\n\n# Final chain: retrieve 20, rerank to top 5\nfrom langchain.retrievers import ContextualCompressionRetriever\n\nfinal_retriever = ContextualCompressionRetriever(\n    base_compressor=reranker,\n    base_retriever=hybrid_retriever,\n)\n</code></pre>\n<p>The two-stage retrieve-then-rerank pattern is the key insight here: you cast a wide net with fast approximate search (retrieving 20 candidates), then apply an expensive but highly accurate cross-encoder to reorder just those 20 into the final top 5. This gives you accuracy close to brute-force search at a fraction of the cost.</p>\n<h2>Step 5: Evaluating RAG Quality</h2>\n<p>RAG quality is hard to measure without a systematic evaluation framework. The three key metrics:</p>\n<p>Without measurement, you cannot tell the difference between a retrieval bug and a generation bug — they both produce wrong answers. The RAGAS library gives you automated scores across four dimensions, letting you pinpoint exactly where your pipeline is failing.</p>\n<pre><code class=\"language-python\"># Using RAGAS (open-source RAG evaluation library)\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,       # Does the answer contain only information from context?\n    answer_relevancy,   # Is the answer relevant to the question?\n    context_precision,  # Are retrieved chunks relevant to the question?\n    context_recall,     # Were all relevant chunks retrieved?\n)\nfrom datasets import Dataset\n\n# Build evaluation dataset\neval_data = {\n    \"question\": [\"What is the return policy?\", \"How do I reset my password?\"],\n    \"answer\": [answer1, answer2],              # Model's generated answers\n    \"contexts\": [[doc1, doc2], [doc3]],        # Retrieved chunks\n    \"ground_truth\": [\"30-day returns...\", \"Click Forgot Password...\"],  # Expected answers\n}\n\ndataset = Dataset.from_dict(eval_data)\nresult = evaluate(dataset, metrics=[faithfulness, answer_relevancy, context_precision, context_recall])\n\nprint(result)\n# faithfulness: 0.92 (high = model stays within retrieved context)\n# answer_relevancy: 0.87\n# context_precision: 0.78 (room to improve retrieval)\n# context_recall: 0.83\n</code></pre>\n<p>A low <code>context_precision</code> score (like 0.78 above) tells you that your retriever is returning irrelevant chunks — the problem is in retrieval, not generation. A low <code>faithfulness</code> score tells you the LLM is going off-script and adding information not in the retrieved context — the fix is a stricter prompt. These metrics let you diagnose and fix the right component.</p>\n<h2>Common Production Pitfalls</h2>\n<p><strong>1. Chunks too large:</strong>\nThe LLM sees the full chunk even if only 2 sentences are relevant. Information density drops. Solution: smaller chunks (500-800 chars) with reranking to surface the best ones.</p>\n<p><strong>2. No metadata filtering:</strong>\nSearching all documents when the user's question is clearly about product X. Solution: extract entities from the query and filter by metadata before vector search.</p>\n<p>Before performing similarity search, you can narrow the candidate pool using structured metadata filters — this is far cheaper than relying on the vector index alone to find the right product or time range.</p>\n<pre><code class=\"language-python\"># Filter by document source before vector search\nresults = vectorstore.similarity_search(\n    query=user_question,\n    k=5,\n    filter={\"product\": \"product-x\", \"version\": \"2.0\"},\n)\n</code></pre>\n<p><strong>3. Missing query rewriting:</strong>\nUser questions are often terse or ambiguous. Rewrite queries before retrieval. A user typing \"refund?\" has a very different retrieval surface area than the full query \"What is the process for requesting a refund for a digital product?\". Query rewriting bridges that gap automatically.</p>\n<pre><code class=\"language-python\">rewrite_prompt = \"\"\"Rewrite this user question into a detailed search query\nthat will retrieve relevant documentation chunks.\n\nUser question: {question}\nSearch query:\"\"\"\n\n# Multi-query: generate 3 variations, union retrieval results\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nmulti_retriever = MultiQueryRetriever.from_llm(\n    retriever=vectorstore.as_retriever(),\n    llm=llm,\n)\n</code></pre>\n<p><strong>4. No citation/grounding:</strong>\nUsers cannot verify answers if sources aren't surfaced. Always return source documents and display them alongside the answer.</p>\n<p><strong>5. Embedding model mismatch:</strong>\nIndexing with one model and querying with another produces garbage results. Document your embedding model version and treat it as a breaking change when upgrading.</p>\n<h2>Production Architecture</h2>\n<p>With all the individual components understood, here is how they fit together in a complete production system. Notice that caching and monitoring are first-class concerns — not afterthoughts — because they determine whether your system is fast and debuggable at scale.</p>\n<pre><code>Query flow:\n  API request → Query rewriter → Hybrid retriever → Reranker → LLM → Response\n                                        ↕\n                               Pinecone + ElasticSearch\n                               (dense + sparse search)\n\nCaching:\n  Embed query → Hash → Redis cache → Return cached answer if hit\n  Cache TTL: 1 hour (for FAQ-style queries that repeat)\n\nMonitoring:\n  - Faithfulness score per request (flag &#x3C;0.8)\n  - Retrieval latency (p99 should be &#x3C;500ms)\n  - LLM latency (p99 should be &#x3C;3s)\n  - Thumbs up/down feedback → used to improve retrieval\n</code></pre>\n<p>RAG is not a silver bullet, but for the right use cases — private knowledge bases, document Q&#x26;A, support bots — it's the most cost-effective and maintainable path to reliable LLM-powered applications. The difference between a demo and a production system lies in chunking quality, hybrid retrieval, reranking, and systematic evaluation.</p>\n","tableOfContents":[{"id":"why-rag-not-fine-tuning","text":"Why RAG, Not Fine-Tuning?","level":2},{"id":"the-rag-pipeline","text":"The RAG Pipeline","level":2},{"id":"step-1-document-loading-and-chunking","text":"Step 1: Document Loading and Chunking","level":2},{"id":"step-2-embedding-models","text":"Step 2: Embedding Models","level":2},{"id":"step-3-vector-databases","text":"Step 3: Vector Databases","level":2},{"id":"step-4-building-the-retrieval-chain","text":"Step 4: Building the Retrieval Chain","level":2},{"id":"advanced-hybrid-search-with-reranking","text":"Advanced: Hybrid Search with Reranking","level":3},{"id":"step-5-evaluating-rag-quality","text":"Step 5: Evaluating RAG Quality","level":2},{"id":"common-production-pitfalls","text":"Common Production Pitfalls","level":2},{"id":"production-architecture","text":"Production Architecture","level":2}]},"relatedPosts":[{"title":"Fine-Tuning LLMs: When to Fine-Tune, When to Prompt","description":"Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.","date":"2025-03-27","category":"AI/ML","tags":["ai","llm","fine-tuning","lora","hugging face","openai","machine learning"],"featured":false,"affiliateSection":"ai-ml-books","slug":"fine-tuning-llms","readingTime":"10 min read","excerpt":"Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning — fo…"},{"title":"Building AI Agents with Tool Use: From Chatbot to Autonomous Agent","description":"Build production AI agents using Claude's tool use API. Learn the agentic loop, error handling, multi-step reasoning, human-in-the-loop patterns, and how to build reliable autonomous systems.","date":"2025-03-23","category":"AI/ML","tags":["ai","agents","claude","tool use","llm","autonomous systems","python"],"featured":false,"affiliateSection":"ai-ml-books","slug":"llm-agents-tool-use","readingTime":"10 min read","excerpt":"A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio…"},{"title":"Vector Embeddings: The Foundation of Modern AI Applications","description":"Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.","date":"2025-03-11","category":"AI/ML","tags":["ai","embeddings","vector database","semantic search","rag","pgvector","pinecone"],"featured":false,"affiliateSection":"ai-ml-books","slug":"vector-embeddings-deep-dive","readingTime":"11 min read","excerpt":"Every modern AI application — semantic search, RAG, recommendations, duplicate detection — is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantic…"}]},"__N_SSG":true}