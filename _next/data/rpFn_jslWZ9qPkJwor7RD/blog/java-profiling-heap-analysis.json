{"pageProps":{"post":{"title":"Java Profiling and Heap Analysis: Finding Memory Leaks and CPU Bottlenecks","description":"Production Java profiling: async-profiler for CPU and allocation profiling, heap dump analysis with Eclipse MAT, finding memory leaks, GC log analysis, detecting thread contention, JVM flags for profiling in production, and reading flame graphs.","date":"2025-05-09","category":"Java","tags":["java","profiling","heap analysis","memory leak","jvm","performance","async-profiler","gc"],"featured":false,"affiliateSection":"java-courses","slug":"java-profiling-heap-analysis","readingTime":"7 min read","excerpt":"Java applications in production develop performance problems that don't reproduce locally: memory that grows slowly until OOM, GC pauses that spike P99 latency, threads that contend on a lock under load. The difference b…","contentHtml":"<p>Java applications in production develop performance problems that don't reproduce locally: memory that grows slowly until OOM, GC pauses that spike P99 latency, threads that contend on a lock under load. The difference between an engineer who fixes these problems in hours versus days is knowing which tool to use and how to read what it shows.</p>\n<h2>The Profiling Toolkit</h2>\n<pre><code>Problem type → Tool to use:\nCPU bottleneck (high CPU, slow response)  → async-profiler (CPU mode)\nMemory leak (OOM, growing heap)           → async-profiler (allocation) + heap dump + MAT\nGC pressure (GC overhead, pauses)        → GC logs + GCViewer / GC Easy\nThread contention (lock waits, deadlock) → async-profiler (wall-clock mode) + thread dump\nLatency spikes (P99 >> P50)              → async-profiler + distributed tracing\nStartup performance                       → JVM startup flags + GraalVM profile\n</code></pre>\n<h2>async-profiler: Low-Overhead Production Profiling</h2>\n<p><a href=\"https://github.com/async-profiler/async-profiler\">async-profiler</a> is the best production-safe profiler for JVM applications. It uses AsyncGetCallTrace (bypasses safepoints — true async sampling, no \"safepoint bias\" that makes synchronized code look fast) and perf_events for native stack frames.</p>\n<p><strong>Installation and basic usage:</strong></p>\n<pre><code class=\"language-bash\"># Download:\nwget https://github.com/async-profiler/async-profiler/releases/download/v3.0/async-profiler-3.0-linux-x64.tar.gz\ntar xzf async-profiler-3.0-linux-x64.tar.gz\n\n# Find target JVM PID:\njps -l\n# → 12345 com.example.OrderServiceApplication\n\n# CPU profiling (30 seconds, flamegraph output):\n./asprof -d 30 -f /tmp/cpu-profile.html 12345\n\n# Allocation profiling (which code allocates the most memory):\n./asprof -e alloc -d 30 -f /tmp/alloc-profile.html 12345\n\n# Wall-clock profiling (including I/O and lock waits — not just CPU):\n./asprof -e wall -d 30 -f /tmp/wall-profile.html 12345\n\n# Lock profiling (find lock contention):\n./asprof -e lock -d 30 -f /tmp/lock-profile.html 12345\n</code></pre>\n<p><strong>Reading a flame graph:</strong></p>\n<pre><code>Flame graph anatomy:\n\nY-axis: stack depth (bottom = thread entry, top = leaf frame where CPU is spent)\nX-axis: time percentage (wider = more time spent in this frame and its callees)\nColors: arbitrary (but consistent — same function same color)\n\nWide frames at the TOP are the bottleneck.\nWide frames in the MIDDLE are code paths that lead to the bottleneck.\n\nExample: Wide frame \"HashMap.get()\" at top of many stacks\n→ All requests are spending time in HashMap.get()\n→ Could be: GC resizing (too-small initial capacity)\n→ Could be: Thread contention on a shared HashMap\n→ Look at the frame below: what's calling HashMap.get()?\n</code></pre>\n<p><strong>JVM flags to enable profiling in production:</strong></p>\n<pre><code class=\"language-bash\"># Add to JVM startup (low overhead, safe for production):\n-XX:+UnlockDiagnosticVMOptions\n-XX:+DebugNonSafepoints  # Required for async-profiler accuracy\n\n# GC logging (essential — always enable in production):\n-Xlog:gc*:file=/var/log/app/gc.log:time,uptime:filecount=10,filesize=50m\n\n# Enable JFR (Java Flight Recorder — built-in profiler, low overhead):\n-XX:+FlightRecorder\n-XX:StartFlightRecording=duration=120s,filename=/tmp/recording.jfr\n</code></pre>\n<h2>Heap Dump Analysis with Eclipse MAT</h2>\n<p>When memory grows and OOM is imminent, take a heap dump:</p>\n<pre><code class=\"language-bash\"># On running JVM:\njmap -dump:live,format=b,file=/tmp/heap.hprof 12345\n# \"live\" — only live objects (reachable from GC roots) — smaller, more useful\n\n# On OOM (add to JVM flags — auto-dump on OOM):\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/tmp/heap-dumps/\n# Creates heap-dump-&#x3C;timestamp>.hprof on OOM\n\n# Compress heap dump for transfer (heap dumps are large):\ngzip /tmp/heap.hprof  # 10GB dump → ~2GB compressed\n</code></pre>\n<p><strong>Eclipse MAT analysis workflow:</strong></p>\n<pre><code>1. Open heap.hprof in Eclipse MAT\n\n2. Overview tab: pie chart of object sizes\n   → Look for: one class consuming disproportionate heap (> 20%)\n\n3. Leak Suspects report (MAT auto-generates):\n   → \"One instance of com.example.RequestContext occupies 2.4GB (85%)\"\n   → This is your leak\n\n4. Dominator Tree: shows objects that retain the most heap\n   → Find the largest retained size objects\n   → Expand: what's inside? Why is it still referenced?\n\n5. Histogram: list all object types by count and size\n   → Filter by count: char[] and byte[] at top = normal (String internals)\n   → \"HashMap$Entry × 5,000,000\" = possibly a growing cache / map\n\n6. Paths to GC Roots: for a suspicious object, find what keeps it alive\n   → Right-click object → \"Merge Shortest Paths to GC Roots\"\n   → Shows the chain of references keeping this object in memory\n</code></pre>\n<p><strong>Common memory leak patterns:</strong></p>\n<pre><code class=\"language-java\">// Pattern 1: Static map used as cache (never evicts)\npublic class ProductService {\n    private static final Map&#x3C;Long, Product> cache = new HashMap&#x3C;>();\n\n    public Product getProduct(long id) {\n        return cache.computeIfAbsent(id, productRepo::findById);\n        // Problem: cache grows forever — every product ever queried is retained\n    }\n}\n// Fix: Use Caffeine or Guava Cache with size limit and TTL\n\n// Pattern 2: Event listener not deregistered\npublic class EventProcessor {\n    @Autowired\n    private EventBus eventBus;\n\n    @PostConstruct\n    public void init() {\n        eventBus.register(this);  // Registers listener\n        // Problem: if EventProcessor is prototype-scoped and created many times,\n        // each instance is retained by eventBus forever → memory leak\n    }\n\n    @PreDestroy\n    public void cleanup() {\n        eventBus.unregister(this);  // Fix: always deregister on destroy\n    }\n}\n\n// Pattern 3: ThreadLocal not cleaned up\npublic class TenantContext {\n    private static final ThreadLocal&#x3C;String> tenant = new ThreadLocal&#x3C;>();\n\n    public static void set(String id) { tenant.set(id); }\n    public static String get() { return tenant.get(); }\n\n    // Missing: clear() called after request completes\n    // In a thread pool, threads are reused — ThreadLocal carries value to next request\n    // If value is a large object: memory leak across requests\n    public static void clear() { tenant.remove(); }  // Must be called in finally block\n}\n</code></pre>\n<h2>GC Log Analysis</h2>\n<p>GC logs reveal pause patterns, GC frequency, and heap sizing problems:</p>\n<pre><code class=\"language-bash\"># GC log enabled (JVM 11+ format):\n-Xlog:gc*:file=/var/log/gc.log:time,uptime\n\n# Sample GC log output:\n[2025-01-15T10:23:45.123+0000][1234.567s] GC(42) Pause Young (Normal) (G1 Evacuation Pause)\n  Eden: 512M->0M(512M)  Survivors: 32M->64M  Heap: 2048M->1536M(4096M)\n  User=0.450s Sys=0.020s Real=0.047s  ← 47ms pause — check if this is acceptable\n\n# Problematic patterns to look for:\n# 1. \"Pause Full\" (stop-the-world full GC) → heap too small or memory leak\n# 2. GC frequency increasing → heap filling faster over time (leak)\n# 3. Heap after GC increasing each time → objects surviving that shouldn't\n# 4. Very long pauses (> 500ms) → GC tuning needed\n</code></pre>\n<p><strong>GC visualization with GCViewer:</strong></p>\n<pre><code class=\"language-bash\"># Parse GC log and show graphical analysis:\njava -jar gcviewer.jar /var/log/gc.log\n# Shows: pause time histogram, heap usage over time, GC throughput %\n</code></pre>\n<p><strong>Key GC metrics in Micrometer (Spring Boot Actuator):</strong></p>\n<pre><code>jvm.gc.pause{action=\"end of minor GC\", cause=\"G1 Evacuation Pause\"}\njvm.gc.memory.allocated  ← Allocation rate (bytes/second)\njvm.gc.memory.promoted   ← Old gen promotion rate\njvm.memory.used{area=\"heap\"}\n\nAlert thresholds:\nGC pause > 200ms (95th percentile) → investigate\nGC time > 5% of total time → heap sizing issue\nOld gen > 80% after full GC → potential memory leak\n</code></pre>\n<h2>Thread Dump Analysis</h2>\n<p>For hung applications and deadlocks:</p>\n<pre><code class=\"language-bash\"># Take thread dump:\njstack 12345 > /tmp/threads.txt\n\n# Or via JVM signal:\nkill -3 12345  # Prints thread dump to stdout (redirected from service logs)\n\n# Thread dump shows each thread's state and stack:\n\"http-nio-8080-exec-42\" #154 daemon prio=5\n   java.lang.Thread.State: BLOCKED (on object monitor)\n   → waiting to lock &#x3C;0x00000006c0987a80> (a java.util.HashMap)\n   → held by \"http-nio-8080-exec-3\"\n\n# This indicates: two request threads contending on a shared HashMap\n# exec-3 holds the lock, exec-42 is waiting\n# Fix: use ConcurrentHashMap or move to per-request scope\n</code></pre>\n<p><strong>fastthread.io:</strong> Upload thread dump for automated analysis — identifies deadlocks, blocked threads, and contention patterns.</p>\n<h2>CPU Profiling: Case Study</h2>\n<p>Scenario: order service P99 latency jumped from 80ms to 800ms after a deploy. No OOM, memory looks fine.</p>\n<pre><code class=\"language-bash\"># 1. Capture CPU flame graph:\n./asprof -d 60 -f /tmp/cpu.html $(pgrep -f OrderService)\n\n# 2. Open flame graph, look for wide top frames:\n# Found: 40% of CPU time in:\n#   com.example.OrderService.calculateTotals\n#   → java.util.stream.Stream.sorted()\n#   → java.util.Arrays.mergeSort()\n\n# This is suspicious — why is sort taking 40% of CPU?\n\n# 3. Check the code:\npublic BigDecimal calculateTotals(List&#x3C;OrderItem> items) {\n    return items.stream()\n        .sorted(Comparator.comparing(OrderItem::getPrice).reversed())  // ← sorting each time\n        .map(OrderItem::getPrice)\n        .reduce(BigDecimal.ZERO, BigDecimal::add);\n}\n</code></pre>\n<p>The bug: <code>sorted()</code> was added \"for display purposes\" but calculateTotals doesn't need sorted items. The sort is O(n log n) per call. At 1,000 calls/second on a list of 100 items — enormous wasted work.</p>\n<p>Fix: remove the sort. P99 drops back to 85ms.</p>\n<p>The tooling only reveals what you're looking for. But the pattern is consistent: CPU flame graph points to the hot function; code review explains why it's hot. async-profiler + 30 seconds of sampling + MAT for memory — these tools find in minutes what weeks of log analysis won't.</p>\n","tableOfContents":[{"id":"the-profiling-toolkit","text":"The Profiling Toolkit","level":2},{"id":"async-profiler-low-overhead-production-profiling","text":"async-profiler: Low-Overhead Production Profiling","level":2},{"id":"heap-dump-analysis-with-eclipse-mat","text":"Heap Dump Analysis with Eclipse MAT","level":2},{"id":"gc-log-analysis","text":"GC Log Analysis","level":2},{"id":"thread-dump-analysis","text":"Thread Dump Analysis","level":2},{"id":"cpu-profiling-case-study","text":"CPU Profiling: Case Study","level":2}]},"relatedPosts":[{"title":"Java Concurrency Patterns: CompletableFuture, Structured Concurrency, and Thread-Safe Design","description":"Production Java concurrency: CompletableFuture pipelines, handling exceptions in async chains, Java 21 structured concurrency, thread-safe collection patterns, and the concurrency bugs that cause data corruption.","date":"2025-07-08","category":"Java","tags":["java","concurrency","completablefuture","virtual threads","java21","thread-safe","async"],"featured":false,"affiliateSection":"java-courses","slug":"java-concurrency-patterns","readingTime":"7 min read","excerpt":"Java concurrency has three eras: raw  and  (Java 1-4), the  framework (Java 5+), and the virtual thread/structured concurrency era (Java 21+). Each era's patterns still exist in production codebases. Understanding all th…"},{"title":"Java Memory Management Deep Dive: Heap, GC, and Production Tuning","description":"How the JVM allocates memory, how G1GC and ZGC work under the hood, heap analysis with JVM tools, and the GC tuning decisions that eliminate latency spikes in production Java services.","date":"2025-06-13","category":"Java","tags":["java","jvm","garbage collection","g1gc","zgc","heap","memory management","performance"],"featured":false,"affiliateSection":"java-courses","slug":"java-memory-management-deep-dive","readingTime":"7 min read","excerpt":"Java's garbage collector is the single biggest source of unexplained latency spikes in production services. A GC pause of 2 seconds is invisible in most logs but visible to every user who happened to make a request durin…"},{"title":"Spring Security OAuth2 and JWT: Production Implementation Guide","description":"Complete Spring Security OAuth2 implementation: JWT token validation, Resource Server configuration, method-level security, custom UserDetailsService, refresh token rotation, and the security pitfalls that lead to authentication bypasses.","date":"2025-06-03","category":"Java","tags":["spring security","oauth2","jwt","spring boot","authentication","authorization","java","security"],"featured":false,"affiliateSection":"java-courses","slug":"spring-security-oauth2-jwt","readingTime":"7 min read","excerpt":"Spring Security is one of the most powerful and most misunderstood frameworks in the Java ecosystem. Its flexibility is its strength — and its complexity. Misconfigured security is worse than no security, because it give…"}]},"__N_SSG":true}