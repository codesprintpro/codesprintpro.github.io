{"pageProps":{"post":{"title":"Redis Caching Strategy at Scale: Beyond Simple Key-Value","description":"Cache stampede, penetration, avalanche, eviction policy selection, clustering, and persistence trade-offs for production Redis deployments. With Java examples and a real production incident walkthrough.","date":"2025-05-03","category":"Databases","tags":["redis","caching","java","spring boot","performance","distributed systems","cache stampede"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"redis-caching-strategy-at-scale","readingTime":"11 min read","excerpt":"Every senior engineer has fought a caching bug that looked simple and turned out to be a distributed systems problem. Cache stampedes, thundering herds, avalanche failures — these happen at scale and they are expensive. …","contentHtml":"<p>Every senior engineer has fought a caching bug that looked simple and turned out to be a distributed systems problem. Cache stampedes, thundering herds, avalanche failures — these happen at scale and they are expensive. This article covers the caching patterns and anti-patterns that separate a cache that works at 100 RPS from one that holds up at 100,000 RPS.</p>\n<h2>Cache-Aside vs Write-Through vs Write-Behind</h2>\n<p><strong>Cache-aside (lazy loading)</strong> is the default pattern: check cache, miss → fetch from DB, populate cache, return.</p>\n<pre><code class=\"language-java\">public Product getProduct(String productId) {\n    // 1. Check cache\n    Product cached = redisTemplate.opsForValue().get(\"product:\" + productId);\n    if (cached != null) return cached;\n\n    // 2. Cache miss: fetch from DB\n    Product product = productRepository.findById(productId)\n        .orElseThrow(() -> new ProductNotFoundException(productId));\n\n    // 3. Populate cache with TTL\n    redisTemplate.opsForValue().set(\"product:\" + productId, product, Duration.ofMinutes(30));\n    return product;\n}\n</code></pre>\n<p><strong>Properties:</strong> Reads are fast after warmup. Cache only holds data that's actually requested (space efficient). Inconsistency window = TTL (or until next write invalidates the key). Cache cold start causes DB load spike — important after deploys.</p>\n<p><strong>Write-through</strong> writes to cache and DB simultaneously on every update:</p>\n<pre><code class=\"language-java\">@Transactional\npublic Product updateProduct(String productId, ProductUpdate update) {\n    Product product = productRepository.findById(productId).orElseThrow();\n    product.apply(update);\n    productRepository.save(product);  // Write to DB\n\n    // Immediately update cache — no stale reads\n    redisTemplate.opsForValue().set(\"product:\" + productId, product, Duration.ofMinutes(30));\n    return product;\n}\n</code></pre>\n<p><strong>Properties:</strong> Cache is always fresh (no inconsistency window). Every write touches both DB and cache, even for data that's never read. Cache warms up on writes, not on reads.</p>\n<p><strong>Write-behind (write-back)</strong> writes to cache immediately and to the DB asynchronously:</p>\n<pre><code class=\"language-java\">public void updateProductPrice(String productId, BigDecimal newPrice) {\n    // Update cache synchronously\n    String key = \"product:\" + productId;\n    Product product = (Product) redisTemplate.opsForValue().get(key);\n    product.setPrice(newPrice);\n    redisTemplate.opsForValue().set(key, product, Duration.ofMinutes(30));\n\n    // Schedule async DB write\n    writeQueue.submit(() -> productRepository.updatePrice(productId, newPrice));\n}\n</code></pre>\n<p><strong>Properties:</strong> Lowest write latency. High risk — if Redis fails before the async write, data is lost. Use only when brief data loss is acceptable (analytics counters, view counts, non-financial metrics).</p>\n<p>For most production services: <strong>cache-aside for reads, write-through or cache invalidation on writes</strong>.</p>\n<h2>Cache Stampede Problem</h2>\n<p>The cache stampede (thundering herd) happens when a popular key expires and many concurrent requests all miss the cache simultaneously, all hitting the database at once:</p>\n<pre><code>T=0: Key \"hot-product-123\" expires (was serving 1000 req/s)\nT=0 to T=200ms: 200 requests miss cache, all query PostgreSQL simultaneously\nPostgreSQL: 200 concurrent queries for the same row\nResult: DB CPU spike, query queue backs up, timeouts cascade\n</code></pre>\n<p>At 1,000 requests/second on a key with a 30-minute TTL, when the key expires, you get ~200 simultaneous DB queries in 200ms.</p>\n<h3>Solution 1: Probabilistic Early Expiry (PER)</h3>\n<p>Recompute the cache before it expires, with probability proportional to how close we are to expiry. Early recompute happens in one thread while others still read the valid (slightly stale) cache:</p>\n<pre><code class=\"language-java\">public Product getProductWithPER(String productId) {\n    String key = \"product:\" + productId;\n    CachedValue&#x3C;Product> cached = getWithTTL(key);\n\n    if (cached == null) {\n        return fetchAndCache(productId);\n    }\n\n    // Probabilistic early expiry\n    long remainingTtlSeconds = cached.ttlSeconds();\n    double fetchTime = 0.1; // 100ms to recompute\n    double beta = 1.0;\n\n    // Recompute if: -fetchTime * beta * ln(random) >= remainingTtl\n    if (-fetchTime * beta * Math.log(Math.random()) >= remainingTtlSeconds) {\n        // Early refresh — only one thread wins this race (via lock below)\n        return fetchAndCacheIfLeader(productId);\n    }\n\n    return cached.value();\n}\n</code></pre>\n<h3>Solution 2: Distributed Lock (Single Recompute)</h3>\n<p>Only one thread recomputes on cache miss; others wait or serve stale:</p>\n<pre><code class=\"language-java\">private static final String LOCK_PREFIX = \"lock:\";\nprivate static final long LOCK_TTL_MS = 5000;\n\npublic Product getProductLocked(String productId) {\n    String valueKey = \"product:\" + productId;\n    String lockKey = LOCK_PREFIX + productId;\n\n    // Fast path: cache hit\n    Product cached = redisTemplate.opsForValue().get(valueKey);\n    if (cached != null) return cached;\n\n    // Try to acquire lock (NX = only set if not exists)\n    Boolean acquired = redisTemplate.opsForValue()\n        .setIfAbsent(lockKey, \"1\", Duration.ofMillis(LOCK_TTL_MS));\n\n    if (Boolean.TRUE.equals(acquired)) {\n        try {\n            // Double-check after acquiring lock\n            cached = redisTemplate.opsForValue().get(valueKey);\n            if (cached != null) return cached;\n\n            // We are the designated recomputer\n            Product product = productRepository.findById(productId).orElseThrow();\n            redisTemplate.opsForValue().set(valueKey, product, Duration.ofMinutes(30));\n            return product;\n        } finally {\n            redisTemplate.delete(lockKey);\n        }\n    } else {\n        // Another thread is recomputing — wait briefly and retry\n        Uninterruptibles.sleepUninterruptibly(100, TimeUnit.MILLISECONDS);\n        return getProductLocked(productId); // Retry — will likely hit cache now\n    }\n}\n</code></pre>\n<h3>Solution 3: Staggered TTL</h3>\n<p>Add random jitter to TTLs so keys in a set don't expire simultaneously:</p>\n<pre><code class=\"language-java\">private Duration jitteredTtl(Duration baseTtl) {\n    long jitterSeconds = ThreadLocalRandom.current()\n        .nextLong(0, baseTtl.toSeconds() / 10); // ±10% jitter\n    return baseTtl.plusSeconds(jitterSeconds);\n}\n\nredisTemplate.opsForValue().set(key, value, jitteredTtl(Duration.ofMinutes(30)));\n</code></pre>\n<h2>Cache Penetration and Avalanche</h2>\n<p><strong>Cache penetration:</strong> Requests for keys that never exist (e.g., <code>user_id=-1</code>, or IDs for deleted entities). These always miss cache and always hit the DB.</p>\n<p><strong>Fix:</strong> Cache negative results with a short TTL, or use a Bloom filter to reject impossible keys upfront:</p>\n<pre><code class=\"language-java\">public Optional&#x3C;User> getUser(Long userId) {\n    if (!bloomFilter.mightContain(userId)) {\n        return Optional.empty(); // Definitely doesn't exist\n    }\n\n    String key = \"user:\" + userId;\n    Object cached = redisTemplate.opsForValue().get(key);\n\n    if (cached instanceof NullSentinel) {\n        return Optional.empty(); // Cached negative result\n    }\n\n    if (cached instanceof User user) {\n        return Optional.of(user);\n    }\n\n    // DB lookup\n    Optional&#x3C;User> user = userRepository.findById(userId);\n    if (user.isEmpty()) {\n        // Cache negative result for 60 seconds\n        redisTemplate.opsForValue().set(key, NullSentinel.INSTANCE, Duration.ofSeconds(60));\n    } else {\n        redisTemplate.opsForValue().set(key, user.get(), Duration.ofMinutes(30));\n    }\n    return user;\n}\n</code></pre>\n<p><strong>Cache avalanche:</strong> Many keys expire simultaneously (same TTL set in a batch job), causing a sudden DB load spike.</p>\n<p><strong>Fix:</strong> Jitter TTLs (shown above). Alternatively, warm the cache before keys expire using a background job that refreshes keys at 80% of their TTL.</p>\n<h2>TTL Strategy Design</h2>\n<p>TTL selection is not guesswork — it's a trade-off between freshness and hit rate.</p>\n<pre><code>TTL decision matrix:\n\nData type              | Update frequency  | Staleness tolerance | Recommended TTL\n-----------------------|-------------------|--------------------|-----------------\nProduct price          | Minutes           | Low (financial)    | 5 minutes\nProduct catalog        | Hours             | Medium             | 1 hour + invalidation\nUser profile           | Daily             | Low                | 30 minutes\nStatic content (i18n)  | Weekly            | High               | 24 hours\nSearch results         | Real-time         | High               | 5 minutes\nSession data           | Per request       | None               | Session timeout\nRate limit counters    | Per request       | None               | Window size (60s)\n</code></pre>\n<p><strong>Invalidation over TTL for write-heavy data:</strong></p>\n<p>When an entity is updated, immediately delete (or update) its cache key rather than waiting for TTL expiry. This requires write operations to know which cache keys to invalidate — a coupling that must be managed carefully.</p>\n<h2>Memory Fragmentation</h2>\n<p>Redis allocates memory using jemalloc. Over time, allocating and freeing keys of varying sizes causes fragmentation — Redis reports 2GB used but actually occupies 3GB of system memory.</p>\n<p>Monitor fragmentation ratio:</p>\n<pre><code class=\"language-bash\">redis-cli INFO memory | grep mem_fragmentation_ratio\n# > 1.5 = high fragmentation, consider restart or activedefrag\n# 1.0–1.5 = normal\n# &#x3C; 1.0 = Redis is using swap (severe problem)\n</code></pre>\n<p>Enable active defragmentation for long-running Redis instances:</p>\n<pre><code>activedefrag yes\nactive-defrag-ignore-bytes 100mb  # Start defrag when 100MB fragmented\nactive-defrag-threshold-lower 10  # Start defrag when fragmentation > 10%\nactive-defrag-threshold-upper 25  # Use max CPU when fragmentation > 25%\n</code></pre>\n<h2>Eviction Policy Comparison</h2>\n<p>When Redis reaches <code>maxmemory</code>, it evicts keys according to the policy:</p>\n<table>\n<thead>\n<tr>\n<th>Policy</th>\n<th>Behavior</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>noeviction</code></td>\n<td>Returns error on writes when full</td>\n<td>When cache full = outage (unacceptable for most)</td>\n</tr>\n<tr>\n<td><code>allkeys-lru</code></td>\n<td>Evict least recently used across all keys</td>\n<td>General-purpose cache</td>\n</tr>\n<tr>\n<td><code>volatile-lru</code></td>\n<td>Evict LRU only from keys with TTL</td>\n<td>Mixed cache + session store</td>\n</tr>\n<tr>\n<td><code>allkeys-lfu</code></td>\n<td>Evict least frequently used</td>\n<td>Workloads with hotspot keys</td>\n</tr>\n<tr>\n<td><code>volatile-ttl</code></td>\n<td>Evict keys closest to expiry</td>\n<td>When you want expiry-driven eviction</td>\n</tr>\n<tr>\n<td><code>allkeys-random</code></td>\n<td>Random eviction</td>\n<td>Uniform access patterns</td>\n</tr>\n</tbody>\n</table>\n<p>For a pure cache: <code>allkeys-lru</code> or <code>allkeys-lfu</code>. <code>allkeys-lfu</code> is better for workloads where a small set of keys is accessed constantly (product catalog, configuration) — LFU keeps hot keys in memory longer than LRU.</p>\n<p>For a mixed cache + session store: <code>volatile-lru</code> — only evicts keys with TTL, protecting session keys that have no TTL.</p>\n<h2>Redis Clustering</h2>\n<p>Redis Cluster shards data across nodes using 16,384 hash slots:</p>\n<pre><code>Redis Cluster (3 primary + 3 replica):\n\nKey: \"product:123\"\nhash_slot = CRC16(\"product:123\") % 16384 = 7483\n\nSlot 7483 is owned by Primary-2\n→ Route request to Primary-2\n\nPrimary-1 (slots 0-5460)          Primary-2 (slots 5461-10922)        Primary-3 (slots 10923-16383)\n     │                                   │                                    │\n     ▼                                   ▼                                    ▼\nReplica-1                           Replica-2                            Replica-3\n</code></pre>\n<p><strong>Cluster limitations:</strong></p>\n<ul>\n<li>Multi-key operations (<code>MGET</code>, <code>MSET</code>, pipeline) only work when all keys are in the same slot</li>\n<li>Use hash tags <code>{user:123}:profile</code> to force co-location: <code>{user:123}</code> is used for slot calculation, so all keys with the same tag go to the same slot</li>\n<li>Transactions (<code>MULTI/EXEC</code>) only work on single nodes — avoid cross-slot transactions</li>\n</ul>\n<pre><code class=\"language-java\">// Hash tags for co-located keys:\nString profileKey = \"{user:\" + userId + \"}:profile\";\nString settingsKey = \"{user:\" + userId + \"}:settings\";\n// Both keys route to the same slot → MGET works\nList&#x3C;Object> results = redisTemplate.opsForValue().multiGet(\n    List.of(profileKey, settingsKey));\n</code></pre>\n<h2>Persistence: RDB vs AOF</h2>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>RDB (Snapshots)</th>\n<th>AOF (Append-Only File)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Recovery</td>\n<td>Point-in-time snapshot</td>\n<td>Replay every write since last snapshot</td>\n</tr>\n<tr>\n<td>Data loss on crash</td>\n<td>Up to snapshot interval (minutes)</td>\n<td>Up to 1 second (with fsync=everysec)</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>Low CPU overhead</td>\n<td>fsync adds ~10% write overhead</td>\n</tr>\n<tr>\n<td>Restart time</td>\n<td>Fast (load snapshot)</td>\n<td>Slow (replay log, can take minutes)</td>\n</tr>\n<tr>\n<td>File size</td>\n<td>Compact</td>\n<td>Grows unbounded, requires AOF rewrite</td>\n</tr>\n</tbody>\n</table>\n<p><strong>For a cache:</strong> RDB only. Losing cache data on crash is acceptable — the cache warms up from the DB.</p>\n<p><strong>For a cache with session data:</strong> AOF with <code>appendfsync everysec</code>. Max 1 second of session data loss on crash.</p>\n<p><strong>For Redis as primary data store (not cache):</strong> AOF with <code>appendfsync always</code> + RDB for backup. Maximum durability, highest write overhead.</p>\n<pre><code># redis.conf for production cache:\nmaxmemory 12gb\nmaxmemory-policy allkeys-lfu\nsave 900 1     # RDB snapshot if 1 key changed in 900s\nsave 300 10    # RDB snapshot if 10 keys changed in 300s\nsave 60 10000  # RDB snapshot if 10000 keys changed in 60s\nappendonly no  # No AOF for pure cache\n</code></pre>\n<h2>Distributed Locks with Redis</h2>\n<p>Redis Redlock is the distributed lock algorithm. For single-node Redis or Redis Cluster, a simpler approach:</p>\n<pre><code class=\"language-java\">public &#x3C;T> T withLock(String resource, Duration timeout, Supplier&#x3C;T> task) {\n    String lockKey = \"lock:\" + resource;\n    String lockValue = UUID.randomUUID().toString(); // Unique per lock acquisition\n    boolean acquired = false;\n\n    try {\n        acquired = Boolean.TRUE.equals(\n            redisTemplate.opsForValue()\n                .setIfAbsent(lockKey, lockValue, timeout)\n        );\n\n        if (!acquired) {\n            throw new LockNotAvailableException(\"Resource locked: \" + resource);\n        }\n\n        return task.get();\n    } finally {\n        if (acquired) {\n            // Release only if we own the lock (Lua script ensures atomicity)\n            redisTemplate.execute(\n                RELEASE_LOCK_SCRIPT,\n                Collections.singletonList(lockKey),\n                lockValue\n            );\n        }\n    }\n}\n\n// Lua script for atomic check-and-delete:\nprivate static final RedisScript&#x3C;Long> RELEASE_LOCK_SCRIPT = RedisScript.of(\n    \"if redis.call('get', KEYS[1]) == ARGV[1] then \" +\n    \"    return redis.call('del', KEYS[1]) \" +\n    \"else return 0 end\",\n    Long.class\n);\n</code></pre>\n<p>The Lua script is critical. Without it, two operations occur: <code>GET</code> to verify ownership, then <code>DEL</code>. A different process could acquire the lock between those two operations, and you'd delete their lock.</p>\n<h2>Real World Production Issue</h2>\n<p><strong>System:</strong> E-commerce product catalog service, 50,000 SKUs, Redis Cluster (6 nodes), Spring Boot.</p>\n<p><strong>Incident:</strong> Flash sale launch. At 12:00:00, 50,000 concurrent users loaded the sale page. Cache hit rate: 98%. The remaining 2% — 1,000 users — missed on the most popular sale items because those specific keys had expired at 11:59:58 due to a batch TTL reset job.</p>\n<p>All 1,000 users simultaneously queried PostgreSQL for 3 products. PostgreSQL's connection pool (20 connections) was saturated. Other queries (cart, checkout) backed up. API P99 hit 45 seconds.</p>\n<p><strong>Root causes:</strong></p>\n<ol>\n<li>Batch job set the same TTL on all keys → mass expiry</li>\n<li>No stampede protection</li>\n<li>PostgreSQL connection pool too small for burst</li>\n</ol>\n<p><strong>Fixes applied:</strong></p>\n<ol>\n<li>Jitter added to all TTL values (±10%)</li>\n<li>Probabilistic early expiry for top-100 accessed keys</li>\n<li>PostgreSQL connection pool increased to 50</li>\n<li>Read replicas added; cache miss reads route to replicas</li>\n</ol>\n<p>Sale metrics before/after fix: Cache stampede incidents dropped from 8/month to 0.</p>\n<h2>Monitoring Redis Memory and CPU</h2>\n<pre><code class=\"language-bash\"># Key memory metrics:\nredis-cli INFO memory\n# used_memory_human: 8.50G     → actual data\n# maxmemory_human: 12.00G      → limit\n# mem_fragmentation_ratio: 1.12 → healthy\n# used_memory_rss_human: 9.54G  → OS-reported\n\n# Eviction monitoring:\nredis-cli INFO stats | grep evicted_keys\n# Rising number = memory pressure, increase maxmemory or evict manually\n\n# Slow query log:\nredis-cli CONFIG SET slowlog-log-slower-than 10000  # 10ms threshold\nredis-cli SLOWLOG GET 25  # View last 25 slow commands\n</code></pre>\n<p>Prometheus alert rules:</p>\n<pre><code class=\"language-yaml\"># Memory usage > 80% of maxmemory\nredis_memory_used_bytes / redis_memory_max_bytes > 0.8\n\n# High eviction rate (cache under memory pressure)\nrate(redis_evicted_keys_total[5m]) > 100\n\n# Cache hit rate dropping\nredis_keyspace_hits_total / (redis_keyspace_hits_total + redis_keyspace_misses_total) &#x3C; 0.90\n</code></pre>\n<p>The difference between a cache that helps you and one that causes your worst production incidents is usually 3 things: TTL jitter, stampede protection, and eviction policy selection. Everything else is tuning.</p>\n","tableOfContents":[{"id":"cache-aside-vs-write-through-vs-write-behind","text":"Cache-Aside vs Write-Through vs Write-Behind","level":2},{"id":"cache-stampede-problem","text":"Cache Stampede Problem","level":2},{"id":"solution-1-probabilistic-early-expiry-per","text":"Solution 1: Probabilistic Early Expiry (PER)","level":3},{"id":"solution-2-distributed-lock-single-recompute","text":"Solution 2: Distributed Lock (Single Recompute)","level":3},{"id":"solution-3-staggered-ttl","text":"Solution 3: Staggered TTL","level":3},{"id":"cache-penetration-and-avalanche","text":"Cache Penetration and Avalanche","level":2},{"id":"ttl-strategy-design","text":"TTL Strategy Design","level":2},{"id":"memory-fragmentation","text":"Memory Fragmentation","level":2},{"id":"eviction-policy-comparison","text":"Eviction Policy Comparison","level":2},{"id":"redis-clustering","text":"Redis Clustering","level":2},{"id":"persistence-rdb-vs-aof","text":"Persistence: RDB vs AOF","level":2},{"id":"distributed-locks-with-redis","text":"Distributed Locks with Redis","level":2},{"id":"real-world-production-issue","text":"Real World Production Issue","level":2},{"id":"monitoring-redis-memory-and-cpu","text":"Monitoring Redis Memory and CPU","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring — every node is equal, there's no primary, and data placement is determined by partit…"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you…"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible — rollback a bad deploy and your code is back to the previous version. Database schema changes are st…"}]},"__N_SSG":true}