{"pageProps":{"post":{"title":"Building AI Agents with Tool Use: From Chatbot to Autonomous Agent","description":"Build production AI agents using Claude's tool use API. Learn the agentic loop, error handling, multi-step reasoning, human-in-the-loop patterns, and how to build reliable autonomous systems.","date":"2025-03-23","category":"AI/ML","tags":["ai","agents","claude","tool use","llm","autonomous systems","python"],"featured":false,"affiliateSection":"ai-ml-books","slug":"llm-agents-tool-use","readingTime":"10 min read","excerpt":"A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio…","contentHtml":"<p>A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real information, run calculations, and modify state, it transforms from a text generator into a capable assistant. This article builds production-grade agents that actually work.</p>\n<h2>The Agentic Loop</h2>\n<p>Before writing any code, it helps to see the agentic loop in action at the conceptual level. The model does not have all the information it needs to answer a question — instead, it decides what information to fetch, fetches it, and then decides whether it knows enough or needs to take another step. This trace shows that exact reasoning process for a two-step stock comparison question.</p>\n<pre><code>User request → LLM → Tool call → Execute → Result → LLM → ... → Final answer\n\nExample: \"What's the current price of AAPL and how does it compare to last month?\"\n\nTurn 1:\n  LLM thinks: \"I need to get the current AAPL price\"\n  LLM calls: get_stock_price(ticker=\"AAPL\")\n  Tool returns: {\"price\": 195.42, \"timestamp\": \"2025-03-23T14:30:00Z\"}\n\nTurn 2:\n  LLM thinks: \"Now I need last month's price\"\n  LLM calls: get_stock_price(ticker=\"AAPL\", date=\"2025-02-23\")\n  Tool returns: {\"price\": 182.15, \"timestamp\": \"2025-02-23T21:00:00Z\"}\n\nTurn 3:\n  LLM has both values, computes: (195.42 - 182.15) / 182.15 = +7.3%\n  LLM answers: \"AAPL is currently $195.42, up 7.3% from $182.15 a month ago.\"\n</code></pre>\n<h2>Defining Tools for Claude</h2>\n<p>The quality of your tool descriptions is just as important as the quality of the tool implementations. The model uses the <code>description</code> and <code>input_schema</code> fields to decide when and how to call each tool — vague or incomplete descriptions lead to incorrect or missing tool calls. Think of writing a tool definition as writing documentation for a junior developer who can only read the docstring, never the source code.</p>\n<pre><code class=\"language-python\">from anthropic import Anthropic\n\nclient = Anthropic()\n\n# Tool definitions: describe capabilities to the model\ntools = [\n    {\n        \"name\": \"get_stock_price\",\n        \"description\": \"Get the current or historical price of a stock ticker. \"\n                       \"Returns price and timestamp.\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"ticker\": {\n                    \"type\": \"string\",\n                    \"description\": \"Stock ticker symbol (e.g., AAPL, GOOGL, MSFT)\"\n                },\n                \"date\": {\n                    \"type\": \"string\",\n                    \"description\": \"Date in YYYY-MM-DD format for historical price. \"\n                                   \"Omit for current price.\"\n                }\n            },\n            \"required\": [\"ticker\"]\n        }\n    },\n    {\n        \"name\": \"search_web\",\n        \"description\": \"Search the web for current information. Use when you need \"\n                       \"facts that might be recent or time-sensitive.\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"Search query\"\n                },\n                \"num_results\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Number of results to return (1-10, default 3)\"\n                }\n            },\n            \"required\": [\"query\"]\n        }\n    },\n    {\n        \"name\": \"execute_python\",\n        \"description\": \"Execute Python code and return the output. \"\n                       \"Use for calculations, data processing, or analysis.\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"code\": {\n                    \"type\": \"string\",\n                    \"description\": \"Python code to execute\"\n                }\n            },\n            \"required\": [\"code\"]\n        }\n    }\n]\n</code></pre>\n<h2>Tool Execution Layer</h2>\n<p>With tools defined, you need a layer that routes tool calls to real implementations and handles errors gracefully. Notice the try/except wrapper in <code>execute</code> — when a tool fails, you want to return a descriptive error string rather than crash the whole agent. The model will receive that error as the tool result and can decide to retry with different parameters or explain the failure to the user.</p>\n<pre><code class=\"language-python\">import yfinance as yf\nimport subprocess\nimport json\nfrom datetime import datetime\n\nclass ToolExecutor:\n    \"\"\"Execute tool calls from the model.\"\"\"\n\n    def execute(self, tool_name: str, tool_input: dict) -> str:\n        \"\"\"Route tool call to the appropriate handler.\"\"\"\n        handlers = {\n            \"get_stock_price\": self._get_stock_price,\n            \"search_web\": self._search_web,\n            \"execute_python\": self._execute_python,\n        }\n\n        handler = handlers.get(tool_name)\n        if not handler:\n            return f\"Error: Unknown tool '{tool_name}'\"\n\n        try:\n            return handler(**tool_input)\n        except Exception as e:\n            return f\"Error executing {tool_name}: {str(e)}\"\n\n    def _get_stock_price(self, ticker: str, date: str | None = None) -> str:\n        ticker_obj = yf.Ticker(ticker)\n\n        if date:\n            hist = ticker_obj.history(start=date, end=date, interval=\"1d\")\n            if hist.empty:\n                return f\"No data found for {ticker} on {date}\"\n            price = hist[\"Close\"].iloc[-1]\n            return json.dumps({\"ticker\": ticker, \"price\": round(price, 2), \"date\": date})\n        else:\n            info = ticker_obj.info\n            price = info.get(\"currentPrice\") or info.get(\"regularMarketPrice\")\n            return json.dumps({\n                \"ticker\": ticker,\n                \"price\": price,\n                \"timestamp\": datetime.now().isoformat()\n            })\n\n    def _execute_python(self, code: str) -> str:\n        \"\"\"Execute Python in a sandboxed subprocess.\"\"\"\n        # IMPORTANT: In production, use a proper sandbox (Docker, Firecracker)\n        # This is a simplified example\n        try:\n            result = subprocess.run(\n                [\"python3\", \"-c\", code],\n                capture_output=True, text=True, timeout=30,\n                # Restrict network access in production\n            )\n            if result.returncode != 0:\n                return f\"Error: {result.stderr}\"\n            return result.stdout.strip()\n        except subprocess.TimeoutExpired:\n            return \"Error: Code execution timed out (30s limit)\"\n</code></pre>\n<h2>The Agentic Loop Implementation</h2>\n<p>This is the core of every agent: a loop that calls the model, checks whether it wants to use a tool or give a final answer, executes tools if requested, and feeds results back into the conversation. The <code>max_turns</code> guard is not optional — without it, a confused model can loop indefinitely and exhaust your API budget. Every production agent needs a hard ceiling on iterations.</p>\n<pre><code class=\"language-python\">def run_agent(user_message: str, max_turns: int = 10) -> str:\n    \"\"\"\n    Run the agentic loop until the model returns a final answer\n    or max_turns is reached.\n    \"\"\"\n    executor = ToolExecutor()\n    messages = [{\"role\": \"user\", \"content\": user_message}]\n\n    for turn in range(max_turns):\n        response = client.messages.create(\n            model=\"claude-opus-4-6\",\n            max_tokens=4096,\n            tools=tools,\n            messages=messages\n        )\n\n        # Append assistant's response to conversation\n        messages.append({\"role\": \"assistant\", \"content\": response.content})\n\n        # Check stop reason\n        if response.stop_reason == \"end_turn\":\n            # Model is done — extract and return the text response\n            for block in response.content:\n                if hasattr(block, \"text\"):\n                    return block.text\n            return \"No response generated\"\n\n        elif response.stop_reason == \"tool_use\":\n            # Model wants to call tools — execute them all\n            tool_results = []\n\n            for block in response.content:\n                if block.type == \"tool_use\":\n                    print(f\"  → Calling {block.name}({block.input})\")\n                    result = executor.execute(block.name, block.input)\n                    print(f\"  ← Result: {result[:100]}...\")\n\n                    tool_results.append({\n                        \"type\": \"tool_result\",\n                        \"tool_use_id\": block.id,\n                        \"content\": result\n                    })\n\n            # Return tool results to the model\n            messages.append({\"role\": \"user\", \"content\": tool_results})\n\n        else:\n            return f\"Unexpected stop reason: {response.stop_reason}\"\n\n    return \"Max turns reached without a final answer\"\n\n\n# Test it\nanswer = run_agent(\"What's the market cap of Apple and how does it compare to Microsoft?\")\nprint(answer)\n</code></pre>\n<p>The <code>stop_reason == \"tool_use\"</code> branch is where the magic happens: the model pauses its response mid-generation, your code executes the real tool, and the result is injected back into the conversation as if the model had looked it up itself. The model never hallucinates the answer because it never has to — it just asks for what it needs.</p>\n<h2>Human-in-the-Loop: Approving Actions</h2>\n<p>For agents that take real-world actions (send emails, delete files, charge payments), add confirmation steps.</p>\n<p>Before you give an agent the ability to take irreversible actions, you need a way to gate those actions behind human approval. The pattern below classifies tools by risk level and automatically approves low-risk reads while requiring explicit confirmation for anything that modifies state or has external effects.</p>\n<pre><code class=\"language-python\">from enum import Enum\n\nclass ActionRisk(Enum):\n    LOW = \"low\"       # Read-only, reversible\n    MEDIUM = \"medium\" # Reversible with effort\n    HIGH = \"high\"     # Irreversible, external effects\n\n# Annotate tools with their risk level\nTOOL_RISK = {\n    \"search_web\": ActionRisk.LOW,\n    \"get_stock_price\": ActionRisk.LOW,\n    \"execute_python\": ActionRisk.MEDIUM,\n    \"send_email\": ActionRisk.HIGH,\n    \"delete_file\": ActionRisk.HIGH,\n    \"charge_payment\": ActionRisk.HIGH,\n}\n\nclass SafeToolExecutor:\n\n    def __init__(self, auto_approve_threshold: ActionRisk = ActionRisk.LOW):\n        self.auto_approve_threshold = auto_approve_threshold\n        self.base_executor = ToolExecutor()\n\n    def execute(self, tool_name: str, tool_input: dict) -> str:\n        risk = TOOL_RISK.get(tool_name, ActionRisk.HIGH)\n\n        # Auto-approve low-risk actions\n        if risk.value &#x3C;= self.auto_approve_threshold.value:\n            return self.base_executor.execute(tool_name, tool_input)\n\n        # Require human approval for high-risk actions\n        approved = self._request_approval(tool_name, tool_input, risk)\n        if not approved:\n            return f\"Action declined by user: {tool_name}\"\n\n        return self.base_executor.execute(tool_name, tool_input)\n\n    def _request_approval(self, tool_name: str, tool_input: dict,\n                          risk: ActionRisk) -> bool:\n        print(f\"\\n⚠️  [{risk.value.upper()} RISK] Agent wants to: {tool_name}\")\n        print(f\"   Parameters: {json.dumps(tool_input, indent=2)}\")\n        response = input(\"Approve? [y/N]: \").strip().lower()\n        return response == 'y'\n</code></pre>\n<p>The default of <code>auto_approve_threshold = ActionRisk.LOW</code> means only reads are automatic — the agent has to ask permission before it executes code or sends anything. In a web application, you would replace the <code>input()</code> call with a UI prompt that surfaces in the user's chat window.</p>\n<h2>Multi-Agent Orchestration</h2>\n<p>Complex tasks benefit from specialized sub-agents.</p>\n<p>Once your single-agent loop is working, you can compose multiple agents together where each one is focused on a narrow capability. The pattern below separates research (web search) from analysis (computation), with an orchestrator that coordinates the two. This separation means each sub-agent's tool list is small and its instructions are focused, which leads to fewer mistakes than giving one agent every possible tool at once.</p>\n<pre><code class=\"language-python\">class ResearchAgent:\n    \"\"\"Specialized agent for information gathering.\"\"\"\n\n    def research(self, topic: str) -> str:\n        return run_agent(\n            f\"Research this topic comprehensively: {topic}. \"\n            \"Use web search to find current information. \"\n            \"Return a structured summary with key facts.\",\n            tools=[web_search_tool]  # Only search tools\n        )\n\nclass AnalysisAgent:\n    \"\"\"Specialized agent for data analysis.\"\"\"\n\n    def analyze(self, data: str, question: str) -> str:\n        return run_agent(\n            f\"Analyze this data and answer: {question}\\n\\nData:\\n{data}\",\n            tools=[execute_python_tool]  # Only computation tools\n        )\n\nclass OrchestratorAgent:\n    \"\"\"Coordinates research and analysis sub-agents.\"\"\"\n\n    def __init__(self):\n        self.researcher = ResearchAgent()\n        self.analyst = AnalysisAgent()\n\n    def answer_complex_question(self, question: str) -> str:\n        # Step 1: Research\n        print(\"Phase 1: Researching...\")\n        research_data = self.researcher.research(question)\n\n        # Step 2: Analyze\n        print(\"Phase 2: Analyzing...\")\n        analysis = self.analyst.analyze(research_data, question)\n\n        # Step 3: Synthesize\n        print(\"Phase 3: Synthesizing...\")\n        return client.messages.create(\n            model=\"claude-opus-4-6\",\n            max_tokens=2048,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Question: {question}\\n\\n\"\n                          f\"Research findings:\\n{research_data}\\n\\n\"\n                          f\"Analysis:\\n{analysis}\\n\\n\"\n                          \"Provide a comprehensive, well-structured final answer.\"\n            }]\n        ).content[0].text\n</code></pre>\n<h2>Production Considerations</h2>\n<p>A working agent in a notebook is very different from a reliable agent in production. You need to handle API failures with retry logic, track costs before they surprise you, and monitor for the failure modes that only appear under real-world usage patterns.</p>\n<pre><code class=\"language-python\"># Error handling and retries\nimport time\n\ndef run_agent_with_retry(user_message: str, max_retries: int = 3) -> str:\n    for attempt in range(max_retries):\n        try:\n            return run_agent(user_message)\n        except anthropic.APIStatusError as e:\n            if e.status_code == 529 and attempt &#x3C; max_retries - 1:\n                time.sleep(2 ** attempt)  # Exponential backoff\n            else:\n                raise\n\n# Cost tracking\nclass CostTrackingAgent:\n\n    def __init__(self):\n        self.total_input_tokens = 0\n        self.total_output_tokens = 0\n\n    def run(self, message: str) -> str:\n        # claude-opus-4-6: $15/MTok input, $75/MTok output\n        INPUT_COST_PER_MILLION = 15.0\n        OUTPUT_COST_PER_MILLION = 75.0\n\n        result, usage = run_agent_with_usage(message)\n\n        self.total_input_tokens += usage.input_tokens\n        self.total_output_tokens += usage.output_tokens\n\n        cost = (usage.input_tokens * INPUT_COST_PER_MILLION / 1_000_000 +\n                usage.output_tokens * OUTPUT_COST_PER_MILLION / 1_000_000)\n\n        print(f\"Cost: ${cost:.4f} | Total: ${self.total_cost:.4f}\")\n        return result\n</code></pre>\n<p>The exponential backoff in <code>run_agent_with_retry</code> is important: <code>time.sleep(2 ** attempt)</code> waits 1s, then 2s, then 4s on successive failures. This gives the API time to recover from transient overload without hammering it with immediate retries. The cost tracker is equally important — agentic tasks can consume surprisingly many tokens per turn, and running hundreds of tasks without tracking will produce an unexpected bill.</p>\n<p>The key insight for building reliable agents: <strong>the model is not magic</strong>. It will misuse tools, make reasoning errors, and get stuck in loops. Robust agents have: clear tool descriptions (garbage in → garbage out), tool output validation, max turn limits (prevent infinite loops), error handling that feeds back to the model, and human checkpoints for irreversible actions. Build the safeguards before the features.</p>\n","tableOfContents":[{"id":"the-agentic-loop","text":"The Agentic Loop","level":2},{"id":"defining-tools-for-claude","text":"Defining Tools for Claude","level":2},{"id":"tool-execution-layer","text":"Tool Execution Layer","level":2},{"id":"the-agentic-loop-implementation","text":"The Agentic Loop Implementation","level":2},{"id":"human-in-the-loop-approving-actions","text":"Human-in-the-Loop: Approving Actions","level":2},{"id":"multi-agent-orchestration","text":"Multi-Agent Orchestration","level":2},{"id":"production-considerations","text":"Production Considerations","level":2}]},"relatedPosts":[{"title":"Fine-Tuning LLMs: When to Fine-Tune, When to Prompt","description":"Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.","date":"2025-03-27","category":"AI/ML","tags":["ai","llm","fine-tuning","lora","hugging face","openai","machine learning"],"featured":false,"affiliateSection":"ai-ml-books","slug":"fine-tuning-llms","readingTime":"10 min read","excerpt":"Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning — fo…"},{"title":"Vector Embeddings: The Foundation of Modern AI Applications","description":"Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.","date":"2025-03-11","category":"AI/ML","tags":["ai","embeddings","vector database","semantic search","rag","pgvector","pinecone"],"featured":false,"affiliateSection":"ai-ml-books","slug":"vector-embeddings-deep-dive","readingTime":"11 min read","excerpt":"Every modern AI application — semantic search, RAG, recommendations, duplicate detection — is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantic…"},{"title":"Prompt Engineering: Advanced Techniques for Production LLMs","description":"Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications.","date":"2025-02-26","category":"AI/ML","tags":["ai","llm","prompt engineering","gpt","claude","production"],"featured":false,"affiliateSection":"ai-ml-books","slug":"prompt-engineering-production","readingTime":"11 min read","excerpt":"Most prompt engineering tutorials stop at \"be specific and provide context.\" That's necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-gra…"}]},"__N_SSG":true}