{"pageProps":{"post":{"title":"Redis Beyond Cache: Sorted Sets, Streams, and Pub/Sub Patterns","description":"Redis is far more than a cache. Explore how sorted sets power leaderboards, streams enable event sourcing, and pub/sub enables real-time notifications — with production Java examples.","date":"2025-01-22","category":"Databases","tags":["redis","cache","java","distributed systems","streams"],"featured":true,"affiliateSection":"distributed-systems-books","slug":"redis-beyond-cache","readingTime":"13 min read","excerpt":"Most teams use Redis for one thing: caching. They store objects with a TTL, check the cache before hitting the database, and call it a day. This barely scratches the surface. Redis is a data structure server. Its real po…","contentHtml":"<p>Most teams use Redis for one thing: caching. They store objects with a TTL, check the cache before hitting the database, and call it a day. This barely scratches the surface.</p>\n<p>Redis is a data structure server. Its real power lies in five data structures that solve distributed computing problems elegantly — problems that would otherwise require separate specialized systems. This article covers each with production Java examples using Lettuce (the recommended Redis client for Spring Boot).</p>\n<h2>Setup: Lettuce Configuration</h2>\n<p>Before you can use any of these data structures, you need a properly configured Redis connection. The setup below uses Lettuce, which is the default client bundled with Spring Boot's <code>spring-boot-starter-data-redis</code>. Notice the 2-second command timeout — in production, a Redis call that hangs indefinitely can cascade into a full service outage, so always set a timeout that matches your SLA.</p>\n<pre><code class=\"language-java\">@Configuration\npublic class RedisConfig {\n\n    @Bean\n    public RedisConnectionFactory redisConnectionFactory() {\n        RedisStandaloneConfiguration config = new RedisStandaloneConfiguration(\"localhost\", 6379);\n        LettuceClientConfiguration clientConfig = LettuceClientConfiguration.builder()\n            .commandTimeout(Duration.ofSeconds(2))\n            .build();\n        return new LettuceConnectionFactory(config, clientConfig);\n    }\n\n    @Bean\n    public RedisTemplate&#x3C;String, Object> redisTemplate(RedisConnectionFactory factory) {\n        RedisTemplate&#x3C;String, Object> template = new RedisTemplate&#x3C;>();\n        template.setConnectionFactory(factory);\n        template.setKeySerializer(new StringRedisSerializer());\n        template.setValueSerializer(new GenericJackson2JsonRedisSerializer());\n        return template;\n    }\n}\n</code></pre>\n<p>The <code>GenericJackson2JsonRedisSerializer</code> means your Java objects are stored as JSON in Redis, which keeps them human-readable and debuggable from the Redis CLI — a small choice that saves a lot of pain during incidents.</p>\n<h2>Sorted Sets: Leaderboards and Rate Limiting</h2>\n<p>The sorted set (<code>ZSET</code>) stores members with a floating-point score, automatically maintaining order. Adding, removing, and querying by rank are all O(log N).</p>\n<p>Think of a sorted set like a scoreboard at an arcade: every player has a name (member) and a score, and the board is always kept in order. Redis does all the sorting for you, and any operation — add a score, find someone's rank, get the top 10 — runs in logarithmic time regardless of how many players are on the board.</p>\n<h3>Use Case 1: Real-Time Leaderboard</h3>\n<p>A leaderboard sounds simple until you try to build one with a relational database. Updating a score, querying a rank, and fetching neighbors all require either expensive queries or complex caching logic. With a sorted set, these are single-command operations. The <code>incrementScore</code> call below is atomic — no race conditions when two requests update the same player simultaneously.</p>\n<pre><code class=\"language-java\">@Service\npublic class LeaderboardService {\n\n    private static final String LEADERBOARD_KEY = \"leaderboard:weekly\";\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    // Add or update a player's score — O(log N)\n    public void recordScore(String playerId, double points) {\n        redis.opsForZSet().incrementScore(LEADERBOARD_KEY, playerId, points);\n    }\n\n    // Get top N players with their scores — O(log N + N)\n    public List&#x3C;PlayerRank> getTopPlayers(int count) {\n        Set&#x3C;ZSetOperations.TypedTuple&#x3C;String>> topPlayers =\n            redis.opsForZSet().reverseRangeWithScores(LEADERBOARD_KEY, 0, count - 1);\n\n        List&#x3C;PlayerRank> result = new ArrayList&#x3C;>();\n        int rank = 1;\n        for (ZSetOperations.TypedTuple&#x3C;String> tuple : topPlayers) {\n            result.add(new PlayerRank(rank++, tuple.getValue(), tuple.getScore()));\n        }\n        return result;\n    }\n\n    // Get a specific player's rank — O(log N)\n    public Long getPlayerRank(String playerId) {\n        Long rank = redis.opsForZSet().reverseRank(LEADERBOARD_KEY, playerId);\n        return rank != null ? rank + 1 : null; // Convert 0-indexed to 1-indexed\n    }\n\n    // Get players around a specific player — for \"your neighbors\" feature\n    public List&#x3C;PlayerRank> getNeighbors(String playerId, int range) {\n        Long rank = redis.opsForZSet().reverseRank(LEADERBOARD_KEY, playerId);\n        if (rank == null) return Collections.emptyList();\n\n        long start = Math.max(0, rank - range);\n        long end = rank + range;\n        return getTopPlayers((int)(end - start + 1));\n    }\n}\n</code></pre>\n<p>The <code>getNeighbors</code> method is the kind of feature that would require a complex window function in SQL. In Redis, it's just two commands: find the rank, then slice the sorted set around it.</p>\n<h3>Use Case 2: Sliding Window Rate Limiter</h3>\n<p>A naive rate limiter counts requests in a fixed bucket (e.g., \"max 100 per minute\"). The problem: a user can fire 100 requests at 00:59 and 100 more at 01:01, effectively making 200 requests in 2 seconds. A sliding window fixes this by always counting the last N seconds from the current moment. Sorted sets make this elegant — each request is stored with its timestamp as the score, so pruning old requests is a single range-delete by score.</p>\n<pre><code class=\"language-java\">@Service\npublic class RateLimiter {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    /**\n     * Sliding window rate limiter using sorted sets.\n     * Each request is stored with its timestamp as the score.\n     * Old entries outside the window are pruned on each check.\n     */\n    public boolean isAllowed(String userId, int maxRequests, Duration window) {\n        String key = \"rate_limit:\" + userId;\n        long now = System.currentTimeMillis();\n        long windowStart = now - window.toMillis();\n\n        return redis.execute((RedisCallback&#x3C;Boolean>) connection -> {\n            connection.multi(); // BEGIN transaction\n\n            // Remove entries outside the window\n            connection.zRemRangeByScore(key.getBytes(), 0, windowStart);\n\n            // Count remaining entries\n            connection.zCard(key.getBytes());\n\n            // Add current request\n            connection.zAdd(key.getBytes(), now, String.valueOf(now).getBytes());\n\n            // Set expiry on the key (auto-cleanup)\n            connection.expire(key.getBytes(), window.getSeconds() + 1);\n\n            List&#x3C;Object> results = connection.exec();\n\n            Long currentCount = (Long) results.get(1);\n            return currentCount != null &#x26;&#x26; currentCount &#x3C; maxRequests;\n        });\n    }\n}\n</code></pre>\n<p>The entire check-and-add is wrapped in a Redis transaction (<code>MULTI</code>/<code>EXEC</code>), which guarantees that no other client can sneak a request in between the count and the add. This is the kind of correctness that is very hard to achieve with application-level locking.</p>\n<h2>Streams: Persistent Event Log</h2>\n<p>Redis Streams (added in 5.0) are a persistent, append-only log similar to Kafka partitions — but inside Redis. They support consumer groups, acknowledgment, and pending message tracking.</p>\n<p>If you've used Kafka, Streams will feel familiar: producers append events, consumer groups distribute work, and unacknowledged messages stay in a Pending Entries List (PEL) so they can be retried. The key difference is scale — Redis Streams are the right tool when your event volume fits in memory and you don't want the operational overhead of a separate Kafka cluster.</p>\n<pre><code class=\"language-java\">@Service\npublic class OrderEventStream {\n\n    private static final String STREAM_KEY = \"order-stream\";\n    private static final String CONSUMER_GROUP = \"order-processors\";\n\n    @Autowired\n    private StreamOperations&#x3C;String, Object, Object> streamOps;\n\n    // Producer: publish an event\n    public String publishOrderEvent(OrderEvent event) {\n        Map&#x3C;String, Object> fields = Map.of(\n            \"orderId\", event.getOrderId(),\n            \"status\", event.getStatus(),\n            \"userId\", event.getUserId(),\n            \"amount\", event.getAmount(),\n            \"timestamp\", Instant.now().toString()\n        );\n\n        // XADD order-stream * orderId 123 status PLACED ...\n        // Returns auto-generated message ID: \"1704067200000-0\"\n        RecordId messageId = streamOps.add(STREAM_KEY, fields);\n        log.info(\"Published order event, messageId={}\", messageId);\n        return messageId.toString();\n    }\n\n    // Consumer: read and acknowledge messages\n    public void consumeEvents() {\n        // Create consumer group if not exists\n        try {\n            streamOps.createGroup(STREAM_KEY, ReadOffset.from(\"0\"), CONSUMER_GROUP);\n        } catch (Exception e) {\n            // Group already exists — ignore\n        }\n\n        while (true) {\n            // XREADGROUP GROUP order-processors consumer-1 COUNT 10 BLOCK 2000 STREAMS order-stream >\n            List&#x3C;MapRecord&#x3C;String, Object, Object>> messages = streamOps.read(\n                Consumer.from(CONSUMER_GROUP, \"consumer-1\"),\n                StreamReadOptions.empty().count(10).block(Duration.ofSeconds(2)),\n                StreamOffset.create(STREAM_KEY, ReadOffset.lastConsumed())\n            );\n\n            for (MapRecord&#x3C;String, Object, Object> message : messages) {\n                try {\n                    processOrderEvent(message.getValue());\n                    // Acknowledge successful processing\n                    streamOps.acknowledge(STREAM_KEY, CONSUMER_GROUP, message.getId());\n                } catch (Exception e) {\n                    log.error(\"Failed to process message {}\", message.getId(), e);\n                    // Message stays in PEL (Pending Entries List) — can be reclaimed\n                }\n            }\n        }\n    }\n\n    // Reclaim messages that have been pending too long (crashed consumers)\n    public void reclaimStalledMessages() {\n        // XAUTOCLAIM: claim messages idle > 5 minutes\n        PendingMessages pending = streamOps.pending(\n            STREAM_KEY,\n            Consumer.from(CONSUMER_GROUP, \"consumer-1\"),\n            Range.unbounded(), 100\n        );\n        // Process pending messages with a different consumer\n    }\n}\n</code></pre>\n<p>Notice that <code>acknowledge</code> is only called after successful processing. If a consumer crashes mid-process, the message stays in the PEL and <code>reclaimStalledMessages</code> can hand it to another consumer — this is how you get at-least-once delivery guarantees without a dedicated message broker.</p>\n<h2>Pub/Sub: Real-Time Notifications</h2>\n<p>Redis Pub/Sub is a fire-and-forget messaging system. Publishers send messages to channels; all current subscribers receive them. Messages are <strong>not persisted</strong> — if a subscriber is down, it misses messages.</p>\n<p>Think of Pub/Sub like a radio broadcast: the station transmits at all times, and you only hear it while your radio is on. This is perfect for live notifications where freshness matters more than completeness — telling a user they have a new chat message is only useful while they're online anyway.</p>\n<pre><code class=\"language-java\">// Publisher\n@Service\npublic class NotificationPublisher {\n\n    @Autowired\n    private RedisTemplate&#x3C;String, Object> redisTemplate;\n\n    public void notifyUser(String userId, NotificationEvent event) {\n        String channel = \"notifications:\" + userId;\n        redisTemplate.convertAndSend(channel, event);\n    }\n\n    public void broadcastSystemAlert(AlertEvent alert) {\n        redisTemplate.convertAndSend(\"system-alerts\", alert);\n    }\n}\n\n// Subscriber configuration\n@Configuration\npublic class RedisPubSubConfig {\n\n    @Bean\n    public RedisMessageListenerContainer redisMessageListenerContainer(\n            RedisConnectionFactory factory,\n            NotificationMessageListener listener) {\n        RedisMessageListenerContainer container = new RedisMessageListenerContainer();\n        container.setConnectionFactory(factory);\n\n        // Subscribe to user-specific notifications\n        container.addMessageListener(listener,\n            new PatternTopic(\"notifications:*\")); // Wildcard subscription\n\n        // Subscribe to system alerts\n        container.addMessageListener(listener,\n            new ChannelTopic(\"system-alerts\"));\n\n        return container;\n    }\n}\n\n@Component\npublic class NotificationMessageListener implements MessageListener {\n\n    @Autowired\n    private WebSocketService webSocketService;\n\n    @Override\n    public void onMessage(Message message, byte[] pattern) {\n        String channel = new String(message.getChannel());\n        String body = new String(message.getBody());\n\n        if (channel.startsWith(\"notifications:\")) {\n            String userId = channel.replace(\"notifications:\", \"\");\n            webSocketService.sendToUser(userId, body);\n        } else if (channel.equals(\"system-alerts\")) {\n            webSocketService.broadcast(body);\n        }\n    }\n}\n</code></pre>\n<p><strong>When to use Pub/Sub vs Streams:</strong></p>\n<ul>\n<li><strong>Pub/Sub</strong>: Real-time notifications where it's acceptable to miss messages (user online alerts, live dashboards)</li>\n<li><strong>Streams</strong>: Event sourcing, order processing, any case where you need persistence and guaranteed delivery</li>\n</ul>\n<h2>Distributed Locking with SETNX</h2>\n<p>Distributed locks solve a problem that seems simple but isn't: ensuring only one instance of your application performs a critical operation at a time. Imagine two payment service instances both receiving a retry for the same order — without a lock, you could charge the user twice. Redis's <code>SETNX</code> (Set if Not eXists) combined with a TTL gives you a lock that is both exclusive and self-expiring.</p>\n<pre><code class=\"language-java\">@Service\npublic class DistributedLockService {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    private static final long LOCK_TTL_SECONDS = 30;\n\n    /**\n     * Acquire a distributed lock. Returns lock token if acquired, null if not.\n     * The token is needed to safely release the lock.\n     */\n    public String acquireLock(String resourceId) {\n        String lockKey = \"lock:\" + resourceId;\n        String lockToken = UUID.randomUUID().toString(); // Unique per lock acquisition\n\n        Boolean acquired = redis.opsForValue().setIfAbsent(\n            lockKey,\n            lockToken,\n            Duration.ofSeconds(LOCK_TTL_SECONDS)\n        );\n\n        return Boolean.TRUE.equals(acquired) ? lockToken : null;\n    }\n\n    /**\n     * Release only if we still own the lock. Uses Lua script for atomicity.\n     * Without Lua: check-then-delete is a TOCTOU race condition.\n     */\n    public boolean releaseLock(String resourceId, String lockToken) {\n        String lockKey = \"lock:\" + resourceId;\n\n        String luaScript = \"\"\"\n            if redis.call('get', KEYS[1]) == ARGV[1] then\n                return redis.call('del', KEYS[1])\n            else\n                return 0\n            end\n            \"\"\";\n\n        Long result = redis.execute(\n            new DefaultRedisScript&#x3C;>(luaScript, Long.class),\n            List.of(lockKey),\n            lockToken\n        );\n\n        return Long.valueOf(1).equals(result);\n    }\n\n    // Usage pattern\n    public void processPayment(String orderId) {\n        String lockToken = acquireLock(\"payment:\" + orderId);\n        if (lockToken == null) {\n            throw new ResourceBusyException(\"Payment already being processed for order \" + orderId);\n        }\n\n        try {\n            paymentService.charge(orderId);\n        } finally {\n            releaseLock(\"payment:\" + orderId, lockToken);\n        }\n    }\n}\n</code></pre>\n<p>The Lua script for releasing the lock is the critical piece here: it checks and deletes the key atomically. Without it, there's a window where your lock could expire between the <code>GET</code> check and the <code>DEL</code> — and you'd end up deleting another process's lock. The unique <code>lockToken</code> per acquisition is what prevents this: even if the TTL fires, you can't accidentally release a lock you don't own.</p>\n<h2>HyperLogLog: Counting Unique Visitors at Scale</h2>\n<p>Counting exact unique visitors requires storing every visitor ID — expensive at scale. HyperLogLog estimates cardinality with ~0.81% error using only 12KB regardless of input size.</p>\n<p>Imagine trying to count how many unique people walked through a mall's entrance over a year. You could keep a list of every face, but storing millions of names is expensive. HyperLogLog is like a probabilistic tally counter — it can't tell you exactly who visited, but it can tell you \"roughly 2.3 million unique visitors\" while using the same amount of memory whether you have 100 or 100 million visitors.</p>\n<pre><code class=\"language-java\">@Service\npublic class UniqueVisitorCounter {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    public void trackVisit(String pageId, String visitorId) {\n        String key = \"page_visitors:\" + pageId + \":\" + LocalDate.now();\n        // PFADD: O(1), uses ~12KB regardless of cardinality\n        redis.opsForHyperLogLog().add(key, visitorId);\n        redis.expire(key, Duration.ofDays(30));\n    }\n\n    public long getUniqueVisitors(String pageId) {\n        String key = \"page_visitors:\" + pageId + \":\" + LocalDate.now();\n        // PFCOUNT: returns estimated cardinality with 0.81% error\n        return redis.opsForHyperLogLog().size(key);\n    }\n\n    public long getUniqueVisitorsAcrossPages(List&#x3C;String> pageIds) {\n        String[] keys = pageIds.stream()\n            .map(id -> \"page_visitors:\" + id + \":\" + LocalDate.now())\n            .toArray(String[]::new);\n        // PFCOUNT on multiple keys: union estimate\n        return redis.opsForHyperLogLog().size(keys);\n    }\n}\n</code></pre>\n<p>The 0.81% error rate is the trade-off you're accepting: for a page with 1 million visitors, your count will be off by at most ~8,100. For analytics dashboards where \"approximately 1 million\" is meaningful, this is a worthwhile trade for a 99.9% reduction in memory usage compared to storing exact visitor IDs.</p>\n<h2>Production Configuration</h2>\n<p>Getting Redis working in development is easy; getting it right in production requires deliberate configuration. The settings below cover connection pooling (to avoid connection storms during traffic spikes), cluster topology awareness (so your client can find the right shard after a failover), and eviction policy (which determines what Redis does when memory fills up).</p>\n<pre><code class=\"language-yaml\"># Redis configuration for production\nspring:\n  redis:\n    host: redis-cluster.internal\n    port: 6379\n    timeout: 2000ms\n    lettuce:\n      pool:\n        max-active: 50\n        max-idle: 10\n        min-idle: 5\n        max-wait: 1000ms\n      cluster:\n        refresh:\n          adaptive: true           # Detect cluster topology changes\n          period: 30s\n\n# Key eviction policy — critical for cache usage\n# volatile-lru: evict keys with TTL, LRU order (recommended for mixed cache/persistent)\n# allkeys-lru: evict any key, LRU order (for pure cache)\n# noeviction: reject writes when full (use for persistent data)\n</code></pre>\n<pre><code class=\"language-bash\"># maxmemory and eviction policy (redis.conf or CONFIG SET)\nCONFIG SET maxmemory 8gb\nCONFIG SET maxmemory-policy allkeys-lfu  # LFU: better than LRU for Zipf distributions\n\n# Monitor evicted keys — should be near 0 for persistent data\nINFO stats | grep evicted_keys\n</code></pre>\n<p>If you are using Redis for both caching and persistent data structures (like Streams or sorted sets with leaderboard data), use <code>volatile-lru</code> so only keys with a TTL are evicted — your persistent data stays safe. If Redis is purely a cache, <code>allkeys-lfu</code> is the best modern choice because it evicts the least-frequently-used keys, which handles the real-world \"long tail\" of cached objects better than a strict LRU.</p>\n<h2>When to Use Redis vs What</h2>\n<table>\n<thead>\n<tr>\n<th>Use Case</th>\n<th>Redis Data Structure</th>\n<th>Alternative</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Simple cache</td>\n<td>String</td>\n<td>Memcached</td>\n</tr>\n<tr>\n<td>Session storage</td>\n<td>Hash</td>\n<td>Database</td>\n</tr>\n<tr>\n<td>Leaderboard</td>\n<td>Sorted Set</td>\n<td>PostgreSQL (complex query)</td>\n</tr>\n<tr>\n<td>Rate limiting</td>\n<td>Sorted Set / String</td>\n<td>API Gateway</td>\n</tr>\n<tr>\n<td>Pub/Sub</td>\n<td>Pub/Sub</td>\n<td>WebSockets (no persistence)</td>\n</tr>\n<tr>\n<td>Event log</td>\n<td>Stream</td>\n<td>Kafka (for high volume)</td>\n</tr>\n<tr>\n<td>Unique count</td>\n<td>HyperLogLog</td>\n<td>Exact count in DB</td>\n</tr>\n<tr>\n<td>Distributed lock</td>\n<td>String (SETNX)</td>\n<td>ZooKeeper</td>\n</tr>\n<tr>\n<td>Queue</td>\n<td>List</td>\n<td>RabbitMQ</td>\n</tr>\n<tr>\n<td>Membership test</td>\n<td>Bloom Filter (RedisBloom)</td>\n<td>Exact set</td>\n</tr>\n</tbody>\n</table>\n<p>Redis's sweet spot is low-latency, high-throughput operations on data structures where the dataset fits in memory. The moment your dataset exceeds available RAM or you need complex query patterns, reach for PostgreSQL or a specialized system.</p>\n<p>The key insight is to <strong>match the data structure to the access pattern</strong> — not to treat Redis as a generic key-value store where everything is a serialized JSON blob.</p>\n","tableOfContents":[{"id":"setup-lettuce-configuration","text":"Setup: Lettuce Configuration","level":2},{"id":"sorted-sets-leaderboards-and-rate-limiting","text":"Sorted Sets: Leaderboards and Rate Limiting","level":2},{"id":"use-case-1-real-time-leaderboard","text":"Use Case 1: Real-Time Leaderboard","level":3},{"id":"use-case-2-sliding-window-rate-limiter","text":"Use Case 2: Sliding Window Rate Limiter","level":3},{"id":"streams-persistent-event-log","text":"Streams: Persistent Event Log","level":2},{"id":"pubsub-real-time-notifications","text":"Pub/Sub: Real-Time Notifications","level":2},{"id":"distributed-locking-with-setnx","text":"Distributed Locking with SETNX","level":2},{"id":"hyperloglog-counting-unique-visitors-at-scale","text":"HyperLogLog: Counting Unique Visitors at Scale","level":2},{"id":"production-configuration","text":"Production Configuration","level":2},{"id":"when-to-use-redis-vs-what","text":"When to Use Redis vs What","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring — every node is equal, there's no primary, and data placement is determined by partit…"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you…"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible — rollback a bad deploy and your code is back to the previous version. Database schema changes are st…"}]},"__N_SSG":true}