{"pageProps":{"post":{"title":"System Design: Building a URL Shortener That Handles Billions of Requests","description":"A complete system design walkthrough for a URL shortener — from requirements and storage estimates through hashing strategy, caching architecture, and global deployment patterns.","date":"2025-01-29","category":"System Design","tags":["system design","distributed systems","databases","caching","aws"],"featured":true,"affiliateSection":"system-design-courses","slug":"system-design-url-shortener","readingTime":"12 min read","excerpt":"The URL shortener is the \"Hello World\" of system design interviews — but most candidates treat it superficially. The interesting parts are not in generating a short code; they're in the numbers that reveal the true scale…","contentHtml":"<p>The URL shortener is the \"Hello World\" of system design interviews — but most candidates treat it superficially. The interesting parts are not in generating a short code; they're in the numbers that reveal the true scale, the caching strategy that makes redirects feel instant, and the analytics pipeline that handles billions of click events without slowing down the redirect path.</p>\n<p>Let's build bit.ly.</p>\n<h2>Requirements</h2>\n<p><strong>Functional:</strong></p>\n<ul>\n<li>Create a short URL from a long URL (with optional custom alias)</li>\n<li>Redirect short URL to the original long URL</li>\n<li>Set optional expiry on URLs</li>\n<li>Track click analytics (count, geo, referrer, device)</li>\n</ul>\n<p><strong>Non-Functional:</strong></p>\n<ul>\n<li>100M new URLs created per day</li>\n<li>10:1 read-to-write ratio (1B redirects/day)</li>\n<li>Redirect latency p99 &#x3C; 50ms globally</li>\n<li>High availability (99.99% uptime — ~52 minutes downtime/year)</li>\n<li>URL data retained for 5 years</li>\n</ul>\n<h2>Back-of-Envelope Calculations</h2>\n<p>Before writing any code, you need to understand the numbers. This calculation reveals the most important architectural constraint: the system is overwhelmingly read-heavy, which means caching is not an optimization — it's the core design requirement.</p>\n<pre><code>Write throughput:\n  100M URLs/day ÷ 86,400 sec/day = ~1,160 writes/sec\n  Peak (10x): ~12,000 writes/sec\n\nRead throughput:\n  1B redirects/day = ~11,600 reads/sec\n  Peak: ~115,000 reads/sec\n\nStorage (5 years):\n  100M URLs/day × 365 days × 5 years = 182.5B URLs\n  Per URL record: 500 bytes (URL + metadata + indexes)\n  Total: 182.5B × 500 = ~91 TB\n\nCache:\n  Pareto principle: 20% of URLs generate 80% of traffic\n  Hot URLs to cache: 20% of daily URLs = 20M\n  Cache storage: 20M × 500 bytes = ~10 GB (fits on a single Redis node)\n\nShort code namespace:\n  Using Base62 (a-z, A-Z, 0-9) with 7 characters:\n  62^7 = 3.5 trillion unique codes (covers 182.5B URLs with room to spare)\n</code></pre>\n<p>Notice the asymmetry: reads outpace writes 100:1 at peak. This single insight drives nearly every architectural decision you'll make — from separate read and write APIs to the three-tier caching strategy described later.</p>\n<h2>API Design</h2>\n<p>Your API surface should be minimal and purpose-driven. The three core endpoints map directly to the three user actions: create a short link, follow it, and measure its impact. Notice that the redirect uses a <code>302</code> status code rather than <code>301</code> — a deliberate choice explained in the redirect implementation section.</p>\n<pre><code>POST /api/v1/urls\nBody: { \"longUrl\": \"https://...\", \"customAlias\": \"my-link\", \"expiresAt\": \"2026-01-01\" }\nResponse: { \"shortUrl\": \"https://codesprt.pro/ab3Xk2p\", \"shortCode\": \"ab3Xk2p\" }\n\nGET /{shortCode}\nResponse: 302 Redirect to long URL\nHeaders: Location: https://original-url.com\n\nDELETE /api/v1/urls/{shortCode}\nAuth: Bearer token (only URL owner can delete)\n\nGET /api/v1/urls/{shortCode}/analytics\nResponse: { \"totalClicks\": 15420, \"uniqueVisitors\": 8930, \"clicksByDay\": [...] }\n</code></pre>\n<h2>Database Schema</h2>\n<p>Your database schema needs to serve two very different workloads: the URL lookup (read path) and click analytics (write-heavy, time-series). Keeping them in the same table — or even the same database — would let the analytics writes compete with redirect reads. The schema below separates these concerns explicitly.</p>\n<pre><code class=\"language-sql\">-- URLs table (primary storage)\nCREATE TABLE urls (\n    id            BIGINT PRIMARY KEY,          -- Auto-increment or Snowflake ID\n    short_code    VARCHAR(10) UNIQUE NOT NULL, -- \"ab3Xk2p\"\n    long_url      TEXT NOT NULL,               -- Original URL (up to 2048 chars)\n    user_id       BIGINT,                      -- NULL for anonymous\n    created_at    TIMESTAMP NOT NULL DEFAULT NOW(),\n    expires_at    TIMESTAMP,                   -- NULL = never expires\n    is_active     BOOLEAN NOT NULL DEFAULT TRUE,\n    click_count   BIGINT NOT NULL DEFAULT 0    -- Approximate counter (for display)\n);\n\nCREATE INDEX idx_urls_short_code ON urls(short_code); -- Primary lookup index\nCREATE INDEX idx_urls_user_id ON urls(user_id);       -- User's URL history\nCREATE INDEX idx_urls_expires_at ON urls(expires_at) WHERE expires_at IS NOT NULL;\n\n-- Click events (analytics) — separate table, separate DB\n-- Consider: ClickHouse or Cassandra for write-heavy analytics\nCREATE TABLE click_events (\n    id            UUID DEFAULT gen_random_uuid(),\n    short_code    VARCHAR(10) NOT NULL,\n    clicked_at    TIMESTAMP NOT NULL,\n    ip_hash       VARCHAR(64),                -- Hashed for privacy\n    country       VARCHAR(2),\n    city          VARCHAR(100),\n    device_type   VARCHAR(20),               -- mobile, desktop, tablet, bot\n    browser       VARCHAR(50),\n    referer       TEXT\n) PARTITION BY RANGE (clicked_at);           -- Monthly partitions\n</code></pre>\n<p>The partial index on <code>expires_at WHERE expires_at IS NOT NULL</code> is a subtle but important optimization: it only indexes rows that actually have an expiry, keeping the index small and fast for the cleanup job that runs nightly.</p>\n<h2>Short Code Generation: Why Base62 Over MD5</h2>\n<p>With the schema established, the next question is how to generate the short code itself. This is where most candidates go straight to \"just MD5 the URL,\" but that approach has fundamental problems at scale. Here's a comparison of the real options and why each matters.</p>\n<p><strong>Option A: MD5 hash of URL</strong> — Take first 7 characters of MD5(long_url)</p>\n<ul>\n<li>Problem: Two identical long URLs produce the same short code</li>\n<li>Problem: Collision probability grows with scale (birthday problem)</li>\n<li>Problem: Must check database for collision on every create — expensive</li>\n</ul>\n<p><strong>Option B: Auto-increment ID encoded in Base62</strong> — Generate an auto-increment ID, encode to Base62</p>\n<ul>\n<li>ID 1 → \"1\", ID 10,000 → \"2bI\", ID 3,500,000,000 → \"DTRK4\"</li>\n<li>Collision-free by design — each ID is unique</li>\n<li>No database read-before-write needed</li>\n<li>Predictable (slightly guessable) — acceptable for public short links</li>\n</ul>\n<p><strong>Option C: Snowflake ID</strong> — 64-bit unique ID with timestamp + worker ID + sequence</p>\n<ul>\n<li>Globally unique across distributed systems without coordination</li>\n<li>Not guessable (includes worker ID and microseconds)</li>\n<li>Encodes to 7-8 Base62 characters</li>\n</ul>\n<p><strong>Recommended: Option B for simplicity, Option C for security.</strong></p>\n<p>The following implementation shows how a Base62 encoder works. Think of it like converting a decimal number to a different base — the same way you convert binary to hexadecimal — except here your \"digits\" are 62 characters from a-z, A-Z, and 0-9.</p>\n<pre><code class=\"language-java\">public class Base62Encoder {\n\n    private static final String ALPHABET = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\";\n\n    public static String encode(long id) {\n        if (id == 0) return String.valueOf(ALPHABET.charAt(0));\n        StringBuilder sb = new StringBuilder();\n        while (id > 0) {\n            sb.append(ALPHABET.charAt((int)(id % 62)));\n            id /= 62;\n        }\n        return sb.reverse().toString();\n    }\n\n    public static long decode(String code) {\n        long id = 0;\n        for (char c : code.toCharArray()) {\n            id = id * 62 + ALPHABET.indexOf(c);\n        }\n        return id;\n    }\n}\n\n// Usage\nlong nextId = idGenerator.nextId(); // Redis INCR or DB sequence\nString shortCode = Base62Encoder.encode(nextId); // \"ab3Xk2p\"\n</code></pre>\n<p>Notice that <code>decode</code> is the inverse operation: it lets you reconstruct the original database row ID from any short code, which is useful for debugging and for analytics joins.</p>\n<h2>System Architecture</h2>\n<p>With your data model and encoding strategy in place, here is how all the components fit together. The key insight is that reads and writes are handled by entirely separate API services — this lets you scale them independently and protects the high-throughput read path from any slowness in the write path.</p>\n<pre><code>                        ┌─────────────────────────┐\n                        │      CloudFront CDN       │\n                        │  (caches 301 redirects)   │\n                        └────────────┬────────────┘\n                                     │\n                        ┌────────────▼────────────┐\n                        │    Application Load       │\n                        │    Balancer (ALB)         │\n                        └──┬──────────────────┬──┘\n                           │                  │\n               ┌───────────▼──┐          ┌────▼────────────┐\n               │  Write API   │          │   Read API       │\n               │  (URL create)│          │   (redirects)    │\n               │  ~1.2K rps   │          │   ~115K rps      │\n               └──────┬───────┘          └────┬────────────┘\n                      │                       │\n         ┌────────────▼───┐     ┌─────────────▼───────────┐\n         │   PostgreSQL   │     │     Redis Cluster        │\n         │   Primary      │     │   (URL cache, ~10GB)     │\n         │   (writes)     │◄────┤   Cache-hit rate: >95%   │\n         └────────────────┘     └─────────────────────────┘\n                  │                         │ cache miss\n         ┌────────▼────────┐      ┌─────────▼────────────┐\n         │  PG Read        │      │   PostgreSQL Read     │\n         │  Replicas (2x)  │◄─────┤   Replicas           │\n         └─────────────────┘      └──────────────────────┘\n                                           │\n                              ┌────────────▼──────────────┐\n                              │   Analytics Pipeline       │\n                              │  Kafka → ClickHouse        │\n                              └───────────────────────────┘\n</code></pre>\n<p>Notice how the analytics pipeline sits entirely off the critical read path. Click events are written asynchronously to Kafka, which means a slow analytics write can never delay a redirect response.</p>\n<h2>The Critical Path: Redirect in Under 50ms</h2>\n<p>The redirect is the most latency-sensitive operation in the system. Think of it like a highway toll booth: you need the transaction to complete in under a second, or traffic backs up. The implementation below achieves this by checking Redis first (1-2ms), falling back to the database only on a miss, and dispatching analytics entirely asynchronously so it never adds to your response time.</p>\n<pre><code class=\"language-java\">@RestController\npublic class RedirectController {\n\n    @Autowired\n    private RedisTemplate&#x3C;String, String> redis;\n\n    @Autowired\n    private UrlRepository urlRepository;\n\n    @Autowired\n    private KafkaTemplate&#x3C;String, ClickEvent> kafkaTemplate;\n\n    @GetMapping(\"/{shortCode}\")\n    public ResponseEntity&#x3C;Void> redirect(\n            @PathVariable String shortCode,\n            HttpServletRequest request) {\n\n        String cacheKey = \"url:\" + shortCode;\n\n        // Step 1: Redis lookup (~1-2ms)\n        String longUrl = redis.opsForValue().get(cacheKey);\n\n        if (longUrl == null) {\n            // Step 2: Database lookup (cache miss, ~5-10ms)\n            Url url = urlRepository.findByShortCode(shortCode)\n                .filter(u -> u.isActive())\n                .filter(u -> u.getExpiresAt() == null || u.getExpiresAt().isAfter(Instant.now()))\n                .orElseThrow(() -> new UrlNotFoundException(shortCode));\n\n            longUrl = url.getLongUrl();\n\n            // Populate cache (24h TTL, refresh on access)\n            redis.opsForValue().set(cacheKey, longUrl, Duration.ofHours(24));\n        }\n\n        // Step 3: Async analytics (never in critical path)\n        String finalUrl = longUrl;\n        CompletableFuture.runAsync(() ->\n            kafkaTemplate.send(\"click-events\", ClickEvent.builder()\n                .shortCode(shortCode)\n                .timestamp(Instant.now())\n                .ipHash(hashIp(request.getRemoteAddr()))\n                .userAgent(request.getHeader(\"User-Agent\"))\n                .referer(request.getHeader(\"Referer\"))\n                .build())\n        );\n\n        // Step 4: Redirect (302 = don't cache, 301 = browser caches)\n        return ResponseEntity.status(HttpStatus.FOUND)\n            .header(HttpHeaders.LOCATION, finalUrl)\n            .build();\n    }\n}\n</code></pre>\n<p>The <code>302</code> vs <code>301</code> choice is worth understanding deeply: a <code>301</code> tells browsers to cache the redirect permanently, reducing server load but making it impossible to update or expire the URL without clearing browser caches across all users. A <code>302</code> ensures every redirect hits your servers, which is why <code>301</code> is only appropriate for truly permanent, immutable links.</p>\n<p><strong>Total redirect time breakdown:</strong></p>\n<ul>\n<li>Redis hit: ~2ms</li>\n<li>Redis miss + DB read: ~12ms</li>\n<li>Response serialization: ~1ms</li>\n<li>Network: depends on region</li>\n</ul>\n<h2>Caching Strategy</h2>\n<p>Now that you understand the critical path, the caching strategy becomes clear: you want to intercept requests as close to the user as possible, at each of three layers. Think of this like a library's book retrieval system — you first check if the book is on your desk (Redis), then the local shelf (read replica), and only then send a request to the central archive (primary database).</p>\n<pre><code>Three-tier caching for redirects:\n\nTier 1: CDN (CloudFront)\n  - Cache 301 redirects at edge nodes (50+ global PoPs)\n  - TTL: match URL expiry, or 24h for non-expiring URLs\n  - Reduces latency to &#x3C;10ms globally for hot URLs\n  - Invalidate via CloudFront API on URL deletion\n\nTier 2: Redis Cluster\n  - 10GB cluster covers top 20M URLs (80% of traffic)\n  - Eviction: allkeys-lfu (keeps frequently-accessed URLs)\n  - TTL: 24 hours, refreshed on access\n  - Replication: 1 primary + 2 replicas per shard\n\nTier 3: PostgreSQL Read Replicas\n  - 2 replicas with connection pooling (PgBouncer)\n  - Only for cache misses (&#x3C;5% of traffic)\n  - Handles ~5,000 queries/sec comfortably\n</code></pre>\n<p>The choice of <code>allkeys-lfu</code> (Least Frequently Used) eviction in Redis is deliberate: it keeps the URLs that receive the most ongoing traffic, rather than the most recently added ones (<code>allkeys-lru</code>). This ensures viral links stay in cache even if they were created weeks ago.</p>\n<h2>Analytics: ClickHouse for Sub-Second Queries on Billions of Rows</h2>\n<p>PostgreSQL cannot efficiently aggregate billions of click events. ClickHouse, a column-oriented OLAP database, handles this natively. The distinction matters because PostgreSQL stores data row-by-row (great for fetching complete records), while ClickHouse stores data column-by-column (great for aggregating one column across billions of rows). When you run <code>COUNT(*)</code> on a 90-day click log, ClickHouse only reads the <code>clicked_at</code> column — not the entire row — which is why it stays fast at scale.</p>\n<pre><code class=\"language-sql\">-- ClickHouse schema\nCREATE TABLE click_events (\n    short_code    LowCardinality(String),   -- dictionary-encoded for efficiency\n    clicked_at    DateTime,\n    country       LowCardinality(String),\n    device_type   LowCardinality(String),\n    browser       LowCardinality(String),\n    ip_hash       FixedString(32)\n) ENGINE = MergeTree()\nPARTITION BY toYYYYMM(clicked_at)\nORDER BY (short_code, clicked_at);\n\n-- Analytics query: 30-day click trend — runs in &#x3C;1 second on billions of rows\nSELECT\n    toDate(clicked_at) AS date,\n    count() AS total_clicks,\n    uniqExact(ip_hash) AS unique_visitors,\n    countIf(device_type = 'mobile') AS mobile_clicks\nFROM click_events\nWHERE short_code = 'ab3Xk2p'\n  AND clicked_at >= now() - INTERVAL 30 DAY\nGROUP BY date\nORDER BY date;\n</code></pre>\n<p>The <code>LowCardinality(String)</code> type is a ClickHouse optimization worth noting: for columns with a small number of distinct values (like <code>device_type</code> or <code>country</code>), ClickHouse applies dictionary encoding automatically, reducing storage by up to 10x and accelerating GROUP BY operations significantly.</p>\n<h2>Edge Cases That Matter in Interviews</h2>\n<p>With the happy path designed, you need to handle the failure modes. These edge cases are not hypothetical — each represents a real abuse vector or operational issue that production URL shorteners encounter daily.</p>\n<p><strong>1. Custom alias collision:</strong></p>\n<p>When a user requests a custom alias, you must check for uniqueness before committing, because unlike auto-generated codes, custom aliases can conflict with existing entries.</p>\n<pre><code class=\"language-java\">if (customAlias != null) {\n    if (urlRepository.existsByShortCode(customAlias)) {\n        throw new AliasAlreadyTakenException(customAlias);\n    }\n    shortCode = customAlias;\n}\n</code></pre>\n<p><strong>2. URL validation:</strong></p>\n<p>Without validation, your shortener becomes a free proxy for phishing and malware distribution. Checking that the target URL is not your own domain prevents redirect loops, while the safety check guards against abuse.</p>\n<pre><code class=\"language-java\">// Prevent redirect loops and abuse\nprivate void validateUrl(String longUrl) {\n    if (longUrl.contains(\"codesprt.pro\")) {\n        throw new InvalidUrlException(\"Cannot shorten our own URLs\");\n    }\n    // Check against malware/phishing blocklist\n    if (safetyCheckService.isMalicious(longUrl)) {\n        throw new MaliciousUrlException();\n    }\n}\n</code></pre>\n<p><strong>3. Expired URL cleanup:</strong></p>\n<p>Expired URLs should never be hard-deleted, because the analytics records still reference them. A nightly soft-delete sets <code>is_active = false</code> so the redirect path rejects the code while the analytics data remains intact.</p>\n<pre><code class=\"language-java\">@Scheduled(cron = \"0 0 2 * * *\") // 2 AM daily\npublic void deactivateExpiredUrls() {\n    urlRepository.deactivateExpired(Instant.now()); // Soft delete\n    // Don't hard delete — analytics need the record\n}\n</code></pre>\n<p><strong>4. Rate limiting URL creation:</strong></p>\n<pre><code class=\"language-java\">// 100 URLs/day for free tier, 10,000/day for pro\n// Redis sliding window: see rate limiter article\n</code></pre>\n<p>The URL shortener is a microcosm of distributed systems challenges: write idempotency, cache invalidation, async event processing, and global latency targets. Nail these, and you've demonstrated the architectural thinking that matters.</p>\n","tableOfContents":[{"id":"requirements","text":"Requirements","level":2},{"id":"back-of-envelope-calculations","text":"Back-of-Envelope Calculations","level":2},{"id":"api-design","text":"API Design","level":2},{"id":"database-schema","text":"Database Schema","level":2},{"id":"short-code-generation-why-base62-over-md5","text":"Short Code Generation: Why Base62 Over MD5","level":2},{"id":"system-architecture","text":"System Architecture","level":2},{"id":"the-critical-path-redirect-in-under-50ms","text":"The Critical Path: Redirect in Under 50ms","level":2},{"id":"caching-strategy","text":"Caching Strategy","level":2},{"id":"analytics-clickhouse-for-sub-second-queries-on-billions-of-rows","text":"Analytics: ClickHouse for Sub-Second Queries on Billions of Rows","level":2},{"id":"edge-cases-that-matter-in-interviews","text":"Edge Cases That Matter in Interviews","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}