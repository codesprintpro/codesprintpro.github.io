{"pageProps":{"post":{"title":"Kafka Exactly-Once Semantics: Myth vs Production Reality","description":"What Kafka's exactly-once guarantee actually covers, where duplicates still happen in practice, and how to design genuinely idempotent consumers with Spring Kafka. Real production mistakes and their fixes.","date":"2025-04-20","category":"Messaging","tags":["kafka","exactly-once","spring kafka","distributed systems","transactions","java"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"kafka-exactly-once-semantics","readingTime":"9 min read","excerpt":"Kafka 0.11 introduced exactly-once semantics (EOS), and every architecture diagram since then has confidently placed a checkbox next to \"exactly once delivery.\" In practice, most teams deploying Kafka with EOS still see …","contentHtml":"<p>Kafka 0.11 introduced exactly-once semantics (EOS), and every architecture diagram since then has confidently placed a checkbox next to \"exactly once delivery.\" In practice, most teams deploying Kafka with EOS still see duplicates in production. The issue is that Kafka's exactly-once guarantee is real and precise — but it covers a narrower scope than most engineers assume.</p>\n<p>This article explains exactly what the guarantee covers, where it breaks, and what you must implement yourself to actually achieve idempotent processing at the system level.</p>\n<h2>What Exactly-Once Really Means</h2>\n<p>Kafka's exactly-once guarantee applies specifically to the <strong>read-process-write</strong> loop within the Kafka ecosystem:</p>\n<pre><code>Exactly-once scope:\n┌─────────────────────────────────────────────────────┐\n│                                                     │\n│  Consumer reads from Topic A                        │\n│       │                                             │\n│       ▼                                             │\n│  Processes message (transforms, aggregates)         │\n│       │                                             │\n│       ▼                                             │\n│  Writes result to Topic B + commits offset atomically│\n│                                                     │\n│  ← Kafka guarantees this is atomic and exactly once │\n└─────────────────────────────────────────────────────┘\n\nNOT covered:\n- Writing to an external database\n- Calling an external API\n- Any side effect outside Kafka's transaction coordinator\n</code></pre>\n<p>If your processing loop writes to PostgreSQL, sends an email, or calls a payment gateway, Kafka's EOS guarantee does not extend to those operations. You must implement idempotency for those side effects yourself.</p>\n<h2>Producer Idempotence</h2>\n<p>Producer idempotence (<code>enable.idempotence=true</code>) prevents duplicate messages caused by producer retries. Without it:</p>\n<pre><code>Producer → Broker: publish(msg1) [network timeout]\nProducer → Broker: retry publish(msg1)  ← duplicate!\nBroker commits both copies\n</code></pre>\n<p>With idempotence enabled, each producer instance gets a <code>ProducerID (PID)</code> and each message gets a monotonically increasing sequence number. The broker tracks <code>(PID, partition, sequence_number)</code> tuples and deduplicates retries:</p>\n<pre><code class=\"language-java\">Properties props = new Properties();\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka:9092\");\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n// Implied by idempotence=true:\n// acks=all, max.in.flight.requests.per.connection=5, retries=MAX_INT\nprops.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);\n\nKafkaProducer&#x3C;String, String> producer = new KafkaProducer&#x3C;>(props);\n</code></pre>\n<p><strong>Important:</strong> Idempotence is per-session. If the producer restarts, it gets a new PID. Messages inflight during the restart can be duplicated — there is no deduplication across producer instances.</p>\n<h2>Transactional Producers</h2>\n<p>Transactions extend idempotence to atomic multi-partition writes and atomic offset commits:</p>\n<pre><code class=\"language-java\">@Bean\npublic ProducerFactory&#x3C;String, PaymentEvent> producerFactory() {\n    Map&#x3C;String, Object> config = new HashMap&#x3C;>();\n    config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka:9092\");\n    config.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n    config.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"payment-processor-1\");\n    // transactional.id must be unique per producer instance\n    // Use: service-name + partition-id for stable identity\n    return new DefaultKafkaProducerFactory&#x3C;>(config);\n}\n\n// Transactional send:\n@Transactional\npublic void processAndForward(ConsumerRecord&#x3C;String, PaymentEvent> record) {\n    PaymentEvent event = record.value();\n    PaymentResult result = paymentService.process(event);\n\n    kafkaTemplate.executeInTransaction(ops -> {\n        ops.send(\"payments-processed\", event.getUserId(), result);\n        ops.send(\"audit-log\", event.getPaymentId(), AuditEntry.from(result));\n        return true;\n    });\n    // Offset commit and both sends are atomic\n    // Either all succeed or none are visible to consumers\n}\n</code></pre>\n<p>The <code>transactional.id</code> must be stable across producer restarts. Kafka uses it to recover the previous producer's pending transactions. If you use random IDs, pending transactions from dead producers never resolve.</p>\n<h2>Consumer Offset Management</h2>\n<p>Consumer offsets in Kafka are stored in an internal topic (<code>__consumer_offsets</code>). The offset represents the next message to consume, not the last processed message. The danger:</p>\n<pre><code>Consumer reads message at offset 100\nConsumer processes message (writes to DB)\nConsumer commits offset 101\nConsumer crashes before commit → next read starts at 100 → DUPLICATE PROCESSING\nConsumer crashes after commit → offset is 101, message was processed → OK\n</code></pre>\n<p>The window between processing and offset commit is the duplicate risk window. Making it smaller reduces exposure but never eliminates it.</p>\n<p>With Spring Kafka's <code>@KafkaListener</code>:</p>\n<pre><code class=\"language-java\">@KafkaListener(topics = \"payments\", groupId = \"payment-processor\")\npublic void processPayment(ConsumerRecord&#x3C;String, PaymentEvent> record,\n                           Acknowledgment ack) {\n    try {\n        paymentService.process(record.value());\n        ack.acknowledge();  // Commit offset after successful processing\n    } catch (RetryableException e) {\n        // Don't ack — message will be redelivered\n        throw e;\n    } catch (NonRetryableException e) {\n        ack.acknowledge();  // Commit offset, send to DLQ\n        dlqProducer.send(\"payments-dlq\", record);\n    }\n}\n</code></pre>\n<p>Use <code>AckMode.MANUAL_IMMEDIATE</code> for fine-grained control over when offsets are committed.</p>\n<h2>Failure Cases Where Duplicates Still Happen</h2>\n<h3>Case 1: Consumer Group Rebalance</h3>\n<p>During a rebalance, partitions are reassigned. A consumer processing a message when the rebalance triggers may lose its partition assignment:</p>\n<pre><code>Timeline:\nT=0: Consumer A holds Partition 7, processes message offset 500\nT=1: New consumer joins group → rebalance triggered\nT=2: Partition 7 reassigned to Consumer B\nT=3: Consumer A's processing completes, tries to commit offset 501\nT=4: Commit fails (partition not owned by Consumer A)\nT=5: Consumer B starts reading from last committed offset: 500\nT=6: Duplicate processing of message 500\n</code></pre>\n<p><strong>Fix:</strong> Use <code>CooperativeStickyAssignor</code> to minimize partition movement during rebalances:</p>\n<pre><code class=\"language-java\">props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG,\n    CooperativeStickyAssignor.class.getName());\n</code></pre>\n<p>And implement idempotent consumers (covered below) so duplicates are harmless.</p>\n<h3>Case 2: ISR and Replication Factor Implications</h3>\n<p>With <code>acks=all</code> and <code>min.insync.replicas=2</code>, a message is only acknowledged when it's on at least 2 replicas. If the leader fails after acknowledging but before replicas sync, the message is lost — but the producer got an <code>ACK</code>. With retries, the producer resends, creating a different kind of inconsistency.</p>\n<pre><code>Replication factor: 3, min.insync.replicas: 2\n\nProducer → Broker Leader (ISR: Leader, Replica1, Replica2)\nLeader writes → Replica1 writes → ACK sent to producer ✓\nReplica2 hasn't written yet → Leader fails\nNew leader elected: Replica1 has the message\nReplica2 becomes leader after Replica1 also fails\nReplica2 doesn't have the message → Message lost\nProducer retries → Duplicate (if message was durably committed elsewhere)\n</code></pre>\n<p>Set <code>min.insync.replicas=2</code> with <code>replication.factor=3</code> for the right balance of durability vs availability. Never set <code>min.insync.replicas = replication.factor</code> — one broker failure makes the topic completely unavailable.</p>\n<h3>Case 3: Long Processing + Session Timeout</h3>\n<p>If message processing takes longer than <code>max.poll.interval.ms</code> (default 5 minutes), Kafka considers the consumer dead and triggers a rebalance:</p>\n<pre><code class=\"language-java\">// This is dangerous if processPayment() can take > 5 minutes:\n@KafkaListener(topics = \"payments\")\npublic void processPayment(PaymentEvent event) {\n    processPayment(event); // Could take 10 minutes for complex reconciliation\n}\n\n// Fix: Increase max.poll.interval.ms to cover realistic processing time\nprops.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 600000); // 10 minutes\n// Or: Move long processing to async and ack quickly\n</code></pre>\n<h2>Designing Idempotent Consumers</h2>\n<p>Since duplicates are unavoidable at the system level, the correct approach is idempotent consumers: processing a message twice produces the same result as processing it once.</p>\n<p><strong>Pattern: Idempotency key in the message + deduplication table</strong></p>\n<pre><code class=\"language-java\">// Message contains an idempotency key\npublic record PaymentEvent(\n    String paymentId,         // Idempotency key\n    String userId,\n    BigDecimal amount,\n    String currency\n) {}\n\n// Deduplication table in PostgreSQL\nCREATE TABLE processed_payments (\n    payment_id      VARCHAR(255) PRIMARY KEY,\n    processed_at    TIMESTAMPTZ DEFAULT NOW(),\n    result          JSONB\n);\n\n// Consumer checks before processing:\n@Service\npublic class IdempotentPaymentConsumer {\n\n    @Transactional\n    public void processPayment(PaymentEvent event) {\n        // Attempt insert — fails silently on duplicate\n        int inserted = jdbcTemplate.update(\n            \"INSERT INTO processed_payments (payment_id) VALUES (?) ON CONFLICT DO NOTHING\",\n            event.paymentId()\n        );\n\n        if (inserted == 0) {\n            log.info(\"Duplicate payment event, skipping: {}\", event.paymentId());\n            return;  // Already processed\n        }\n\n        // Process only if not already done\n        PaymentResult result = paymentGateway.charge(event);\n        jdbcTemplate.update(\n            \"UPDATE processed_payments SET result = ?::jsonb WHERE payment_id = ?\",\n            objectMapper.writeValueAsString(result), event.paymentId()\n        );\n    }\n}\n</code></pre>\n<p>The <code>ON CONFLICT DO NOTHING</code> insert is atomic — concurrent duplicates resolve correctly without application-level locking.</p>\n<h2>Retry Topics and DLQ Strategy</h2>\n<pre><code>Retry topic architecture:\n\npayments (main topic)\n    │\n    ▼\nConsumer Group: payment-processor\n    │\n    ├── Success → payments-processed\n    │\n    ├── Retryable failure\n    │       └── payments-retry-1 (delay: 30s via consumer pause)\n    │               └── payments-retry-2 (delay: 5min)\n    │                       └── payments-retry-3 (delay: 30min)\n    │                               └── payments-dlq\n    │\n    └── Non-retryable failure → payments-dlq (immediately)\n</code></pre>\n<p>Spring Kafka's <code>@RetryableTopic</code>:</p>\n<pre><code class=\"language-java\">@RetryableTopic(\n    attempts = \"4\",\n    backoff = @Backoff(delay = 30000, multiplier = 5, maxDelay = 1800000),\n    dltStrategy = DltStrategy.FAIL_ON_ERROR,\n    autoCreateTopics = \"false\",\n    include = {RetryablePaymentException.class}\n)\n@KafkaListener(topics = \"payments\", groupId = \"payment-processor\")\npublic void processPayment(PaymentEvent event) {\n    paymentService.process(event);\n}\n\n@DltHandler\npublic void handleDlt(PaymentEvent event, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) {\n    log.error(\"Message sent to DLT: topic={}, paymentId={}\", topic, event.paymentId());\n    alertingService.sendDltAlert(event);\n    deadLetterRepository.save(DeadLetterRecord.from(event, topic));\n}\n</code></pre>\n<h2>Handling Poison Messages</h2>\n<p>A poison message is a message that consistently fails processing — malformed data, schema mismatch, or an edge case that triggers a bug. Without DLQ handling, poison messages block a partition indefinitely.</p>\n<p>Indicators:</p>\n<ul>\n<li>Consumer lag growing on one partition while others are healthy</li>\n<li>Same offset appearing repeatedly in error logs</li>\n<li>Consumer processing rate drops to 0 on specific partitions</li>\n</ul>\n<p>Always configure a DLQ (<code>dlt-strategy=FAIL_ON_ERROR</code>) with an alert on DLQ topic lag growth. Poison messages in the DLQ should trigger an on-call page and manual investigation.</p>\n<h2>Performance Trade-offs of EOS</h2>\n<p>Transactions add overhead:</p>\n<table>\n<thead>\n<tr>\n<th>Mode</th>\n<th>Throughput (approx)</th>\n<th>Latency</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>No idempotence, acks=1</td>\n<td>Baseline 100%</td>\n<td>Baseline</td>\n</tr>\n<tr>\n<td>Idempotence only, acks=all</td>\n<td>~80%</td>\n<td>+10ms</td>\n</tr>\n<tr>\n<td>Transactions (EOS)</td>\n<td>~40–60%</td>\n<td>+20–50ms</td>\n</tr>\n</tbody>\n</table>\n<p>The overhead comes from:</p>\n<ul>\n<li><code>beginTransaction()</code>/<code>commitTransaction()</code> calls to the transaction coordinator</li>\n<li>Waiting for all ISR replicas to acknowledge (<code>acks=all</code>)</li>\n<li>Fencing previous transactions from zombie producers</li>\n</ul>\n<p>For high-throughput pipelines where the downstream consumer is idempotent anyway, EOS's performance cost often isn't justified. Use idempotent producers + idempotent consumers instead of full EOS transactions.</p>\n<h2>Real Production Mistakes</h2>\n<p><strong>Mistake 1: Sharing a transactional.id across multiple producer instances.</strong> When two pods start with <code>transactional.id=payment-processor</code>, Kafka fences the older one. Your second pod's transactions are rejected. Use <code>payment-processor-${pod.ip}</code> or <code>payment-processor-${partition.id}</code>.</p>\n<p><strong>Mistake 2: Using EOS for Kafka → Database writes and assuming no duplicates.</strong> EOS is Kafka-to-Kafka. The database write is outside the transaction boundary. Always implement idempotency at the database layer regardless of Kafka configuration.</p>\n<p><strong>Mistake 3: Not handling <code>ProducerFencedException</code>.</strong> When a producer is fenced by a newer instance with the same <code>transactional.id</code>, it throws <code>ProducerFencedException</code>. This is not retryable — the producer must be shut down and restarted. Handling this as a generic exception causes infinite retry loops.</p>\n<pre><code class=\"language-java\">try {\n    kafkaTemplate.executeInTransaction(ops -> {\n        ops.send(\"topic\", record);\n        return true;\n    });\n} catch (ProducerFencedException e) {\n    // DO NOT retry — shut down and restart the producer\n    log.error(\"Producer fenced, restarting: {}\", e.getMessage());\n    producerFactory.reset();\n}\n</code></pre>\n<p><strong>Mistake 4: Ignoring rebalance-induced duplicates under load.</strong> Teams test EOS with low throughput where rebalances are rare. Under production load with frequent membership changes (rolling deploys, autoscaling), rebalances happen constantly. Load test with rolling restarts to expose rebalance-induced duplicates.</p>\n<h2>Architecture Diagram</h2>\n<pre><code>Payments EOS Pipeline:\n\n┌──────────────┐     ┌──────────────────┐     ┌──────────────────┐\n│  Payment API │────►│  payments topic  │────►│  EOS Consumer    │\n│  (Producer)  │     │  64 partitions   │     │  Group           │\n│  idempotent  │     │  RF=3, ISR=2     │     │  Transactional   │\n│  acks=all    │     └──────────────────┘     │  writes          │\n└──────────────┘                              └──────┬───────────┘\n                                                     │\n                    ┌────────────────────────────────┼─────────────┐\n                    │                                │             │\n                    ▼                                ▼             ▼\n         ┌──────────────────┐           ┌──────────────┐  ┌──────────────┐\n         │ payments-processed│           │  PostgreSQL  │  │ payments-dlq │\n         │ topic             │           │  (idempotent │  │ (DLQ topic)  │\n         │ (Kafka-to-Kafka   │           │  insert)     │  │              │\n         │  EOS covered)     │           └──────────────┘  └──────────────┘\n         └──────────────────┘           (NOT EOS covered — must be idempotent)\n</code></pre>\n<p>Kafka's exactly-once guarantee is genuine and valuable — for Kafka-to-Kafka pipelines. For everything beyond that, the responsibility shifts to you. The engineers who understand this distinction build reliable systems; the ones who don't spend weekends investigating duplicate payments.</p>\n","tableOfContents":[{"id":"what-exactly-once-really-means","text":"What Exactly-Once Really Means","level":2},{"id":"producer-idempotence","text":"Producer Idempotence","level":2},{"id":"transactional-producers","text":"Transactional Producers","level":2},{"id":"consumer-offset-management","text":"Consumer Offset Management","level":2},{"id":"failure-cases-where-duplicates-still-happen","text":"Failure Cases Where Duplicates Still Happen","level":2},{"id":"case-1-consumer-group-rebalance","text":"Case 1: Consumer Group Rebalance","level":3},{"id":"case-2-isr-and-replication-factor-implications","text":"Case 2: ISR and Replication Factor Implications","level":3},{"id":"case-3-long-processing-session-timeout","text":"Case 3: Long Processing + Session Timeout","level":3},{"id":"designing-idempotent-consumers","text":"Designing Idempotent Consumers","level":2},{"id":"retry-topics-and-dlq-strategy","text":"Retry Topics and DLQ Strategy","level":2},{"id":"handling-poison-messages","text":"Handling Poison Messages","level":2},{"id":"performance-trade-offs-of-eos","text":"Performance Trade-offs of EOS","level":2},{"id":"real-production-mistakes","text":"Real Production Mistakes","level":2},{"id":"architecture-diagram","text":"Architecture Diagram","level":2}]},"relatedPosts":[{"title":"SQS vs Kafka vs EventBridge: Choosing the Right Messaging System on AWS","description":"A senior engineer's guide to selecting between Amazon SQS, Apache Kafka on AWS, and EventBridge. Throughput benchmarks, cost breakdowns, ordering guarantees, and real production trade-offs.","date":"2025-04-02","category":"Messaging","tags":["aws","sqs","kafka","eventbridge","distributed systems","messaging","msk"],"featured":false,"affiliateSection":"aws-resources","slug":"sqs-kafka-eventbridge-aws-comparison","readingTime":"10 min read","excerpt":"Every AWS backend team eventually faces the same decision: you need asynchronous messaging. SQS is right there in the console. Your architect says you need Kafka. Someone from DevOps mentions EventBridge. Each option has…"},{"title":"Kafka Internals Deep Dive: Partitions, Offsets, and Consumer Groups","description":"Understand how Apache Kafka achieves high throughput through log-based storage, how offsets enable reliable consumption, and how consumer groups scale processing horizontally.","date":"2025-01-15","category":"Messaging","tags":["kafka","distributed systems","streaming","java"],"featured":true,"affiliateSection":"distributed-systems-books","slug":"kafka-internals-deep-dive","readingTime":"10 min read","excerpt":"Apache Kafka is the de facto standard for event streaming in distributed systems, but most developers treat it as a black box — a durable message queue with a fancy name. Understanding Kafka's internals unlocks its true …"}]},"__N_SSG":true}