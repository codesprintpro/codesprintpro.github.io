{"pageProps":{"post":{"title":"System Design: Search Autocomplete at Google Scale","description":"Design a typeahead/autocomplete system that returns relevant suggestions in under 100ms for billions of queries. Covers trie vs inverted index, ranking algorithms, and distributed architecture.","date":"2025-02-20","category":"System Design","tags":["system design","search","autocomplete","trie","distributed systems"],"featured":false,"affiliateSection":"system-design-courses","slug":"system-design-autocomplete","readingTime":"11 min read","excerpt":"Search autocomplete — the dropdown that appears as you type — seems simple but is one of the most latency-sensitive features in any product. Google returns suggestions in under 100ms for billions of queries per day. This…","contentHtml":"<p>Search autocomplete — the dropdown that appears as you type — seems simple but is one of the most latency-sensitive features in any product. Google returns suggestions in under 100ms for billions of queries per day. This article designs the system behind that.</p>\n<h2>Requirements</h2>\n<p><strong>Functional:</strong></p>\n<ul>\n<li>Return top 5 suggestions as the user types (after each keystroke)</li>\n<li>Suggestions ranked by historical query frequency + recency</li>\n<li>Personalized suggestions (user's history)</li>\n<li>Support for typo tolerance (fuzzy matching)</li>\n<li>Trending queries bubble up quickly</li>\n</ul>\n<p><strong>Non-Functional:</strong></p>\n<ul>\n<li>10B daily queries → 115,000 queries/sec</li>\n<li>Latency p99 &#x3C; 100ms (including network round-trip)</li>\n<li>High availability (99.99%)</li>\n<li>Suggestions updated from query logs within 10 minutes (near real-time)</li>\n</ul>\n<h2>Data Model: What Are We Searching?</h2>\n<p>Before choosing a data structure, you need to understand what the input and output of the system actually are. The system receives a user's partially typed query and must return the five most relevant completions. The \"most relevant\" part is not about text matching — it is about predicting what the user intends to type, which requires a scoring model built from historical behavior.</p>\n<pre><code>Query log (source of truth):\n  Each search query is recorded with timestamp, user_id, result_click_count.\n\nAggregation pipeline:\n  Raw logs → Count per query → Filter noise → Rank → Index\n\nRanked query store:\n  query: \"java virtual threads\"\n  score: 8,432,100         (weighted: frequency × recency × CTR)\n  updated_at: 2025-02-20\n\nGoal: Given prefix \"java v\", return:\n  1. java virtual threads\n  2. java versions\n  3. java volatile keyword\n  4. java vector api\n  5. java var keyword\n</code></pre>\n<p>The score formula <code>frequency × recency × CTR</code> is the key insight here: a query that was searched a million times two years ago should not outrank a query searched 100,000 times in the last hour if the recent one shows high click-through rate. Weighting these three signals together produces suggestions that feel current and useful rather than historically accurate but stale.</p>\n<h2>Core Data Structure: Trie vs Inverted Index</h2>\n<p>With the data model defined, the next question is how to index it so that a prefix lookup returns the top-5 completions in under 10ms. There are two fundamentally different approaches, and understanding their tradeoffs is what interviewers are really testing here.</p>\n<h3>Option 1: Trie (Prefix Tree)</h3>\n<p>A trie is a tree where each path from root to a leaf spells out a string. It is the most natural data structure for prefix lookups, but its real power — and its main limitation — comes from how you store suggestions at each node.</p>\n<pre><code>Trie for [\"java\", \"java virtual\", \"javascript\"]:\n\nroot\n └─ j\n    └─ a\n       └─ v\n          ├─ a [end: score=8M]\n          │  └─  [space]\n          │      └─ v\n          │         └─ i\n          │            └─ r [end: score=8.4M]\n          └─ a\n             └─ s [end: score=12M]\n</code></pre>\n<p>Each node can store the top-K suggestions for that prefix (precomputed). Lookup: O(prefix_length). Memory: O(total characters × K suggestions per node).</p>\n<p>The critical insight of storing top-K suggestions at every node is what makes the trie usable for autocomplete: instead of traversing all children to find the best suggestions at query time, you precompute the answer during index build and store it directly at the node. A query for \"java v\" returns results in exactly as many steps as there are characters in the prefix.</p>\n<pre><code class=\"language-java\">class TrieNode {\n    Map&#x3C;Character, TrieNode> children = new HashMap&#x3C;>();\n    // Store top-K (e.g., 5) suggestions at this node — avoids tree traversal on query\n    PriorityQueue&#x3C;Suggestion> topK = new PriorityQueue&#x3C;>(Comparator.comparingLong(Suggestion::getScore));\n    boolean isEnd;\n}\n\nclass AutocompleteTrie {\n\n    private final TrieNode root = new TrieNode();\n    private final int K = 5;\n\n    public void insert(String query, long score) {\n        TrieNode node = root;\n        for (char c : query.toCharArray()) {\n            node.children.putIfAbsent(c, new TrieNode());\n            node = node.children.get(c);\n            updateTopK(node, new Suggestion(query, score));\n        }\n        node.isEnd = true;\n    }\n\n    private void updateTopK(TrieNode node, Suggestion suggestion) {\n        node.topK.offer(suggestion);\n        if (node.topK.size() > K) {\n            node.topK.poll(); // Remove lowest score\n        }\n    }\n\n    public List&#x3C;String> search(String prefix) {\n        TrieNode node = root;\n        for (char c : prefix.toCharArray()) {\n            node = node.children.get(c);\n            if (node == null) return Collections.emptyList();\n        }\n        // Top-K already precomputed at this node\n        return node.topK.stream()\n            .sorted(Comparator.comparingLong(Suggestion::getScore).reversed())\n            .map(Suggestion::getQuery)\n            .collect(Collectors.toList());\n    }\n}\n</code></pre>\n<p>The <code>PriorityQueue</code> used as a min-heap of size K is the right choice here: when you call <code>poll()</code>, it removes the lowest-scoring suggestion, so after processing all inserts you are left with the K highest-scoring ones. This gives you O(log K) insert time at each node, which is effectively constant since K is fixed at 5.</p>\n<p><strong>Trie pros/cons:</strong></p>\n<ul>\n<li>Pros: O(L) lookup where L = prefix length, perfect for prefix matching</li>\n<li>Cons: Memory-intensive for large vocabularies, difficult to update incrementally, no fuzzy matching</li>\n</ul>\n<h3>Option 2: Elasticsearch with Edge N-Grams (Production Choice)</h3>\n<p>The trie is elegant for teaching, but production systems at Google or LinkedIn scale choose Elasticsearch because it combines prefix search, typo tolerance, and popularity ranking in a single system that is horizontally scalable. The edge n-gram analyzer is the key configuration: it pre-indexes every prefix of every query term at index time, so a search for \"java v\" matches any document that contains a token starting with \"java v\" — without any trie traversal at all.</p>\n<p>For production at scale, Elasticsearch handles prefix search, typo tolerance, and ranking in one system:</p>\n<pre><code class=\"language-json\">// Index mapping with edge n-gram analyzer\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"autocomplete_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\"lowercase\", \"autocomplete_filter\"]\n        },\n        \"search_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\"lowercase\"]\n        }\n      },\n      \"filter\": {\n        \"autocomplete_filter\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 1,\n          \"max_gram\": 20\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"query\": {\n        \"type\": \"text\",\n        \"analyzer\": \"autocomplete_analyzer\",\n        \"search_analyzer\": \"search_analyzer\"\n      },\n      \"score\": { \"type\": \"long\" },\n      \"updated_at\": { \"type\": \"date\" }\n    }\n  }\n}\n</code></pre>\n<p>Notice that <code>autocomplete_analyzer</code> is used at index time but <code>search_analyzer</code> (without the edge n-gram filter) is used at search time. This asymmetry is intentional: you want to store all prefixes in the index, but at search time you want to match the user's typed prefix as-is against those stored tokens.</p>\n<p>With the index configured to handle prefix matching, the query below adds a second layer: it boosts documents by their historical score and applies a time-decay so that recently trending queries rank higher than equally popular but older ones.</p>\n<pre><code class=\"language-json\">// Query: prefix \"java v\" with boost for recency\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": { \"match\": { \"query\": \"java v\" } },\n      \"functions\": [\n        { \"field_value_factor\": { \"field\": \"score\", \"modifier\": \"log1p\", \"factor\": 1 } },\n        {\n          \"gauss\": {\n            \"updated_at\": { \"origin\": \"now\", \"scale\": \"7d\", \"decay\": 0.5 }\n          }\n        }\n      ],\n      \"boost_mode\": \"multiply\"\n    }\n  },\n  \"size\": 5\n}\n</code></pre>\n<p>The <code>gauss</code> decay function is what makes trending queries surface quickly: a query updated today gets a decay score of 1.0, while a query updated 7 days ago gets a score of 0.5 (<code>decay</code> parameter), and 14 days ago about 0.25. Multiplied by the frequency score, this ensures a viral query can jump from page 3 to position 1 within hours of spiking.</p>\n<h2>Distributed Architecture</h2>\n<p>With the core search logic defined, the architecture adds a caching layer in front of Elasticsearch. This is not optional: at 115,000 queries per second, Elasticsearch would need hundreds of nodes to handle the full load. Caching popular prefixes in Redis and CDN reduces the load Elasticsearch actually sees to a small fraction of total traffic.</p>\n<pre><code>Client                CDN                 API          Redis          Elasticsearch\n  │                    │                   │              │                │\n  ├─ type \"j\" ────────►├─── cache hit? ────►│              │                │\n  │                    │    YES: return     │              │                │\n  │◄────── [\"java\"]────┤                   │              │                │\n  │                    │                   │              │                │\n  ├─ type \"ja\" ───────►├─── miss ──────────►├─ get(\"ja\") ─►│                │\n  │                    │                   │◄─ [\"java\"] ──┤                │\n  │◄─ [\"java\", \"java\"] ┤◄───────────────────┤              │                │\n  │                    │                   │              │                │\n  ├─ type \"jav\" ──────►├─── miss ──────────►├─ miss ───────►├─ search(\"jav\")►│\n  │                    │                   │◄─────────────────────────────┤\n  │◄─ [suggestions] ───┤◄───────────────────┤              │                │\n</code></pre>\n<p><strong>Caching strategy:</strong></p>\n<ul>\n<li>CDN (CloudFront): Cache responses for common prefixes (\"a\", \"th\", \"he\" — ~80% of traffic)</li>\n<li>Redis: Cache prefix → suggestions with 5-minute TTL</li>\n<li>Cache key: <code>suggest:{lang}:{prefix}</code> (normalize: lowercase, trim)</li>\n</ul>\n<p>The short 5-minute TTL in Redis is deliberate: it ensures that trending queries — which your pipeline updates every minute — propagate to users within 5 minutes of spiking, even for cached prefixes. A longer TTL would make the system more cache-efficient but less responsive to trends.</p>\n<pre><code class=\"language-java\">@Service\npublic class AutocompleteService {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    @Autowired\n    private ElasticsearchClient es;\n\n    public List&#x3C;String> suggest(String prefix, String locale) {\n        String normalized = prefix.toLowerCase().trim();\n        if (normalized.length() &#x3C; 2) return Collections.emptyList(); // Min 2 chars\n\n        String cacheKey = \"suggest:\" + locale + \":\" + normalized;\n        String cached = redis.opsForValue().get(cacheKey);\n\n        if (cached != null) {\n            return objectMapper.readValue(cached, List.class);\n        }\n\n        List&#x3C;String> suggestions = searchElasticsearch(normalized, locale);\n\n        redis.opsForValue().set(cacheKey, objectMapper.writeValueAsString(suggestions),\n            Duration.ofMinutes(5));\n\n        return suggestions;\n    }\n}\n</code></pre>\n<p>The minimum prefix length of 2 characters is a practical optimization: single-character prefixes like \"a\" or \"t\" would match millions of queries and are too ambiguous to be useful, while their cache entries would occupy disproportionate memory. By skipping them, you eliminate a class of expensive queries with low signal.</p>\n<h2>Keeping Suggestions Fresh: Real-Time Updates</h2>\n<p>With the serving layer in place, you need a pipeline that continuously feeds new query data back into the index. The challenge is balancing freshness (how quickly a viral query appears) against noise (a query that spikes once due to a bot should not permanently pollute the index).</p>\n<p>Query logs are processed to update suggestion scores:</p>\n<pre><code>Pipeline:\n  User searches → App logs query → Kafka topic \"search-queries\"\n      → Flink/Spark aggregation (5-minute windows)\n      → Top queries with updated scores\n      → Update Elasticsearch + rebuild Redis cache\n\nFrequency:\n  Trend detection: 1-minute windows (detect viral queries immediately)\n  Full re-rank: 10-minute windows (stabilize rankings)\n  Full index rebuild: Daily (garbage collect dead queries)\n</code></pre>\n<p>The three-tier frequency schedule is the key design insight here: 1-minute windows for trend detection mean a breaking news query surfaces within 60 seconds, while the daily full rebuild prunes queries that trended briefly and are now dead weight in the index.</p>\n<pre><code class=\"language-java\">// Kafka Streams aggregation\nKStream&#x3C;String, SearchEvent> searches = builder.stream(\"search-queries\");\n\nKTable&#x3C;String, Long> queryCounts = searches\n    .groupBy((key, event) -> event.getNormalizedQuery())\n    .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofMinutes(5)))\n    .count();\n\nqueryCounts.toStream()\n    .map((window, count) -> KeyValue.pair(window.key(), count))\n    .to(\"query-scores\", Produced.with(Serdes.String(), Serdes.Long()));\n</code></pre>\n<p><code>ofSizeWithNoGrace</code> is worth understanding: the \"no grace period\" setting means the window closes immediately at the 5-minute mark and emits its result without waiting for late-arriving events. For autocomplete scoring, this tradeoff is correct — a small number of late events does not materially change query rankings, and lower latency is more valuable than perfect accuracy.</p>\n<h2>Personalization</h2>\n<p>Global suggestions are a strong baseline, but users who have searched for \"java concurrency\" three times this week should see Java-related completions ranked above unrelated queries. The personalization layer blends a small number of personal suggestions into the global top-5, placing them first so they appear immediately when relevant.</p>\n<p>Personal suggestions boost queries from the user's search history:</p>\n<pre><code class=\"language-java\">public List&#x3C;String> suggestPersonalized(String prefix, String userId) {\n    // Blend global suggestions with personal history\n    List&#x3C;String> global = suggest(prefix, \"en\");\n\n    List&#x3C;String> personal = userHistoryService.getMatchingHistory(userId, prefix, 3);\n\n    // Merge: personal first (max 2), then global (fill remaining 3)\n    return Stream.concat(personal.stream(), global.stream())\n        .distinct()\n        .limit(5)\n        .collect(Collectors.toList());\n}\n</code></pre>\n<p>Capping personal suggestions at 2 out of 5 ensures the global ranking still dominates. If you let personalization dominate entirely, users in a narrow interest category would see increasingly narrow suggestions, a filter-bubble effect that reduces discovery of new topics.</p>\n<h2>Typo Tolerance</h2>\n<p>Even with perfect indexing and caching, users make typos. Without fuzzy matching, a user typing \"jva virtual\" would see no suggestions at all. Elasticsearch's built-in <code>fuzziness</code> setting handles this by finding documents within a configurable edit distance from the typed query.</p>\n<p>Use Elasticsearch's fuzzy matching for queries with typos:</p>\n<pre><code class=\"language-json\">{\n  \"query\": {\n    \"multi_match\": {\n      \"query\": \"jva virtual\",\n      \"fields\": [\"query\"],\n      \"fuzziness\": \"AUTO\",     // 0 edits for 1-2 chars, 1 for 3-5, 2 for 6+\n      \"prefix_length\": 2,      // First 2 chars must match exactly (performance)\n      \"max_expansions\": 50\n    }\n  }\n}\n</code></pre>\n<p>The <code>prefix_length: 2</code> parameter is a critical performance guard: it tells Elasticsearch that the first two characters must match exactly, which dramatically reduces the search space for fuzzy expansion. Without this, \"AUTO\" fuzziness on a short query like \"jv\" could expand to thousands of candidate terms and make every keystroke slow.</p>\n<p>The difference between a good autocomplete and a great one is the ranking function. Frequency alone gives stale results. Recency alone gives noisy trending results. The combination — frequency × recency × click-through rate — matches user intent. Instrument your system to measure suggestion acceptance rate and use that signal to continuously improve rankings.</p>\n","tableOfContents":[{"id":"requirements","text":"Requirements","level":2},{"id":"data-model-what-are-we-searching","text":"Data Model: What Are We Searching?","level":2},{"id":"core-data-structure-trie-vs-inverted-index","text":"Core Data Structure: Trie vs Inverted Index","level":2},{"id":"option-1-trie-prefix-tree","text":"Option 1: Trie (Prefix Tree)","level":3},{"id":"option-2-elasticsearch-with-edge-n-grams-production-choice","text":"Option 2: Elasticsearch with Edge N-Grams (Production Choice)","level":3},{"id":"distributed-architecture","text":"Distributed Architecture","level":2},{"id":"keeping-suggestions-fresh-real-time-updates","text":"Keeping Suggestions Fresh: Real-Time Updates","level":2},{"id":"personalization","text":"Personalization","level":2},{"id":"typo-tolerance","text":"Typo Tolerance","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}