{"pageProps":{"post":{"title":"Kafka Streams: Real-Time Stream Processing Without a Separate Cluster","description":"Production Kafka Streams: KStream vs KTable semantics, stateful transformations with RocksDB state stores, windowed aggregations, stream-table joins, topology design, changelog topics, and the operational patterns for running Kafka Streams in production.","date":"2025-04-24","category":"Data Engineering","tags":["kafka","kafka streams","stream processing","real-time","java","rocksdb","windowing","data engineering"],"featured":false,"affiliateSection":"data-engineering-resources","slug":"kafka-streams-real-time-processing","readingTime":"6 min read","excerpt":"Kafka Streams is a Java library for building real-time stream processing applications. Unlike Flink or Spark Streaming, it has no separate cluster — it runs as a library inside your Java application. Each instance of you…","contentHtml":"<p>Kafka Streams is a Java library for building real-time stream processing applications. Unlike Flink or Spark Streaming, it has no separate cluster — it runs as a library inside your Java application. Each instance of your application processes a subset of partitions. Scale by adding instances. It's operationally simple (just another Spring Boot application), yet powerful enough for complex stateful streaming computations.</p>\n<h2>KStream vs. KTable: The Core Abstraction</h2>\n<p>Understanding the difference between KStream and KTable is fundamental to Kafka Streams:</p>\n<pre><code>KStream: an unbounded sequence of events\n  → Each message represents an independent event\n  → \"Order placed\", \"Payment received\", \"Item shipped\"\n  → Records are APPENDED — every record matters\n  → Like a database transaction log\n\nKTable: a changelog stream representing state\n  → Each message represents the LATEST value for a key\n  → \"Current inventory level for product X: 42\"\n  → New record with same key REPLACES old record\n  → Like a database table with CDC (change data capture)\n\nGlobalKTable: like KTable, but ALL data is loaded into every instance\n  → Enables enrichment joins without repartitioning\n  → Use for small, read-heavy reference data (users, products, configs)\n</code></pre>\n<pre><code class=\"language-java\">// Topology setup:\n@Configuration\npublic class StreamTopologyConfig {\n\n    @Bean\n    public KafkaStreamsConfiguration kStreamsConfig(\n            @Value(\"${spring.kafka.bootstrap-servers}\") String bootstrapServers) {\n        Map&#x3C;String, Object> props = new HashMap&#x3C;>();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"order-processing\");  // Consumer group ID\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);  // Checkpoint every 1s\n        // RocksDB state stores in persistent directory (survive restart):\n        props.put(StreamsConfig.STATE_DIR_CONFIG, \"/var/lib/kafka-streams\");\n        return new KafkaStreamsConfiguration(props);\n    }\n\n    @Bean\n    public StreamsBuilder streamsBuilder() {\n        return new StreamsBuilder();\n    }\n}\n</code></pre>\n<h2>Stateless Transformations</h2>\n<pre><code class=\"language-java\">@Component\npublic class OrderEnrichmentTopology {\n\n    @Autowired\n    private StreamsBuilder streamsBuilder;\n\n    @PostConstruct\n    public void buildTopology() {\n        // Input: raw order events (JSON string)\n        KStream&#x3C;String, String> rawOrders = streamsBuilder.stream(\"orders-raw\");\n\n        // Stateless filter + map:\n        KStream&#x3C;String, Order> orders = rawOrders\n            .filter((key, value) -> value != null &#x26;&#x26; !value.isEmpty())\n            .mapValues(value -> deserialize(value, Order.class))\n            .filter((key, order) -> order.getTotalCents() > 0);  // Skip zero-value orders\n\n        // Branch: route high-value orders to separate topic:\n        Map&#x3C;String, KStream&#x3C;String, Order>> branches = orders.split(Named.as(\"branch-\"))\n            .branch((key, order) -> order.getTotalCents() >= 100_000,\n                    Branched.as(\"high-value\"))     // $1000+\n            .branch((key, order) -> order.getTotalCents() >= 10_000,\n                    Branched.as(\"medium-value\"))   // $100-$999\n            .defaultBranch(Branched.as(\"standard\"));\n\n        branches.get(\"branch-high-value\")\n            .mapValues(order -> serialize(order))\n            .to(\"orders-high-value\");\n\n        // Rekeying: change partition key (triggers repartitioning)\n        // Input: keyed by order_id → rekey by customer_id\n        KStream&#x3C;String, Order> byCustomer = orders\n            .selectKey((orderId, order) -> order.getCustomerId());\n        // After selectKey, data is repartitioned — a network shuffle happens\n\n        byCustomer\n            .mapValues(order -> serialize(order))\n            .to(\"orders-by-customer\");\n    }\n}\n</code></pre>\n<h2>Stateful Aggregations with Windowing</h2>\n<pre><code class=\"language-java\">@Component\npublic class RevenueAggregationTopology {\n\n    @PostConstruct\n    public void buildTopology() {\n        KStream&#x3C;String, Order> orders = streamsBuilder\n            .stream(\"orders\", Consumed.with(Serdes.String(), orderSerde));\n\n        // Tumbling window: non-overlapping, fixed-size time windows\n        // Count orders and sum revenue per customer per hour:\n        KTable&#x3C;Windowed&#x3C;String>, RevenueAggregate> hourlyRevenue = orders\n            .selectKey((k, order) -> order.getCustomerId())\n            .groupByKey(Grouped.with(Serdes.String(), orderSerde))\n            .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofHours(1)))\n            .aggregate(\n                RevenueAggregate::new,       // Initializer\n                (customerId, order, agg) -> { // Aggregator\n                    agg.addOrder(order);\n                    return agg;\n                },\n                Materialized.&#x3C;String, RevenueAggregate, WindowStore&#x3C;Bytes, byte[]>>as(\n                    \"customer-hourly-revenue-store\")  // Named state store\n                    .withKeySerde(Serdes.String())\n                    .withValueSerde(revenueAggregateSerde)\n            );\n\n        // Output aggregation results:\n        hourlyRevenue\n            .toStream()\n            .map((windowedKey, aggregate) -> KeyValue.pair(\n                windowedKey.key() + \"@\" + windowedKey.window().start(),\n                serialize(aggregate)\n            ))\n            .to(\"customer-hourly-revenue\");\n\n        // Sliding window: overlapping windows for moving averages\n        // Session window: variable-length windows based on activity gaps\n        KTable&#x3C;Windowed&#x3C;String>, Long> sessionCounts = orders\n            .selectKey((k, order) -> order.getCustomerId())\n            .groupByKey()\n            .windowedBy(SessionWindows.ofInactivityGapWithNoGrace(Duration.ofMinutes(30)))\n            .count(Materialized.as(\"customer-sessions\"));\n    }\n}\n</code></pre>\n<h2>Stream-Table Join: Enrichment Pattern</h2>\n<pre><code class=\"language-java\">@Component\npublic class OrderEnrichmentWithProducts {\n\n    @PostConstruct\n    public void buildTopology() {\n        // Stream: order events\n        KStream&#x3C;String, Order> orders = streamsBuilder.stream(\"orders\");\n\n        // GlobalKTable: product catalog (small, reference data)\n        GlobalKTable&#x3C;String, Product> products = streamsBuilder.globalTable(\n            \"products\",\n            Materialized.as(\"products-store\")  // Locally stored in RocksDB\n        );\n\n        // Enrich each order with product details (no repartitioning needed with GlobalKTable):\n        KStream&#x3C;String, EnrichedOrder> enriched = orders.join(\n            products,\n            (orderId, order) -> order.getProductId(),  // Key extractor (join key)\n            (order, product) -> new EnrichedOrder(order, product)  // Value joiner\n        );\n\n        enriched.to(\"orders-enriched\");\n\n        // Regular KTable join (both sides can be large — requires co-partitioning):\n        KTable&#x3C;String, Customer> customers = streamsBuilder.table(\"customers\");\n\n        // Co-partitioning requirement: orders and customers must have the same\n        // number of partitions and use the same key (customerId)\n        KStream&#x3C;String, Order> ordersByCustomer = orders\n            .selectKey((k, order) -> order.getCustomerId());\n\n        KStream&#x3C;String, EnrichedOrder> withCustomer = ordersByCustomer.join(\n            customers,\n            (order, customer) -> enrichWithCustomer(order, customer),\n            JoinWindows.ofTimeDifferenceWithNoGrace(Duration.ofMinutes(5))\n            // Stream-stream join: events within 5 minutes are matched\n        );\n    }\n}\n</code></pre>\n<h2>State Stores and Interactive Queries</h2>\n<p>Kafka Streams stores stateful computation results in local RocksDB instances. You can query these stores directly from your application:</p>\n<pre><code class=\"language-java\">@RestController\n@RequestMapping(\"/api/analytics\")\npublic class StreamAnalyticsController {\n\n    @Autowired\n    private KafkaStreams kafkaStreams;\n\n    // Query the hourly revenue aggregation state store:\n    @GetMapping(\"/revenue/{customerId}\")\n    public ResponseEntity&#x3C;List&#x3C;RevenueAggregate>> getCustomerRevenue(\n            @PathVariable String customerId) {\n\n        ReadOnlyWindowStore&#x3C;String, RevenueAggregate> store = kafkaStreams.store(\n            StoreQueryParameters.fromNameAndType(\n                \"customer-hourly-revenue-store\",\n                QueryableStoreTypes.windowStore()\n            )\n        );\n\n        long now = System.currentTimeMillis();\n        long oneDayAgo = now - Duration.ofDays(1).toMillis();\n\n        WindowStoreIterator&#x3C;RevenueAggregate> iterator =\n            store.fetch(customerId, oneDayAgo, now);\n\n        List&#x3C;RevenueAggregate> results = new ArrayList&#x3C;>();\n        while (iterator.hasNext()) {\n            KeyValue&#x3C;Long, RevenueAggregate> entry = iterator.next();\n            results.add(entry.value);\n        }\n        iterator.close();\n\n        return ResponseEntity.ok(results);\n    }\n}\n</code></pre>\n<p><strong>For queries across all instances (distributed state):</strong> Kafka Streams assigns partitions to instances. Customer X's state may be on instance 2, but the request hits instance 1. Use Kafka Streams' <code>queryMetadataForKey()</code> to find which instance owns the data, then make an HTTP call to that instance:</p>\n<pre><code class=\"language-java\">KeyQueryMetadata metadata = kafkaStreams.queryMetadataForKey(\n    \"customer-hourly-revenue-store\",\n    customerId,\n    Serdes.String().serializer()\n);\nHostInfo activeHost = metadata.activeHost();\n// If activeHost is this instance: query local store\n// If not: HTTP call to activeHost.host():activeHost.port()\n</code></pre>\n<h2>Changelog Topics and Fault Tolerance</h2>\n<p>Every state store has a corresponding changelog topic in Kafka (e.g., <code>order-processing-customer-hourly-revenue-store-changelog</code>). On failure and restart, Kafka Streams replays the changelog to rebuild the state store:</p>\n<pre><code>State restoration on restart:\n1. Application starts, reads its partition assignments\n2. For each partition: find latest offset in changelog topic\n3. Replay changelog records from last checkpoint to latest offset\n4. State store is restored — then normal processing resumes\n\nRestoration time: proportional to changelog size since last checkpoint\nAt 1000 records/second for 10 minutes = 600,000 records to replay\nAt 100,000 records/second replay speed = ~6 seconds restoration\n\nOptimization: use standby replicas (process on 2 instances, 0 restoration time on failover)\nprops.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);\n</code></pre>\n<h2>Production Configuration</h2>\n<pre><code class=\"language-java\">// Essential production settings:\nprops.put(StreamsConfig.APPLICATION_ID_CONFIG, \"order-processing-v2\");\nprops.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);  // Changelog topic replication\nprops.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);  // Standby for fast failover\nprops.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);  // Checkpoint every 1s\nprops.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024L);  // 10MB in-memory buffer\n\n// Consumer settings:\nprops.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");  // Start from beginning\nprops.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);  // 30s timeout\n\n// Producer settings (for output topics):\nprops.put(ProducerConfig.ACKS_CONFIG, \"all\");  // All replicas must acknowledge\nprops.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);  // Exactly-once semantics\n\n// Exactly-once processing (requires Kafka >= 2.5):\nprops.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE_V2);\n</code></pre>\n<p>Exactly-once semantics (<code>EXACTLY_ONCE_V2</code>) means state store updates and output topic writes are committed atomically — no duplicates even on failure. The cost: ~20% latency overhead from transactional producer coordination.</p>\n<p>Kafka Streams' architecture — library embedded in your application, state in local RocksDB, fault tolerance via changelog topics — is the right abstraction for stream processing when your team already runs Kafka and doesn't want to operate a separate cluster. The learning curve is the topology DSL and the KStream/KTable semantics. Once those click, building complex stateful streaming pipelines becomes straightforward Java development.</p>\n","tableOfContents":[{"id":"kstream-vs-ktable-the-core-abstraction","text":"KStream vs. KTable: The Core Abstraction","level":2},{"id":"stateless-transformations","text":"Stateless Transformations","level":2},{"id":"stateful-aggregations-with-windowing","text":"Stateful Aggregations with Windowing","level":2},{"id":"stream-table-join-enrichment-pattern","text":"Stream-Table Join: Enrichment Pattern","level":2},{"id":"state-stores-and-interactive-queries","text":"State Stores and Interactive Queries","level":2},{"id":"changelog-topics-and-fault-tolerance","text":"Changelog Topics and Fault Tolerance","level":2},{"id":"production-configuration","text":"Production Configuration","level":2}]},"relatedPosts":[{"title":"Change Data Capture with Debezium: Real-Time Data Synchronization Patterns","description":"CDC lets you stream every database change as an event. Learn how Debezium captures PostgreSQL WAL logs, publishes to Kafka, and powers cache invalidation, search indexing, and microservice sync.","date":"2025-02-01","category":"Data Engineering","tags":["cdc","debezium","kafka","data engineering","postgresql","microservices"],"featured":false,"affiliateSection":"data-engineering-resources","slug":"cdc-debezium-kafka-patterns","readingTime":"11 min read","excerpt":"Change Data Capture (CDC) is one of those techniques that, once you understand it, you see it everywhere. The pattern: instead of your application explicitly publishing events when data changes, let the database engine i…"}]},"__N_SSG":true}