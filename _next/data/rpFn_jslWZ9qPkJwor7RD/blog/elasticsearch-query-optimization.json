{"pageProps":{"post":{"title":"Elasticsearch Query Optimization: From Slow to Sub-100ms","description":"Elasticsearch performance tuning in production: query vs filter context, mapping optimization, shard sizing strategy, field data vs doc values, aggregation performance, index lifecycle management, and the profiling tools that identify bottlenecks.","date":"2025-05-29","category":"Databases","tags":["elasticsearch","search","performance","indexing","aggregations","kibana","spring boot"],"featured":false,"affiliateSection":"database-resources","slug":"elasticsearch-query-optimization","readingTime":"8 min read","excerpt":"Elasticsearch is a distributed search and analytics engine built on top of Apache Lucene. At small scale, it's fast regardless of what you do. At production scale — billions of documents, hundreds of concurrent queries, …","contentHtml":"<p>Elasticsearch is a distributed search and analytics engine built on top of Apache Lucene. At small scale, it's fast regardless of what you do. At production scale — billions of documents, hundreds of concurrent queries, real-time indexing — the difference between a well-tuned cluster and a poorly configured one is 10-100× in query latency. Most Elasticsearch performance problems are caused by a small set of well-understood mistakes.</p>\n<h2>Query Context vs. Filter Context</h2>\n<p>The single most impactful optimization: use filter context instead of query context wherever relevance scoring is not needed.</p>\n<pre><code class=\"language-json\">// SLOW: Query context — calculates relevance scores for every document\nGET /orders/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"term\": { \"status\": \"pending\" } },\n        { \"term\": { \"region\": \"us-east-1\" } },\n        { \"range\": { \"created_at\": { \"gte\": \"2025-01-01\" } } }\n      ]\n    }\n  }\n}\n\n// FAST: Filter context — binary match/no-match, results are CACHED\nGET /orders/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": [\n        { \"term\": { \"status\": \"pending\" } },\n        { \"term\": { \"region\": \"us-east-1\" } },\n        { \"range\": { \"created_at\": { \"gte\": \"2025-01-01\" } } }\n      ]\n    }\n  }\n}\n</code></pre>\n<p>Filter context differences from query context:</p>\n<ol>\n<li><strong>No scoring</strong> — filters are true/false, no TF-IDF calculation</li>\n<li><strong>Cached</strong> — Elasticsearch caches filter results in the filter cache (not query cache)</li>\n<li><strong>Faster</strong> — 2-10× faster for exact-value and range queries</li>\n</ol>\n<p>Rule: put <code>must</code> clauses in <code>filter</code> unless you actually need relevance ranking. Use <code>must</code> (query context) only for full-text search where you need <code>_score</code>.</p>\n<pre><code class=\"language-json\">// Correct hybrid: full-text search with filters\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"match\": { \"description\": \"blue running shoes\" } }  // Query context: scoring needed\n      ],\n      \"filter\": [\n        { \"term\": { \"in_stock\": true } },                    // Filter context: no scoring\n        { \"range\": { \"price\": { \"lte\": 100 } } }\n      ]\n    }\n  }\n}\n</code></pre>\n<h2>Mapping Optimization: Disable What You Don't Need</h2>\n<p>Elasticsearch's default dynamic mapping indexes everything with maximum flexibility. In production, disable features you don't use:</p>\n<pre><code class=\"language-json\">PUT /orders\n{\n  \"mappings\": {\n    \"dynamic\": \"strict\",  // Reject unknown fields (don't silently index new fields)\n    \"properties\": {\n      \"order_id\": {\n        \"type\": \"keyword\"  // Exact match — don't use 'text' for IDs\n      },\n      \"status\": {\n        \"type\": \"keyword\",\n        \"doc_values\": true,  // For sorting/aggregations (default true for keyword)\n        \"index\": true        // For filtering (default true)\n      },\n      \"description\": {\n        \"type\": \"text\",\n        \"index\": true,\n        \"doc_values\": false,  // Text fields can't be aggregated anyway\n        \"norms\": false,       // Disable length normalization if not needed\n        \"index_options\": \"docs\"  // 'docs' &#x3C; 'freqs' &#x3C; 'positions' &#x3C; 'offsets' (ascending cost)\n      },\n      \"user_id\": {\n        \"type\": \"keyword\",\n        \"index\": true,\n        \"doc_values\": false   // No aggregations on user_id → disable doc_values (saves heap)\n      },\n      \"internal_notes\": {\n        \"type\": \"text\",\n        \"index\": false        // Store the field but don't index it (can't search, can retrieve)\n      },\n      \"created_at\": {\n        \"type\": \"date\",\n        \"format\": \"strict_date_optional_time\"\n      },\n      \"amount_cents\": {\n        \"type\": \"long\"\n      }\n    }\n  }\n}\n</code></pre>\n<p><strong>doc_values vs fielddata:</strong></p>\n<p>For aggregations and sorting on <code>keyword</code> fields: use <code>doc_values</code> (default on, stored on disk, low heap impact).</p>\n<p>For aggregations on <code>text</code> fields: requires <code>fielddata: true</code> — this loads the entire inverted index into heap memory. On a large index, this can OOM your cluster.</p>\n<pre><code class=\"language-json\">// DANGEROUS: Enabling fielddata on a high-cardinality text field\nPUT /orders/_mapping\n{\n  \"properties\": {\n    \"description\": {\n      \"type\": \"text\",\n      \"fielddata\": true  // Loads all text terms into heap — can cause OOM\n    }\n  }\n}\n\n// CORRECT: Use a multi-field — text for searching, keyword for aggregating\n\"product_name\": {\n  \"type\": \"text\",\n  \"fields\": {\n    \"keyword\": {           // product_name.keyword → exact match + aggregations\n      \"type\": \"keyword\",\n      \"ignore_above\": 256  // Don't index very long strings as keyword\n    }\n  }\n}\n</code></pre>\n<h2>Shard Sizing: The Root Cause of Most Performance Problems</h2>\n<p>Shards are the unit of parallelism in Elasticsearch. Too few: can't parallelize. Too many: excessive overhead.</p>\n<pre><code>Shard sizing guidelines:\n- Target: 10-50GB per shard\n- Too small (&#x3C; 1GB): overhead per shard dominates, cluster management expensive\n- Too large (> 50GB): recovery time after node failure is too long\n\nCommon mistake: 5 shards × 1 replica = 10 shards for an index with 1GB of data\n→ Each shard: 100MB — massive overhead\n→ Should be 1 shard or reduce replica count\n\nCalculation example:\nIndex: product catalog\nData: 50GB expected\nShards: 50GB ÷ 30GB target = ~2 primary shards\nReplicas: 1 (for redundancy)\nTotal: 4 shards across cluster\n\nNumber of shards also determines maximum parallelism for a single query:\nA query hits all shards — 2 shards = query runs on 2 nodes in parallel\n</code></pre>\n<p><strong>You cannot change the number of primary shards without reindexing.</strong> Set it correctly when creating the index. For time-based data, use ILM (Index Lifecycle Management) instead of one giant index.</p>\n<h2>Index Lifecycle Management (ILM) for Time-Series Data</h2>\n<pre><code class=\"language-json\">// ILM policy: roll over active index when it hits 50GB or 30 days\nPUT _ilm/policy/logs-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_size\": \"50GB\",\n            \"max_age\": \"30d\"\n          },\n          \"set_priority\": { \"priority\": 100 }\n        }\n      },\n      \"warm\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"shrink\": { \"number_of_shards\": 1 },   // Reduce to 1 shard (read-only)\n          \"forcemerge\": { \"max_num_segments\": 1 }, // Merge to 1 segment (fast reads)\n          \"set_priority\": { \"priority\": 50 }\n        }\n      },\n      \"cold\": {\n        \"min_age\": \"90d\",\n        \"actions\": {\n          \"freeze\": {}     // Minimize memory usage — slow to query but searchable\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"365d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<p>Forcemerge in the warm phase reduces each index from many small segments (created during heavy indexing) to a single segment. Single-segment indexes are faster to read because Lucene doesn't need to merge results from multiple segments.</p>\n<h2>Aggregation Performance</h2>\n<pre><code class=\"language-json\">// SLOW: Aggregation on a high-cardinality text field with fielddata\n// FAST: Aggregation on a keyword field with doc_values\n\n// Cardinality aggregation (approximate count of unique values):\nGET /orders/_search\n{\n  \"size\": 0,  // Don't return hits, just aggregation results\n  \"aggs\": {\n    \"unique_customers\": {\n      \"cardinality\": {\n        \"field\": \"user_id\",\n        \"precision_threshold\": 40000  // Higher = more accurate, more memory (max 40000)\n      }\n    }\n  }\n}\n\n// Date histogram for time-series data:\nGET /orders/_search\n{\n  \"size\": 0,\n  \"query\": {\n    \"bool\": {\n      \"filter\": [\n        { \"range\": { \"created_at\": { \"gte\": \"now-30d/d\", \"lte\": \"now/d\" } } }\n      ]\n    }\n  },\n  \"aggs\": {\n    \"orders_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"created_at\",\n        \"calendar_interval\": \"1d\",\n        \"time_zone\": \"UTC\"\n      },\n      \"aggs\": {\n        \"total_revenue\": {\n          \"sum\": { \"field\": \"amount_cents\" }\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<p><strong>Aggregation optimization techniques:</strong></p>\n<ol>\n<li><strong>Filter before aggregating</strong> — use <code>query.bool.filter</code> to reduce the document set before running aggregations</li>\n<li><strong>Use <code>size: 0</code></strong> — if you only need aggregation results, don't fetch any hits (eliminates top-N scoring overhead)</li>\n<li><strong>Limit terms aggregation size</strong> — <code>\"terms\": {\"field\": \"status\", \"size\": 10}</code> — default size is 10, but large sizes (> 10,000) are expensive</li>\n<li><strong>Shard-level aggregation</strong> — Elasticsearch aggregates on each shard, then merges. More shards = more parallel aggregation = faster for large datasets</li>\n</ol>\n<h2>Query Profiling</h2>\n<p>Use the Profile API to understand where time goes:</p>\n<pre><code class=\"language-json\">GET /orders/_search\n{\n  \"profile\": true,\n  \"query\": {\n    \"bool\": {\n      \"filter\": [\n        { \"term\": { \"status\": \"pending\" } },\n        { \"range\": { \"created_at\": { \"gte\": \"now-1d\" } } }\n      ]\n    }\n  }\n}\n\n// Profile response (simplified):\n{\n  \"profile\": {\n    \"shards\": [{\n      \"searches\": [{\n        \"query\": [{\n          \"type\": \"BooleanQuery\",\n          \"description\": \"+status:pending +created_at:[...]\",\n          \"time_in_nanos\": 1532000,  // 1.5ms — whole query\n          \"breakdown\": {\n            \"create_weight\": 450000,\n            \"build_scorer\": 380000,\n            \"next_doc\": 420000,\n            \"score\": 80000,          // 0ms — filter context, no scoring\n            \"advance\": 120000\n          },\n          \"children\": [...]\n        }]\n      }]\n    }]\n  }\n}\n</code></pre>\n<p>High <code>create_weight</code> time often indicates an expensive <code>must</code> clause that should be moved to <code>filter</code>. High <code>next_doc</code> time indicates iterating many documents — consider if the index is missing a useful field for filtering.</p>\n<h2>Bulk Indexing Optimization</h2>\n<pre><code class=\"language-java\">// Spring Data Elasticsearch — bulk indexing:\n@Service\npublic class ProductIndexService {\n\n    @Autowired\n    private ElasticsearchOperations operations;\n\n    public void bulkIndex(List&#x3C;Product> products) {\n        List&#x3C;IndexQuery> queries = products.stream()\n            .map(product -> new IndexQueryBuilder()\n                .withId(product.getId().toString())\n                .withObject(product)\n                .build())\n            .collect(Collectors.toList());\n\n        operations.bulkIndex(queries, IndexCoordinates.of(\"products\"));\n    }\n}\n\n// Optimal bulk indexing settings (disable during bulk load):\nPUT /products/_settings\n{\n  \"settings\": {\n    \"refresh_interval\": \"-1\",        // Disable auto-refresh during bulk load\n    \"number_of_replicas\": \"0\"        // No replicas during load (re-enable after)\n  }\n}\n// After bulk load completes:\nPUT /products/_settings\n{\n  \"settings\": {\n    \"refresh_interval\": \"30s\",       // Or \"1s\" for near-real-time search\n    \"number_of_replicas\": \"1\"\n  }\n}\nPOST /products/_forcemerge?max_num_segments=5  // Merge segments after bulk load\n</code></pre>\n<p><strong>Index refresh_interval:</strong> Every 1 second (default), Elasticsearch makes new documents searchable by refreshing the in-memory buffer to disk. Each refresh creates a new Lucene segment. Too many small segments → slow searches. During bulk indexing, set <code>refresh_interval: -1</code> to batch many documents into fewer, larger segments. After loading, set to <code>30s</code> or <code>1s</code> depending on your freshness requirement.</p>\n<h2>Java Client Configuration</h2>\n<pre><code class=\"language-java\">@Configuration\npublic class ElasticsearchConfig {\n\n    @Bean\n    public ElasticsearchClient elasticsearchClient() {\n        RestClient restClient = RestClient.builder(\n            new HttpHost(\"es-cluster.example.com\", 9200, \"https\")\n        )\n        .setRequestConfigCallback(config -> config\n            .setConnectTimeout(5000)\n            .setSocketTimeout(30000)   // Allow time for complex queries\n        )\n        .setHttpClientConfigCallback(httpClient -> httpClient\n            .setMaxConnTotal(50)           // Connection pool size\n            .setMaxConnPerRoute(50)\n        )\n        .build();\n\n        ElasticsearchTransport transport = new RestClientTransport(\n            restClient, new JacksonJsonpMapper());\n\n        return new ElasticsearchClient(transport);\n    }\n}\n</code></pre>\n<p>The path to fast Elasticsearch queries is systematic: understand why filter context is cached, map only what you query, size shards to 10-50GB, let ILM manage index rollover, profile slow queries to find the expensive clause, and use bulk API with refresh disabled for heavy indexing. Each optimization compounds — a well-mapped index in filter context on properly-sized shards can be 10-50× faster than the same data with default settings.</p>\n","tableOfContents":[{"id":"query-context-vs-filter-context","text":"Query Context vs. Filter Context","level":2},{"id":"mapping-optimization-disable-what-you-dont-need","text":"Mapping Optimization: Disable What You Don't Need","level":2},{"id":"shard-sizing-the-root-cause-of-most-performance-problems","text":"Shard Sizing: The Root Cause of Most Performance Problems","level":2},{"id":"index-lifecycle-management-ilm-for-time-series-data","text":"Index Lifecycle Management (ILM) for Time-Series Data","level":2},{"id":"aggregation-performance","text":"Aggregation Performance","level":2},{"id":"query-profiling","text":"Query Profiling","level":2},{"id":"bulk-indexing-optimization","text":"Bulk Indexing Optimization","level":2},{"id":"java-client-configuration","text":"Java Client Configuration","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring — every node is equal, there's no primary, and data placement is determined by partit…"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you…"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible — rollback a bad deploy and your code is back to the previous version. Database schema changes are st…"}]},"__N_SSG":true}