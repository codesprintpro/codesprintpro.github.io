{"pageProps":{"post":{"title":"PostgreSQL vs MongoDB vs DynamoDB: When to Use Which Database","description":"A pragmatic guide to choosing between relational, document, and key-value stores. Real trade-offs, access patterns, and decision criteria — not marketing material.","date":"2025-01-12","category":"Databases","tags":["postgresql","mongodb","dynamodb","nosql","sql","databases"],"featured":false,"affiliateSection":"database-resources","slug":"postgresql-mongodb-dynamodb-comparison","readingTime":"11 min read","excerpt":"Every database choice is a bet on your access patterns. PostgreSQL, MongoDB, and DynamoDB are all excellent databases — for different problems. The mistake engineers make is choosing based on hype (\"NoSQL scales better\")…","contentHtml":"<p>Every database choice is a bet on your access patterns. PostgreSQL, MongoDB, and DynamoDB are all excellent databases — for different problems. The mistake engineers make is choosing based on hype (\"NoSQL scales better\") or familiarity (\"we've always used Postgres\") rather than modeling their actual data access patterns.</p>\n<p>This article gives you a decision framework based on concrete trade-offs, not vendor marketing.</p>\n<h2>The Core Trade-off: Flexibility vs Predictability</h2>\n<p>Before diving into each database, it helps to understand the fundamental spectrum they sit on. As you move from PostgreSQL toward DynamoDB, you give up query flexibility in exchange for more predictable performance at scale. Neither end is universally better — the right position on the spectrum depends entirely on what your application needs to do.</p>\n<pre><code>PostgreSQL:  Schema-first, ACID, flexible queries, bounded performance at scale\nMongoDB:     Schema-flexible, eventual consistency by default, rich queries, scales horizontally\nDynamoDB:    Schema-minimal, predictable single-digit millisecond latency, massive scale, constrained queries\n</code></pre>\n<p>The more you sacrifice query flexibility (PostgreSQL → MongoDB → DynamoDB), the more predictable and scalable your read/write performance becomes. Choose the right level for your use case.</p>\n<h2>PostgreSQL: When Your Data Is Relational</h2>\n<p>PostgreSQL excels when:</p>\n<ul>\n<li>Data has complex relationships (foreign keys, joins)</li>\n<li>You need ACID transactions across multiple entities</li>\n<li>Query patterns are varied and exploratory (reporting, analytics)</li>\n<li>Your dataset fits in a few TB (or you can shard)</li>\n</ul>\n<h3>PostgreSQL Strengths</h3>\n<p><strong>MVCC and True ACID:</strong></p>\n<p>Financial operations are the canonical example of why ACID transactions matter. The code below shows a bank transfer — the classic case where partial success is worse than failure. PostgreSQL's MVCC (Multi-Version Concurrency Control) ensures that other transactions reading the accounts during this transfer see a consistent snapshot, not half-transferred state.</p>\n<pre><code class=\"language-sql\">-- Bank transfer: both operations succeed or both fail\nBEGIN;\n  UPDATE accounts SET balance = balance - 100 WHERE id = 1;\n  UPDATE accounts SET balance = balance + 100 WHERE id = 2;\n  -- If any error here, ROLLBACK; both changes undone\nCOMMIT;\n</code></pre>\n<p><strong>JSONB for schema flexibility within structure:</strong></p>\n<p>One of PostgreSQL's underappreciated features is JSONB — it lets you store structured relational data alongside flexible document data in the same table. The <code>attributes</code> column below can hold different keys for different product types (a shirt has <code>color</code> and <code>size</code>, while a laptop has <code>ram</code> and <code>storage</code>), while the rest of the table stays strongly typed. You get schema flexibility where you need it, without giving it up everywhere.</p>\n<pre><code class=\"language-sql\">-- Mix relational and document in one table\nCREATE TABLE products (\n    id         BIGSERIAL PRIMARY KEY,\n    name       VARCHAR(255) NOT NULL,\n    price      DECIMAL(10,2) NOT NULL,\n    category   VARCHAR(100) NOT NULL,\n    attributes JSONB,                 -- flexible product-specific fields\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Index into JSONB for performant queries\nCREATE INDEX idx_products_color ON products USING gin((attributes->'color'));\n\n-- Query JSONB fields\nSELECT name, price, attributes->>'color' AS color\nFROM products\nWHERE category = 'shirts'\n  AND (attributes->>'size')::text = 'L'\n  AND price &#x3C; 50;\n</code></pre>\n<p><strong>Partitioning for time-series data:</strong></p>\n<p>When a table grows into the hundreds of millions of rows, even indexed queries slow down because the index itself becomes large and expensive to scan. Table partitioning solves this by splitting the data into smaller physical tables (partitions) based on a key — in this case, the month. The query planner then automatically skips partitions that can't possibly match your time range, a technique called partition pruning.</p>\n<pre><code class=\"language-sql\">-- Monthly partitions for events table\nCREATE TABLE events (\n    id         UUID DEFAULT gen_random_uuid(),\n    user_id    BIGINT NOT NULL,\n    event_type VARCHAR(50) NOT NULL,\n    occurred_at TIMESTAMPTZ NOT NULL,\n    payload    JSONB\n) PARTITION BY RANGE (occurred_at);\n\nCREATE TABLE events_2025_01 PARTITION OF events\n    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');\n\nCREATE TABLE events_2025_02 PARTITION OF events\n    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');\n\n-- Queries automatically prune irrelevant partitions\nSELECT * FROM events WHERE occurred_at BETWEEN '2025-01-15' AND '2025-01-20';\n-- Scans only events_2025_01 partition\n</code></pre>\n<p><strong>When PostgreSQL struggles:</strong></p>\n<ul>\n<li>Horizontal sharding is manual and complex (CitusDB or application-level sharding)</li>\n<li>Connection overhead at high concurrency (use PgBouncer — connection pooling is essential)</li>\n<li>Full-text search is functional but not as feature-rich as Elasticsearch</li>\n<li>Writes slow down as table size grows without careful partition/index management</li>\n</ul>\n<h2>MongoDB: When Documents Are Natural</h2>\n<p>MongoDB excels when:</p>\n<ul>\n<li>Your data is naturally document-shaped (nested objects, arrays)</li>\n<li>Schema evolves frequently (rapidly changing product attributes, user profiles)</li>\n<li>You need horizontal sharding without application-level complexity</li>\n<li>Read patterns favor fetching complete documents</li>\n</ul>\n<h3>MongoDB Strengths</h3>\n<p><strong>Document model matches application objects:</strong></p>\n<p>The key insight behind MongoDB's document model is that it eliminates the object-relational impedance mismatch. In a relational database, a user profile with preferences, an address, and subscription details is spread across 3-4 tables. Every read requires a join. In MongoDB, the whole user is one document — one read, no joins. This makes the most sense when you almost always need the whole object together, not fragments of it.</p>\n<pre><code class=\"language-javascript\">// No joins needed — embed related data that's fetched together\n{\n  \"_id\": ObjectId(\"...\"),\n  \"userId\": \"user_123\",\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\",\n  \"address\": {\n    \"street\": \"123 Main St\",\n    \"city\": \"San Francisco\",\n    \"country\": \"US\"\n  },\n  \"preferences\": {\n    \"theme\": \"dark\",\n    \"notifications\": [\"email\", \"push\"],\n    \"timezone\": \"America/New_York\"\n  },\n  \"subscription\": {\n    \"tier\": \"pro\",\n    \"expiresAt\": ISODate(\"2025-12-31\")\n  },\n  \"createdAt\": ISODate(\"2024-01-15\")\n}\n</code></pre>\n<p>With PostgreSQL, this requires 3-4 tables and a join query. With MongoDB, it's one document — one read.</p>\n<p><strong>Aggregation pipeline for complex analytics:</strong></p>\n<p>MongoDB's aggregation pipeline works like a Unix pipe: data flows through stages, each stage transforming the output of the previous one. The pipeline below unwinds order line items, joins with product data, groups by region and category, and sorts by revenue — all inside the database. For analytics that need to stay close to the document model, this is significantly more natural than translating to SQL.</p>\n<pre><code class=\"language-javascript\">// Sales report by region and product category\ndb.orders.aggregate([\n  { $match: { status: \"completed\", date: { $gte: new Date(\"2025-01-01\") } } },\n  { $unwind: \"$items\" },\n  { $lookup: {\n    from: \"products\",\n    localField: \"items.productId\",\n    foreignField: \"_id\",\n    as: \"product\"\n  }},\n  { $unwind: \"$product\" },\n  { $group: {\n    _id: { region: \"$region\", category: \"$product.category\" },\n    totalRevenue: { $sum: { $multiply: [\"$items.quantity\", \"$items.price\"] } },\n    orderCount: { $count: {} }\n  }},\n  { $sort: { totalRevenue: -1 } }\n]);\n</code></pre>\n<p><strong>When MongoDB struggles:</strong></p>\n<ul>\n<li>Multi-document transactions (added in 4.0, but have overhead — design to avoid them)</li>\n<li>Complex joins across collections (schema design mistake — embed more)</li>\n<li>Strong consistency requirements (default is eventual — use <code>readConcern: majority</code> for strong reads, at latency cost)</li>\n<li>Fixed-cost analytics at scale (Aggregation pipeline is powerful but slower than ClickHouse for OLAP)</li>\n</ul>\n<h2>DynamoDB: When You Need Predictable Scale</h2>\n<p>DynamoDB excels when:</p>\n<ul>\n<li>You need guaranteed single-digit millisecond latency at any scale</li>\n<li>Access patterns are known and simple (get by key, query by partition)</li>\n<li>You expect massive, unpredictable traffic spikes</li>\n<li>Operational overhead must be near-zero (fully managed, no tuning)</li>\n</ul>\n<h3>Single-Table Design: The Key Concept</h3>\n<p>DynamoDB's most unusual characteristic is that it rewards putting all your entity types into a single table. This feels wrong to SQL-trained engineers, but there's a reason: DynamoDB can only efficiently query by partition key and sort key. By encoding entity type and relationships into those keys, you make all your access patterns fast. Think of it as designing your access API first, then building the schema to serve it.</p>\n<p>DynamoDB's data model forces you to think about access patterns first, schema second. All entity types live in one table.</p>\n<pre><code>Table: \"app-table\"\n\nPK              | SK              | GSI1PK      | GSI1SK      | Data\n----------------|-----------------|-------------|-------------|----------\nUSER#user123    | PROFILE         | EMAIL#j@e   | USER#user123| name, phone\nUSER#user123    | ORDER#order456  | STATUS#PAID | 2025-01-15  | total, items\nUSER#user123    | ORDER#order789  | STATUS#SHIP | 2025-01-20  | total, items\nORDER#order456  | ITEM#item001    |             |             | qty, price\nORDER#order456  | ITEM#item002    |             |             | qty, price\nPRODUCT#prod001 | METADATA        |             |             | name, price\n\nAccess patterns satisfied by this schema:\n1. GetItem: Get user profile → PK=USER#user123, SK=PROFILE\n2. Query: Get all orders for user → PK=USER#user123, SK begins_with \"ORDER#\"\n3. Query: Get all paid orders (any user) → GSI1, PK=STATUS#PAID\n4. Query: Get all items in order → PK=ORDER#order456, SK begins_with \"ITEM#\"\n</code></pre>\n<p>The Java code below shows how to talk to this table using the DynamoDB Enhanced Client, which maps your annotated Java classes to DynamoDB items. Notice that the <code>pk</code> field holds compound keys like <code>USER#user123</code> — the <code>#</code> separator is a convention that keeps different entity types from accidentally colliding while still letting you query all entities of a type by prefix.</p>\n<pre><code class=\"language-java\">// DynamoDB single-table access with DynamoDB Enhanced Client\n@DynamoDbBean\npublic class UserProfile {\n    private String pk;  // \"USER#user123\"\n    private String sk;  // \"PROFILE\"\n    private String name;\n    private String email;\n    // ...\n\n    @DynamoDbPartitionKey\n    public String getPk() { return pk; }\n\n    @DynamoDbSortKey\n    public String getSk() { return sk; }\n}\n\nDynamoDbEnhancedClient client = DynamoDbEnhancedClient.builder()\n    .dynamoDbClient(DynamoDbClient.create())\n    .build();\n\nDynamoDbTable&#x3C;UserProfile> table = client.table(\"app-table\", TableSchema.fromBean(UserProfile.class));\n\n// Put\ntable.putItem(userProfile);\n\n// Get\nUserProfile profile = table.getItem(Key.builder()\n    .partitionValue(\"USER#user123\")\n    .sortValue(\"PROFILE\")\n    .build());\n\n// Query all orders for user\nPageIterable&#x3C;UserProfile> orders = table.query(\n    QueryConditional.sortBeginsWith(Key.builder()\n        .partitionValue(\"USER#user123\")\n        .sortValue(\"ORDER#\")\n        .build())\n);\n</code></pre>\n<p><strong>DynamoDB Streams for event-driven architecture:</strong></p>\n<p>DynamoDB Streams captures a time-ordered log of every write to your table, which you can process with a Lambda function. This turns your database into an event bus: a new order row appears, Streams fires an event, Lambda processes it. This pattern is how you build event-driven microservices without adding a separate message broker.</p>\n<pre><code class=\"language-java\">// DynamoDB Streams captures every write as an event\n// Lambda trigger: process changes in real-time\n\n@Override\npublic void handleRequest(DynamodbEvent event, Context context) {\n    for (DynamodbStreamRecord record : event.getRecords()) {\n        if (\"INSERT\".equals(record.getEventName())) {\n            Map&#x3C;String, AttributeValue> newItem = record.getDynamodb().getNewImage();\n            String pk = newItem.get(\"pk\").getS();\n\n            if (pk.startsWith(\"ORDER#\")) {\n                // New order created — trigger fulfillment workflow\n                fulfillmentService.processOrder(pk.replace(\"ORDER#\", \"\"));\n            }\n        }\n    }\n}\n</code></pre>\n<p><strong>When DynamoDB struggles:</strong></p>\n<ul>\n<li>Ad-hoc queries (you don't know your access patterns upfront)</li>\n<li>Complex joins across entity types (impossible without multiple round-trips)</li>\n<li>Large item sizes (400KB item limit)</li>\n<li>ACID transactions (supported but limited to 25 items, higher cost)</li>\n<li>Migrations (no schema = no migration tooling, managing attribute evolution is manual)</li>\n</ul>\n<h2>Decision Framework</h2>\n<p>Use this flow when you are evaluating which database to pick for a new service or feature. The questions are ordered by the factors that most strongly constrain your choice — start at the top and stop when you have a clear answer.</p>\n<pre><code>Question 1: Do you have complex relationships and varied query patterns?\n  YES → PostgreSQL\n\nQuestion 2: Is your data naturally document-shaped with evolving schema?\n  YES → MongoDB\n\nQuestion 3: Do you need guaranteed &#x3C; 10ms latency at 100K+ RPS with zero ops burden?\n  YES → DynamoDB\n\nQuestion 4: Do you need ACID across multiple entities?\n  YES → PostgreSQL (or MongoDB 4.0+ with caveats)\n\nQuestion 5: Do you expect 10x traffic spikes without pre-scaling?\n  YES → DynamoDB (serverless on-demand mode handles this automatically)\n</code></pre>\n<h2>The Hybrid Architecture (Real World)</h2>\n<p>In practice, the most robust production systems don't pick one database and force everything through it. They use each database for what it does best, accepting the operational cost of running multiple systems in exchange for the performance and scalability benefits. Here is a realistic example of how these databases coexist in a mature e-commerce platform.</p>\n<p>Most production systems use multiple databases:</p>\n<pre><code>E-commerce platform:\n\n  PostgreSQL:\n    - User accounts, authentication\n    - Products, inventory (complex relationships)\n    - Orders (ACID transactions for payment)\n\n  DynamoDB:\n    - Session storage (high throughput, simple key-value)\n    - Shopping cart (real-time, per-user state)\n    - Search history (write-heavy, simple structure)\n\n  Redis:\n    - Product catalog cache (hot reads)\n    - Rate limiting\n    - Real-time inventory counters\n\n  Elasticsearch:\n    - Product search with full-text + facets\n\n  ClickHouse:\n    - Analytics (revenue reports, funnel analysis)\n</code></pre>\n<p>No single database is the right answer. The polyglot persistence approach — using each database for what it does best — is the production-grade solution.</p>\n<p>The interview trap is picking one and defending it universally. The production mindset is asking: \"What are the actual access patterns?\" and then matching the tool to the requirement.</p>\n","tableOfContents":[{"id":"the-core-trade-off-flexibility-vs-predictability","text":"The Core Trade-off: Flexibility vs Predictability","level":2},{"id":"postgresql-when-your-data-is-relational","text":"PostgreSQL: When Your Data Is Relational","level":2},{"id":"postgresql-strengths","text":"PostgreSQL Strengths","level":3},{"id":"mongodb-when-documents-are-natural","text":"MongoDB: When Documents Are Natural","level":2},{"id":"mongodb-strengths","text":"MongoDB Strengths","level":3},{"id":"dynamodb-when-you-need-predictable-scale","text":"DynamoDB: When You Need Predictable Scale","level":2},{"id":"single-table-design-the-key-concept","text":"Single-Table Design: The Key Concept","level":3},{"id":"decision-framework","text":"Decision Framework","level":2},{"id":"the-hybrid-architecture-real-world","text":"The Hybrid Architecture (Real World)","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring — every node is equal, there's no primary, and data placement is determined by partit…"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you…"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible — rollback a bad deploy and your code is back to the previous version. Database schema changes are st…"}]},"__N_SSG":true}