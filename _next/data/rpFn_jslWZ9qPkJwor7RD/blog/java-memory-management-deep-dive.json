{"pageProps":{"post":{"title":"Java Memory Management Deep Dive: Heap, GC, and Production Tuning","description":"How the JVM allocates memory, how G1GC and ZGC work under the hood, heap analysis with JVM tools, and the GC tuning decisions that eliminate latency spikes in production Java services.","date":"2025-06-13","category":"Java","tags":["java","jvm","garbage collection","g1gc","zgc","heap","memory management","performance"],"featured":false,"affiliateSection":"java-courses","slug":"java-memory-management-deep-dive","readingTime":"7 min read","excerpt":"Java's garbage collector is the single biggest source of unexplained latency spikes in production services. A GC pause of 2 seconds is invisible in most logs but visible to every user who happened to make a request durin…","contentHtml":"<p>Java's garbage collector is the single biggest source of unexplained latency spikes in production services. A GC pause of 2 seconds is invisible in most logs but visible to every user who happened to make a request during that window. Understanding how memory is managed — from object allocation to heap regions to collector algorithms — is not optional for engineers running Java at scale.</p>\n<h2>JVM Memory Layout</h2>\n<pre><code>JVM Process Memory:\n┌─────────────────────────────────────────────────────────┐\n│  Java Heap                                              │\n│  ┌─────────────────────┐  ┌──────────────────────────┐  │\n│  │  Young Generation   │  │   Old Generation         │  │\n│  │  ┌──────┐ ┌──────┐  │  │  (long-lived objects)    │  │\n│  │  │Eden  │ │Surv  │  │  │                          │  │\n│  │  │Space │ │ivor  │  │  │                          │  │\n│  │  │      │ │Spaces│  │  │                          │  │\n│  │  └──────┘ └──────┘  │  │                          │  │\n│  └─────────────────────┘  └──────────────────────────┘  │\n│                                                         │\n│  Metaspace (class metadata — NOT in heap)              │\n│  Thread Stacks (one per thread, outside heap)           │\n│  Code Cache (JIT compiled code)                         │\n│  Direct Memory (ByteBuffer.allocateDirect)              │\n└─────────────────────────────────────────────────────────┘\n</code></pre>\n<p><strong>Object lifecycle:</strong></p>\n<ol>\n<li>New objects allocated in <strong>Eden</strong> (fast, bump-pointer allocation)</li>\n<li>Minor GC: surviving Eden objects copied to Survivor spaces</li>\n<li>Objects surviving multiple minor GCs promoted to <strong>Old Generation</strong></li>\n<li>Major (Full) GC: collects Old Generation — expensive, may pause</li>\n</ol>\n<p><strong>Why most objects die young:</strong> In a typical Spring Boot service, the vast majority of objects are request-scoped: HttpServletRequest, method parameters, response DTOs. They're allocated in Eden and die before the next minor GC. This is the \"generational hypothesis\" and why young-generation collection is cheap.</p>\n<h2>G1GC: How It Works</h2>\n<p>G1 (Garbage First) replaced CMS as the default GC in JDK 9. It divides the heap into equal-sized regions (typically 1-32MB each) rather than fixed young/old spaces:</p>\n<pre><code>G1 Heap Regions (each ~16MB with -XX:G1HeapRegionSize=16m):\n\n[E][E][E][E][E][E][E][E]  ← Eden regions (active allocation)\n[S][S]                    ← Survivor regions (recently promoted)\n[O][O][O][O][O][O][O][O]  ← Old regions (long-lived)\n[H]                       ← Humongous region (objects > 50% of region size)\n[ ][ ][ ][ ]              ← Free regions\n</code></pre>\n<p><strong>G1 collection phases:</strong></p>\n<ol>\n<li><strong>Young GC (stop-the-world):</strong> Evacuates Eden + Survivor regions to new Survivor/Old regions</li>\n<li><strong>Concurrent Marking:</strong> Marks live objects in Old regions concurrently with application threads</li>\n<li><strong>Mixed GC:</strong> Collects Young regions + the Old regions with most garbage (Garbage First = collect highest-garbage regions first)</li>\n</ol>\n<p><strong>Why G1 can miss pause targets:</strong> If promotion is too fast (too many objects promoted to Old), G1 cannot run concurrent marking fast enough. When Old region occupancy exceeds <code>InitiatingHeapOccupancyPercent</code>, G1 starts concurrent marking. If it can't finish before Old gen fills up, a Full GC (single-threaded Stop-The-World) occurs.</p>\n<h2>ZGC: Sub-Millisecond Pauses</h2>\n<p>ZGC (available since JDK 15, production-ready in JDK 17) achieves sub-millisecond pause times by doing almost all work concurrently:</p>\n<pre><code>ZGC vs G1GC pause times (16GB heap, 4-core server):\nG1GC: Minor GC 10-50ms, Major GC 200ms-2s\nZGC:  All GC pauses &#x3C; 1ms (even at 1TB heap)\n</code></pre>\n<p>ZGC achieves this using <strong>colored pointers</strong> (metadata encoded in object references) and <strong>load barriers</strong> (code inserted at every object read that checks and fixes pointer state). This moves GC work from stop-the-world pauses into the application thread's critical path — you pay a steady ~5-10% throughput overhead instead of occasional large pauses.</p>\n<p><strong>When to use ZGC:</strong></p>\n<ul>\n<li>P99/P999 latency requirements (&#x3C; 100ms SLOs)</li>\n<li>Large heaps (> 8GB) where G1 pause times grow</li>\n<li>Interactive services where pauses are user-visible</li>\n</ul>\n<p><strong>When to stick with G1GC:</strong></p>\n<ul>\n<li>Throughput-optimized batch processing</li>\n<li>Small heaps (&#x3C; 4GB) where G1 pauses are already &#x3C; 50ms</li>\n<li>JDK 11 environments (ZGC not production-ready)</li>\n</ul>\n<h2>GC Tuning Configuration</h2>\n<pre><code class=\"language-bash\"># G1GC for latency-sensitive services:\n-XX:+UseG1GC\n-Xms8g -Xmx8g                              # Fixed heap size (no resizing pauses)\n-XX:MaxGCPauseMillis=100                    # Target: 100ms max pause\n-XX:G1HeapRegionSize=16m                    # For 8GB heap: 512 regions\n-XX:InitiatingHeapOccupancyPercent=35       # Start concurrent marking earlier\n-XX:ConcGCThreads=4                         # Concurrent marking threads = CPU/4\n-XX:ParallelGCThreads=8                     # Parallel GC threads = CPU\n-XX:+ParallelRefProcEnabled                 # Parallel reference processing\n-XX:G1RSetUpdatingPauseTimePercent=10\n\n# ZGC for ultra-low latency:\n-XX:+UseZGC\n-Xms8g -Xmx8g\n-XX:ZCollectionInterval=5                  # Force GC every 5 seconds if idle\n-XX:ZUncommitDelay=300                     # Return memory to OS after 5 min idle\n# No MaxGCPauseMillis — ZGC handles this automatically\n\n# Memory regions (both GCs):\n-XX:MetaspaceSize=256m\n-XX:MaxMetaspaceSize=512m\n-XX:ReservedCodeCacheSize=256m\n\n# GC logging for production diagnosis:\n-Xlog:gc*:file=/var/log/app/gc.log:time,uptime,level:filecount=5,filesize=20m\n</code></pre>\n<h2>Identifying GC Problems</h2>\n<p><strong>Tool 1: jstat — real-time GC monitoring</strong></p>\n<pre><code class=\"language-bash\">jstat -gcutil &#x3C;pid> 1000   # Print every 1 second\n\n# Output columns:\n# S0    S1    E     O     M     CCS   YGC  YGCT  FGC  FGCT   CGC  CGCT   GCT\n# 0.00  42.31 78.92 45.12 93.45 89.23 1847 12.431   2  3.241    0  0.000 15.672\n\n# S0/S1: Survivor space utilization\n# E:     Eden utilization\n# O:     Old gen utilization\n# YGC:   Young GC count  YGCT: Young GC total time\n# FGC:   Full GC count   FGCT: Full GC total time (2 full GCs = ALERT)\n</code></pre>\n<p><strong>Tool 2: GC log analysis</strong></p>\n<pre><code class=\"language-bash\"># Parse GC log for pause time distribution:\ngrep \"Pause\" gc.log | awk '{print $NF}' | sort -n | awk '\nBEGIN { count=0; sum=0 }\n{ times[count++] = $1; sum += $1 }\nEND {\n    print \"Count:\", count\n    print \"Avg:\", sum/count \"ms\"\n    print \"P95:\", times[int(count*0.95)] \"ms\"\n    print \"P99:\", times[int(count*0.99)] \"ms\"\n    print \"Max:\", times[count-1] \"ms\"\n}'\n</code></pre>\n<p><strong>Tool 3: Heap dump analysis with Eclipse MAT</strong></p>\n<pre><code class=\"language-bash\"># Trigger heap dump on OOM:\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/var/log/app/heapdump.hprof\n\n# Manual heap dump:\njmap -dump:format=b,file=/tmp/heap.hprof &#x3C;pid>\n\n# Or via JCmd (safer for running processes):\njcmd &#x3C;pid> GC.heap_dump /tmp/heap.hprof\n</code></pre>\n<p>In Eclipse MAT, look at:</p>\n<ul>\n<li><strong>Dominator Tree:</strong> Objects retaining the most heap — often reveals caches or collections that grew unchecked</li>\n<li><strong>Leak Suspects:</strong> MAT's automated analysis of probable memory leaks</li>\n<li><strong>Top Consumers:</strong> Classes with the most instances</li>\n</ul>\n<h2>Common Memory Problems</h2>\n<p><strong>Problem 1: Old Gen growing to 100% → Full GC</strong></p>\n<p>Cause: Objects promoted to Old Gen faster than GC can collect them.</p>\n<p>Diagnosis: <code>jstat</code> shows O% growing monotonically. <code>jmap -histo &#x3C;pid></code> shows which classes have millions of instances.</p>\n<p>Fix: Usually a cache without size/TTL limits, or a large static collection.</p>\n<pre><code class=\"language-java\">// BAD: Unbounded cache\nprivate static final Map&#x3C;String, UserProfile> cache = new HashMap&#x3C;>();\n\n// GOOD: Size-bounded cache with eviction\nprivate static final Map&#x3C;String, UserProfile> cache = Caffeine.newBuilder()\n    .maximumSize(10_000)\n    .expireAfterWrite(Duration.ofMinutes(30))\n    .build()\n    .asMap();\n</code></pre>\n<p><strong>Problem 2: Humongous object allocations causing GC pressure</strong></p>\n<p>Objects larger than 50% of a G1 region size (typically 8MB+) go directly to Humongous regions and skip Young Gen entirely. Frequent large allocations cause GC pressure.</p>\n<pre><code class=\"language-bash\"># Detect humongous allocations:\n-Xlog:gc+humongous=debug:file=gc.log\n# Shows: \"Humongous region X to Y (Z regions)\"\n</code></pre>\n<p>Fix: Avoid large temporary arrays. Stream large data in chunks. Re-use byte buffers with <code>ByteBuffer.allocateDirect</code>.</p>\n<p><strong>Problem 3: Excessive finalization queue depth</strong></p>\n<p>Objects with <code>finalize()</code> methods (mostly legacy code or certain libraries) must wait for the finalizer thread before their memory is reclaimed. Under GC pressure, the finalization queue can grow unboundedly.</p>\n<pre><code class=\"language-bash\">jmap -histo:live &#x3C;pid> | grep Finalizable\n# If count is growing: finalizer thread is falling behind\n</code></pre>\n<h2>Memory Profiling in Production with JFR</h2>\n<p>Java Flight Recorder has negligible overhead (&#x3C;1%) and is safe for production:</p>\n<pre><code class=\"language-bash\"># Start a 60-second recording:\njcmd &#x3C;pid> JFR.start duration=60s filename=/tmp/recording.jfr settings=profile\n\n# Key events to analyze in JDK Mission Control:\n# - GC configuration and pause times\n# - Object allocation by class (top allocators)\n# - Thread profiling (method-level)\n# - Lock contention\n</code></pre>\n<p>JFR allocation profiling shows you exactly which call sites are allocating the most objects — far more actionable than heap dumps for performance optimization.</p>\n<h2>JVM Ergonomics and Container Awareness</h2>\n<p>In containers, the JVM must know the container's memory limit, not the host's total RAM:</p>\n<pre><code class=\"language-bash\"># JDK 10+ auto-detects container limits:\n# No explicit -Xmx needed when running in container with limits set\n\n# But verify:\njava -XX:+PrintFlagsFinal -version 2>/dev/null | grep MaxHeapSize\n# Should be ~25% of container memory limit (default ergonomics)\n\n# Override if needed:\n-XX:MaxRAMPercentage=75.0    # Use 75% of container RAM for heap\n# Better than hard-coded -Xmx in containerized environments\n</code></pre>\n<p>For Kubernetes pods with <code>memory.limit=2Gi</code>:</p>\n<pre><code class=\"language-bash\">-XX:MaxRAMPercentage=75.0   # Heap = 1.5GB\n# Leaves 512MB for: Metaspace (~200MB), thread stacks (~100MB),\n# direct memory, code cache — sufficient.\n</code></pre>\n","tableOfContents":[{"id":"jvm-memory-layout","text":"JVM Memory Layout","level":2},{"id":"g1gc-how-it-works","text":"G1GC: How It Works","level":2},{"id":"zgc-sub-millisecond-pauses","text":"ZGC: Sub-Millisecond Pauses","level":2},{"id":"gc-tuning-configuration","text":"GC Tuning Configuration","level":2},{"id":"identifying-gc-problems","text":"Identifying GC Problems","level":2},{"id":"common-memory-problems","text":"Common Memory Problems","level":2},{"id":"memory-profiling-in-production-with-jfr","text":"Memory Profiling in Production with JFR","level":2},{"id":"jvm-ergonomics-and-container-awareness","text":"JVM Ergonomics and Container Awareness","level":2}]},"relatedPosts":[{"title":"Java Concurrency Patterns: CompletableFuture, Structured Concurrency, and Thread-Safe Design","description":"Production Java concurrency: CompletableFuture pipelines, handling exceptions in async chains, Java 21 structured concurrency, thread-safe collection patterns, and the concurrency bugs that cause data corruption.","date":"2025-07-08","category":"Java","tags":["java","concurrency","completablefuture","virtual threads","java21","thread-safe","async"],"featured":false,"affiliateSection":"java-courses","slug":"java-concurrency-patterns","readingTime":"7 min read","excerpt":"Java concurrency has three eras: raw  and  (Java 1-4), the  framework (Java 5+), and the virtual thread/structured concurrency era (Java 21+). Each era's patterns still exist in production codebases. Understanding all th…"},{"title":"Spring Security OAuth2 and JWT: Production Implementation Guide","description":"Complete Spring Security OAuth2 implementation: JWT token validation, Resource Server configuration, method-level security, custom UserDetailsService, refresh token rotation, and the security pitfalls that lead to authentication bypasses.","date":"2025-06-03","category":"Java","tags":["spring security","oauth2","jwt","spring boot","authentication","authorization","java","security"],"featured":false,"affiliateSection":"java-courses","slug":"spring-security-oauth2-jwt","readingTime":"7 min read","excerpt":"Spring Security is one of the most powerful and most misunderstood frameworks in the Java ecosystem. Its flexibility is its strength — and its complexity. Misconfigured security is worse than no security, because it give…"},{"title":"Scaling Spring Boot Applications to Handle 10 Million Daily Active Users","description":"A practical performance engineering guide: load balancing, horizontal scaling, database tuning, JVM optimization, autoscaling, and the observability stack to find and fix bottlenecks before they page you.","date":"2025-05-28","category":"Java","tags":["spring boot","java","scaling","performance","jvm","kubernetes","prometheus","grafana"],"featured":false,"affiliateSection":"java-courses","slug":"scaling-spring-boot-10m-dau","readingTime":"10 min read","excerpt":"10 million daily active users is not an exotic scale — it's where a successful mid-stage startup or a growing enterprise service lands. At this scale, the things that worked for 100,000 users start breaking in interestin…"}]},"__N_SSG":true}