{"pageProps":{"post":{"title":"Spring Boot Performance Tuning: From 200 to 2000 RPS","description":"Systematic approach to Spring Boot performance tuning. Covers connection pooling, N+1 query elimination, caching strategies, async processing, and JVM tuning to multiply throughput.","date":"2025-03-13","category":"Java","tags":["spring boot","java","performance","hikaricp","redis","jpa","tuning"],"featured":false,"affiliateSection":"java-courses","slug":"spring-boot-performance","readingTime":"13 min read","excerpt":"A Spring Boot application that handles 200 RPS on Day 1 often has the same underlying hardware capacity to handle 2000 RPS — the gap is how you use it. Most performance bottlenecks in Java web services follow predictable…","contentHtml":"<p>A Spring Boot application that handles 200 RPS on Day 1 often has the same underlying hardware capacity to handle 2000 RPS — the gap is how you use it. Most performance bottlenecks in Java web services follow predictable patterns: the database is waiting, threads are blocking, results are recomputed on every request. This guide walks through a systematic process to find and fix each category.</p>\n<h2>Step 1: Measure Before Tuning</h2>\n<p>The single most important rule: <strong>profile first, optimize second</strong>. Optimizing what you think is slow is almost always wrong.</p>\n<p>Before touching a single line of code, you need hard data. The tools below let you generate realistic load and identify exactly where your application spends its time — whether that's CPU computation, memory allocation, or waiting on I/O. Think of this step as the doctor ordering tests before prescribing medicine.</p>\n<pre><code class=\"language-bash\"># Generate production-like load\n# Apache Bench: 10,000 requests, 50 concurrent\nab -n 10000 -c 50 http://localhost:8080/api/orders\n\n# k6: more realistic load simulation\nk6 run --vus 50 --duration 60s - &#x3C;&#x3C;'EOF'\nimport http from 'k6/http';\nimport { check } from 'k6';\n\nexport default function() {\n  const res = http.get('http://localhost:8080/api/orders');\n  check(res, { 'status is 200': (r) => r.status === 200 });\n}\nEOF\n\n# JVM metrics: CPU profiling with async-profiler (production-safe)\n./profiler.sh -d 30 -e cpu -f cpu_profile.html &#x3C;pid>\n# Opens as HTML: shows exact methods consuming CPU\n\n# Memory allocation profiling\n./profiler.sh -d 30 -e alloc -f alloc_profile.html &#x3C;pid>\n# Shows what code is creating the most garbage\n</code></pre>\n<p>The <code>async-profiler</code> output will produce a flame graph — the widest bars at the top are your real bottlenecks. Run this against your staging environment under load before deciding where to spend your optimization effort.</p>\n<h2>Fix 1: Connection Pool Tuning (HikariCP)</h2>\n<p>The most common Spring Boot performance problem: too few database connections, or too many.</p>\n<p>Your database connection pool is like the number of checkout lanes at a supermarket. Too few lanes and customers queue up. Too many lanes and you waste staff who spend time idle. HikariCP is Spring Boot's default pool, and its defaults are conservative — you almost always need to tune it for your workload.</p>\n<pre><code class=\"language-yaml\"># application.yml\nspring:\n  datasource:\n    hikari:\n      # Formula: (core_count × 2) + effective_spindle_count\n      # For 8-core server with SSD: (8 × 2) + 1 = 17 → use 20\n      # Counter-intuitive: more than ~20 connections slows things down (DB queuing)\n      maximum-pool-size: 20\n      minimum-idle: 5\n      connection-timeout: 3000        # Fail fast: 3s max wait for connection\n      idle-timeout: 600000            # 10 minutes — release idle connections\n      max-lifetime: 1800000           # 30 minutes — recycle connections (avoid stale)\n      keepalive-time: 30000           # 30s keepalive pings\n      pool-name: OrderServicePool\n      # Validate connection before use (PostgreSQL)\n      connection-test-query: SELECT 1\n</code></pre>\n<p>Notice that <code>max-lifetime</code> recycles connections every 30 minutes — this prevents stale connections that can silently fail after your database or network firewall drops them. Once you've set the pool size, you need visibility into whether it's working correctly.</p>\n<pre><code class=\"language-java\">// Monitor pool health\n@Component\npublic class HikariPoolMonitor {\n\n    @Autowired\n    private HikariDataSource dataSource;\n\n    @Scheduled(fixedDelay = 60000)\n    public void logPoolStats() {\n        HikariPoolMXBean pool = dataSource.getHikariPoolMXBean();\n        log.info(\"Hikari pool - active: {}, idle: {}, waiting: {}, total: {}\",\n            pool.getActiveConnections(),\n            pool.getIdleConnections(),\n            pool.getThreadsAwaitingConnection(),\n            pool.getTotalConnections());\n\n        // Alert if waiting > 0 consistently (pool starvation)\n        if (pool.getThreadsAwaitingConnection() > 5) {\n            log.warn(\"POOL STARVATION: {} threads waiting — increase maximum-pool-size\",\n                pool.getThreadsAwaitingConnection());\n        }\n    }\n}\n</code></pre>\n<p>The key metric to watch is <code>threadsAwaitingConnection</code> — if this is consistently above zero, your pool is too small and requests are queuing. Expose this to your monitoring dashboard so you catch pool starvation before users do.</p>\n<h2>Fix 2: Eliminate N+1 Queries</h2>\n<p>N+1 is the most common JPA performance killer. One query fetches N entities, then N queries fetch their relationships — total: N+1 database round trips.</p>\n<p>This is one of those bugs that's invisible in development with small datasets but devastating in production. To understand it, imagine walking into a library and asking for a list of 100 books. The librarian gives you the titles, but then you have to walk back to the desk individually to ask who authored each one — 100 extra trips instead of just getting everything upfront.</p>\n<pre><code class=\"language-java\">// PROBLEM: N+1 in action\n@Entity\npublic class Order {\n    @OneToMany(fetch = FetchType.LAZY)  // Lazy is default and correct\n    private List&#x3C;OrderItem> items;\n\n    @ManyToOne(fetch = FetchType.LAZY)\n    private Customer customer;\n}\n\n@Repository\npublic interface OrderRepository extends JpaRepository&#x3C;Order, Long> {\n\n    // BAD: This loads N orders, then fires N queries for customer, N for items\n    List&#x3C;Order> findByStatus(String status);\n}\n\n// In service:\nList&#x3C;Order> orders = orderRepo.findByStatus(\"PENDING\");\norders.forEach(o -> {\n    log.info(\"Customer: {}\", o.getCustomer().getName()); // N queries\n    log.info(\"Items: {}\", o.getItems().size());          // N more queries\n});\n// Total: 1 + N + N = 201 queries for 100 orders\n</code></pre>\n<p>The problem above is subtle — the code looks innocent, but each property access on a lazy-loaded relationship triggers a separate database round trip. JPA provides three ways to fix this depending on your access pattern.</p>\n<pre><code class=\"language-java\">// SOLUTION 1: JOIN FETCH (when you always need the associations)\n@Repository\npublic interface OrderRepository extends JpaRepository&#x3C;Order, Long> {\n\n    @Query(\"SELECT DISTINCT o FROM Order o \" +\n           \"LEFT JOIN FETCH o.customer \" +\n           \"LEFT JOIN FETCH o.items \" +\n           \"WHERE o.status = :status\")\n    List&#x3C;Order> findByStatusWithDetails(@Param(\"status\") String status);\n    // Total: 1 query\n}\n\n// SOLUTION 2: @EntityGraph (cleaner syntax)\n@EntityGraph(attributePaths = {\"customer\", \"items\"})\nList&#x3C;Order> findByStatus(String status);\n\n// SOLUTION 3: Projections (when you only need specific fields — fastest)\npublic interface OrderSummary {\n    Long getId();\n    String getStatus();\n    String getCustomerName();  // Derived from JOIN\n}\n\n@Query(\"SELECT o.id as id, o.status as status, c.name as customerName \" +\n       \"FROM Order o JOIN o.customer c WHERE o.status = :status\")\nList&#x3C;OrderSummary> findSummaryByStatus(@Param(\"status\") String status);\n// Returns a flat projection — no entity loading, no lazy initialization\n</code></pre>\n<p>Projections are often the best solution for list views — you skip loading full entity objects entirely and get back only the fields your UI actually needs, which is both faster and lighter on memory. To catch N+1 problems early, add query counting to your development environment.</p>\n<pre><code class=\"language-java\">// Detect N+1 in development\n// Datasource-proxy: logs every query with stack trace\n@Bean\npublic DataSource dataSource() {\n    var realDataSource = actualDataSource();\n    return ProxyDataSourceBuilder\n        .create(realDataSource)\n        .name(\"DS-Proxy\")\n        .logQueryBySlf4j(SLF4JLogLevel.DEBUG)\n        .countQuery()                   // Count total queries per request\n        .build();\n}\n</code></pre>\n<p>With <code>datasource-proxy</code> active, you'll see a total query count logged after every request — if a single API call shows 50+ queries, you've found an N+1 problem. This tool pays for itself in the first week.</p>\n<h2>Fix 3: Caching with Spring Cache + Redis</h2>\n<p>Results that are expensive to compute and change rarely are prime caching candidates.</p>\n<p>Caching is the single highest-leverage optimization in most web applications. Think of it like a sticky note on your desk: instead of walking to the file cabinet every time someone asks for the same information, you check the note first. Spring's caching abstraction lets you add this behavior to any method with a single annotation, without changing the method's logic.</p>\n<pre><code class=\"language-java\">// Enable Spring Cache\n@SpringBootApplication\n@EnableCaching\npublic class Application {}\n</code></pre>\n<pre><code class=\"language-yaml\">spring:\n  cache:\n    type: redis\n  data:\n    redis:\n      host: localhost\n      port: 6379\n      timeout: 200ms\n      lettuce:\n        pool:\n          max-active: 20\n          min-idle: 5\n</code></pre>\n<p>With Redis configured, Spring automatically routes your <code>@Cacheable</code> annotations to store and retrieve from Redis — giving you a distributed cache that all your application instances share. Here's how to apply the most common caching patterns to a product service.</p>\n<pre><code class=\"language-java\">@Service\npublic class ProductService {\n\n    // Cache miss: execute method + store result\n    // Cache hit: return cached result, skip method\n    @Cacheable(\n        value = \"products\",\n        key = \"#productId\",\n        condition = \"#productId != null\",\n        unless = \"#result == null\"  // Don't cache null results\n    )\n    public Product findById(String productId) {\n        return productRepository.findById(productId).orElse(null);\n    }\n\n    // Invalidate on update\n    @CacheEvict(value = \"products\", key = \"#product.id\")\n    public Product update(Product product) {\n        return productRepository.save(product);\n    }\n\n    // Update cache in-place (instead of evict + fetch)\n    @CachePut(value = \"products\", key = \"#result.id\")\n    public Product save(Product product) {\n        return productRepository.save(product);\n    }\n\n    // Evict entire cache\n    @CacheEvict(value = \"products\", allEntries = true)\n    @Scheduled(cron = \"0 0 2 * * *\")  // Nightly cache clear\n    public void clearProductCache() {}\n}\n</code></pre>\n<p>The <code>unless = \"#result == null\"</code> condition is important — without it, cache misses (null results) get stored, and every subsequent request skips the database and returns null even after the product is created. For cases where you need more control than annotations provide, use <code>RedisTemplate</code> directly.</p>\n<pre><code class=\"language-java\">// For fine-grained cache control, use RedisTemplate directly\n@Service\npublic class LeaderboardService {\n\n    @Autowired\n    private RedisTemplate&#x3C;String, String> redis;\n\n    private static final Duration LEADERBOARD_TTL = Duration.ofMinutes(5);\n\n    public List&#x3C;LeaderboardEntry> getTopUsers(int limit) {\n        String key = \"leaderboard:top:\" + limit;\n\n        // Try cache first\n        List&#x3C;String> cached = redis.opsForList().range(key, 0, -1);\n        if (cached != null &#x26;&#x26; !cached.isEmpty()) {\n            return cached.stream().map(this::deserialize).toList();\n        }\n\n        // Cache miss: compute and cache\n        List&#x3C;LeaderboardEntry> entries = computeLeaderboard(limit);\n        redis.opsForList().rightPushAll(key, entries.stream().map(this::serialize).toList());\n        redis.expire(key, LEADERBOARD_TTL);\n\n        return entries;\n    }\n}\n</code></pre>\n<p>The leaderboard is a good example of where the annotation-based cache isn't flexible enough — you need to store a list and set an explicit TTL based on business logic. With a 5-minute TTL, your leaderboard computation runs at most 12 times per hour instead of thousands of times.</p>\n<h2>Fix 4: Async Processing with Virtual Threads (Java 21)</h2>\n<p>Blocking I/O (database, HTTP calls) ties up threads. Virtual threads let you run thousands of concurrent I/O operations cheaply.</p>\n<p>The traditional thread model is like having a fixed team of workers where each worker can only do one thing at a time — if they're waiting on an API response, they're blocked and unavailable for other work. Virtual threads are like workers who can put a task on hold, do something else, and return to it when the response arrives — all without creating actual OS threads.</p>\n<pre><code class=\"language-yaml\"># Enable virtual threads for Spring MVC (Java 21+)\nspring:\n  threads:\n    virtual:\n      enabled: true\n# That's it — Spring Boot 3.2+ automatically uses virtual threads for Tomcat\n# Each request gets its own virtual thread — blocking I/O is cheap\n</code></pre>\n<pre><code class=\"language-java\">// Before virtual threads: thread pool exhaustion\n// server.tomcat.max-threads=200 (default)\n// At 200 concurrent slow requests → all threads blocked → new requests queue\n\n// After virtual threads: unlimited concurrency\n// Each request has its own virtual thread — 10,000 concurrent blocked I/O calls\n// cost the same as 200 platform threads\n\n// Async service calls with CompletableFuture\n@Service\npublic class OrderOrchestrationService {\n\n    public OrderSummary getOrderSummary(String orderId) {\n        // Fire all fetches concurrently — each runs in its own virtual thread\n        CompletableFuture&#x3C;Order> orderFuture =\n            CompletableFuture.supplyAsync(() -> orderService.findById(orderId));\n\n        CompletableFuture&#x3C;Customer> customerFuture =\n            CompletableFuture.supplyAsync(() -> customerService.findById(orderId));\n\n        CompletableFuture&#x3C;List&#x3C;OrderItem>> itemsFuture =\n            CompletableFuture.supplyAsync(() -> itemService.findByOrderId(orderId));\n\n        // Wait for all to complete\n        CompletableFuture.allOf(orderFuture, customerFuture, itemsFuture).join();\n\n        return buildSummary(orderFuture.join(), customerFuture.join(), itemsFuture.join());\n        // Total time: max(order, customer, items) instead of sum\n        // If each takes 50ms: 50ms instead of 150ms\n    }\n}\n</code></pre>\n<p>The key insight here is the comment at the bottom: instead of paying 150ms for three sequential 50ms calls, you pay only 50ms by running them in parallel. This pattern is especially valuable for dashboard endpoints that aggregate data from multiple sources — the user experience improvement is dramatic.</p>\n<h2>Fix 5: Efficient Serialization</h2>\n<p>ObjectMapper creation is expensive. Reuse it. And choose the right format.</p>\n<p>Every time you serialize or deserialize JSON in your application, Jackson uses an <code>ObjectMapper</code>. Creating a new <code>ObjectMapper</code> per request is surprisingly expensive — it's like setting up a translation booth from scratch every time someone needs a word translated, rather than keeping a permanent translator on staff. Spring Boot auto-configures one, but customizing it centrally ensures consistent behavior and maximum reuse.</p>\n<pre><code class=\"language-java\">@Configuration\npublic class SerializationConfig {\n\n    @Bean\n    @Primary\n    public ObjectMapper objectMapper() {\n        return JsonMapper.builder()\n            // Performance\n            .disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n            .disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS)\n            .enable(MapperFeature.DEFAULT_VIEW_INCLUSION)\n            // Date handling\n            .addModule(new JavaTimeModule())\n            // Don't serialize null fields (smaller payload)\n            .serializationInclusion(JsonInclude.Include.NON_NULL)\n            .build();\n    }\n}\n\n// For internal service-to-service: use MessagePack (binary, 30-50% smaller)\n// For high-frequency events: use Avro with schema registry\n</code></pre>\n<p>The <code>NON_NULL</code> inclusion setting alone can reduce your JSON payload size by 20-40% on entities with optional fields, which directly reduces bandwidth and deserialization time on the client. With serialization handled, you now need to instrument your application so you can see the impact of all these changes in production.</p>\n<h2>Production Monitoring Checklist</h2>\n<p>Now that you've applied optimizations, you need to know if they're working — and catch regressions before users notice. Spring Boot Actuator with Prometheus gives you the metrics backbone to build alerts around the most important performance signals.</p>\n<pre><code class=\"language-yaml\">management:\n  endpoints:\n    web:\n      exposure:\n        include: health,info,metrics,prometheus\n  metrics:\n    export:\n      prometheus:\n        enabled: true\n  endpoint:\n    health:\n      show-details: always\n\n# Key metrics to alert on:\n# hikaricp.connections.active > (max_pool * 0.8) → pool saturation\n# jvm.gc.pause{action=\"end of major GC\"} > 1s → GC pressure\n# http.server.requests p99 > 2s → latency degradation\n# http.server.requests error rate > 1% → error spike\n# system.cpu.usage > 0.8 → CPU saturation\n</code></pre>\n<p>Beyond infrastructure metrics, you want business-level metrics that tie performance directly to outcomes. The following shows how to track order creation latency and volume — metrics that let you correlate performance changes with business impact.</p>\n<pre><code class=\"language-java\">// Custom business metrics\n@Service\npublic class OrderMetrics {\n\n    private final MeterRegistry registry;\n\n    private final Counter orderCreationTotal;\n    private final Timer orderCreationDuration;\n    private final Gauge pendingOrdersGauge;\n\n    public OrderMetrics(MeterRegistry registry) {\n        this.registry = registry;\n        this.orderCreationTotal = Counter.builder(\"orders.created.total\")\n            .description(\"Total orders created\")\n            .tag(\"version\", \"v1\")\n            .register(registry);\n\n        this.orderCreationDuration = Timer.builder(\"orders.creation.duration\")\n            .description(\"Order creation latency\")\n            .register(registry);\n    }\n\n    public Order createOrder(OrderRequest request) {\n        return orderCreationDuration.record(() -> {\n            Order order = orderService.create(request);\n            orderCreationTotal.increment();\n            return order;\n        });\n    }\n}\n</code></pre>\n<p>Wrapping your business logic in a <code>Timer</code> gives you p50, p95, and p99 latency percentiles automatically — this is far more useful than averages because a p99 spike tells you that 1% of your users are having a bad experience even when the average looks fine.</p>\n<h2>The Performance Tuning Priority List</h2>\n<p>With all the fixes covered, here's how to prioritize your effort. This ordering reflects real-world impact: database problems almost always cost 10x more than JVM tuning, so fix the database layer first.</p>\n<pre><code>Impact → Fix\n──────────────────────────────────────────────\nHighest  N+1 queries               → @EntityGraph, JOIN FETCH\n         Missing database indexes  → EXPLAIN ANALYZE + add indexes\n         Absent caching            → Redis @Cacheable on hot reads\n         Small connection pool     → Tune HikariCP max-pool-size\n         Synchronous fan-out       → CompletableFuture.allOf()\n\nMedium   Lazy serialization        → Jackson optimization\n         Full object load          → Projections for list views\n         Per-request computation   → @Scheduled + cache result\n\nLower    JVM GC tuning             → G1GC MaxGCPauseMillis\n         HTTP client timeouts      → Prevent thread starvation\n         Logging verbosity         → INFO in prod, not DEBUG\n</code></pre>\n<p>Performance optimization is detective work: follow the evidence. Measure, find the bottleneck, fix it, measure again. The common mistakes are optimizing in the wrong layer (tuning JVM when the bottleneck is the database) and premature optimization (spending days on a service that handles 50 RPS). Profile first, fix what the profiler shows, and repeat.</p>\n","tableOfContents":[{"id":"step-1-measure-before-tuning","text":"Step 1: Measure Before Tuning","level":2},{"id":"fix-1-connection-pool-tuning-hikaricp","text":"Fix 1: Connection Pool Tuning (HikariCP)","level":2},{"id":"fix-2-eliminate-n1-queries","text":"Fix 2: Eliminate N+1 Queries","level":2},{"id":"fix-3-caching-with-spring-cache-redis","text":"Fix 3: Caching with Spring Cache + Redis","level":2},{"id":"fix-4-async-processing-with-virtual-threads-java-21","text":"Fix 4: Async Processing with Virtual Threads (Java 21)","level":2},{"id":"fix-5-efficient-serialization","text":"Fix 5: Efficient Serialization","level":2},{"id":"production-monitoring-checklist","text":"Production Monitoring Checklist","level":2},{"id":"the-performance-tuning-priority-list","text":"The Performance Tuning Priority List","level":2}]},"relatedPosts":[{"title":"Java Concurrency Patterns: CompletableFuture, Structured Concurrency, and Thread-Safe Design","description":"Production Java concurrency: CompletableFuture pipelines, handling exceptions in async chains, Java 21 structured concurrency, thread-safe collection patterns, and the concurrency bugs that cause data corruption.","date":"2025-07-08","category":"Java","tags":["java","concurrency","completablefuture","virtual threads","java21","thread-safe","async"],"featured":false,"affiliateSection":"java-courses","slug":"java-concurrency-patterns","readingTime":"7 min read","excerpt":"Java concurrency has three eras: raw  and  (Java 1-4), the  framework (Java 5+), and the virtual thread/structured concurrency era (Java 21+). Each era's patterns still exist in production codebases. Understanding all th…"},{"title":"Java Memory Management Deep Dive: Heap, GC, and Production Tuning","description":"How the JVM allocates memory, how G1GC and ZGC work under the hood, heap analysis with JVM tools, and the GC tuning decisions that eliminate latency spikes in production Java services.","date":"2025-06-13","category":"Java","tags":["java","jvm","garbage collection","g1gc","zgc","heap","memory management","performance"],"featured":false,"affiliateSection":"java-courses","slug":"java-memory-management-deep-dive","readingTime":"7 min read","excerpt":"Java's garbage collector is the single biggest source of unexplained latency spikes in production services. A GC pause of 2 seconds is invisible in most logs but visible to every user who happened to make a request durin…"},{"title":"Spring Security OAuth2 and JWT: Production Implementation Guide","description":"Complete Spring Security OAuth2 implementation: JWT token validation, Resource Server configuration, method-level security, custom UserDetailsService, refresh token rotation, and the security pitfalls that lead to authentication bypasses.","date":"2025-06-03","category":"Java","tags":["spring security","oauth2","jwt","spring boot","authentication","authorization","java","security"],"featured":false,"affiliateSection":"java-courses","slug":"spring-security-oauth2-jwt","readingTime":"7 min read","excerpt":"Spring Security is one of the most powerful and most misunderstood frameworks in the Java ecosystem. Its flexibility is its strength — and its complexity. Misconfigured security is worse than no security, because it give…"}]},"__N_SSG":true}