{"pageProps":{"post":{"title":"System Design: Distributed Rate Limiter — Token Bucket vs Sliding Window","description":"Design a rate limiter that handles millions of requests across distributed servers. Compare token bucket, leaky bucket, fixed window, and sliding window algorithms with Redis-backed implementations.","date":"2025-01-08","category":"System Design","tags":["system design","rate limiting","redis","distributed systems","api"],"featured":false,"affiliateSection":"system-design-courses","slug":"system-design-rate-limiter","readingTime":"10 min read","excerpt":"Rate limiting is deceptively simple in concept and surprisingly tricky in distributed systems. Every API at scale — Stripe, GitHub, Twitter — implements rate limiting. Done wrong, it allows bursts that overwhelm backends…","contentHtml":"<p>Rate limiting is deceptively simple in concept and surprisingly tricky in distributed systems. Every API at scale — Stripe, GitHub, Twitter — implements rate limiting. Done wrong, it allows bursts that overwhelm backends. Done right, it's invisible to legitimate users and impenetrable to abusers.</p>\n<p>This article covers every algorithm, compares their tradeoffs, and shows production-ready Redis implementations.</p>\n<h2>Why Rate Limiting?</h2>\n<ul>\n<li><strong>DoS protection</strong>: Prevent any single client from consuming all resources</li>\n<li><strong>Cost control</strong>: Metered APIs bill per request — enforce usage quotas</li>\n<li><strong>Fairness</strong>: Ensure no tenant starves others in a shared system</li>\n<li><strong>Backend protection</strong>: Databases and downstream services have capacity limits</li>\n</ul>\n<h2>Algorithm Comparison</h2>\n<p>Choosing the right algorithm is not about technical preference — it is about which failure mode you can tolerate. Each algorithm below trades memory, accuracy, and burst behavior in different ways. Understanding the tradeoffs lets you pick the right one for your specific workload.</p>\n<h3>Fixed Window Counter</h3>\n<p>Divide time into fixed windows (e.g., 1-minute buckets). Count requests per window. This approach is appealing because it requires only a single integer counter per client per window, but it has a dangerous edge case that has caused real production incidents.</p>\n<pre><code>Window: 12:00:00 - 12:01:00\n  Client A: 95 requests ✓\n  Client A: 96th request at 12:00:59 ✗ (over limit of 95)\n\nProblem: Boundary burst\n  Client A sends 95 requests at 12:00:55 ✓\n  Client A sends 95 requests at 12:01:05 ✓\n  = 190 requests in 10 seconds (2x the intended rate)\n</code></pre>\n<p><strong>Verdict</strong>: Simple, but the boundary burst is a real vulnerability.</p>\n<h3>Sliding Window Log</h3>\n<p>Record the exact timestamp of every request. Count requests within the last N seconds. This approach is conceptually the most correct — there are no windows and no boundary artifacts — but the memory cost grows linearly with request volume.</p>\n<pre><code>Rate limit: 100 requests per minute\n\nAt 12:01:30:\n  Log: [12:00:31, 12:00:45, ..., 12:01:28, 12:01:29, 12:01:30]\n  Filter to last 60 seconds: count requests since 12:00:30\n  If count &#x3C; 100: allow, else reject\n</code></pre>\n<p><strong>Verdict</strong>: Accurate, no boundary bursts. Memory-intensive (store all timestamps). Impractical for >10K RPS.</p>\n<h3>Sliding Window Counter</h3>\n<p>Approximate the sliding window using two fixed window counters and a weighted average. This is the sweet spot for most production systems: it eliminates the boundary burst problem of fixed windows while using only two integers per client instead of a full timestamp log.</p>\n<pre><code>Rate limit: 100 requests/minute\n\nAt 12:01:45 (45 seconds into current window):\n  Previous window (12:00 - 12:01): 80 requests\n  Current window (12:01 - 12:02): 30 requests so far\n\n  Weight of previous window = (60 - 45) / 60 = 25%\n  Estimated requests in last 60s = 80 × 0.25 + 30 = 50\n  Under limit → allow\n</code></pre>\n<p><strong>Verdict</strong>: Memory-efficient, approximation error is &#x3C;0.1% in practice. The best tradeoff for most systems.</p>\n<h3>Token Bucket</h3>\n<p>Tokens accumulate at a fixed rate (refill rate). Each request consumes one token. Burst is allowed up to the bucket capacity. Think of it like a prepaid phone plan: you accumulate credit over time and can spend it in bursts, but you can never spend more than you have.</p>\n<pre><code>Bucket capacity: 10 tokens (max burst)\nRefill rate: 1 token/second\n\nTimeline:\n  T=0: bucket=10 (full), 8 requests → bucket=2\n  T=3: bucket=5 (3 tokens added), 3 requests → bucket=2\n  T=5: bucket=4 (2 tokens added), 6 requests → REJECT (only 4 available)\n\nProperties:\n  - Allows bursting up to bucket capacity\n  - Average rate controlled by refill rate\n  - Intuitive for \"requests per second with burst allowance\"\n</code></pre>\n<p><strong>Verdict</strong>: Best for APIs where controlled bursting is acceptable (e.g., initial page load).</p>\n<h3>Leaky Bucket</h3>\n<p>Requests enter a queue. A worker drains the queue at a fixed rate. Overflow requests are rejected. Unlike the token bucket, which smooths the average rate but allows bursts, the leaky bucket smooths the output rate — it is useful when your downstream system needs a steady, predictable call rate rather than handling bursty traffic.</p>\n<pre><code>Queue size: 10 requests\nDrain rate: 5 requests/second\n\n  Burst of 15 requests arrives:\n  → 10 queued, 5 rejected immediately\n  → Queue drains at 5/s → smooth outgoing traffic\n</code></pre>\n<p><strong>Verdict</strong>: Smoothes traffic for backends that need steady input. Adds latency (queue wait). Use for outbound call shaping, not incoming API protection.</p>\n<h2>Redis Implementation: Token Bucket with Lua</h2>\n<p>The critical requirement for distributed rate limiting: <strong>atomicity</strong>. Read-modify-write must be atomic, or concurrent requests on different servers will both read \"under limit\" and both write \"at limit+1\" — a race condition. Without atomicity, a client could send 10 simultaneous requests on 10 different app servers, and all 10 would pass the rate check before any of them had a chance to decrement the counter.</p>\n<p>Redis Lua scripts run atomically on the Redis server, solving this problem without requiring distributed locks:</p>\n<pre><code class=\"language-lua\">-- token_bucket.lua\n-- KEYS[1]: rate limit key (e.g., \"rate:user:123\")\n-- ARGV[1]: max tokens (bucket capacity)\n-- ARGV[2]: refill rate (tokens per second)\n-- ARGV[3]: current timestamp (Unix seconds with milliseconds)\n-- ARGV[4]: requested tokens (usually 1)\n\nlocal key = KEYS[1]\nlocal max_tokens = tonumber(ARGV[1])\nlocal refill_rate = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\nlocal requested = tonumber(ARGV[4])\n\n-- Get current state\nlocal data = redis.call('HMGET', key, 'tokens', 'last_refill')\nlocal tokens = tonumber(data[1]) or max_tokens\nlocal last_refill = tonumber(data[2]) or now\n\n-- Calculate tokens to add based on elapsed time\nlocal elapsed = math.max(0, now - last_refill)\nlocal tokens_to_add = elapsed * refill_rate\ntokens = math.min(max_tokens, tokens + tokens_to_add)\n\n-- Check if request can be served\nlocal allowed = 0\nif tokens >= requested then\n    tokens = tokens - requested\n    allowed = 1\nend\n\n-- Save state with TTL (auto-cleanup for idle clients)\nredis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)\nredis.call('EXPIRE', key, math.ceil(max_tokens / refill_rate) + 1)\n\nreturn {allowed, math.floor(tokens), math.floor(tokens_to_add)}\n</code></pre>\n<p>The <code>EXPIRE</code> call at the end is a crucial operational detail: it automatically removes rate limit state for idle clients, preventing unbounded Redis memory growth. The TTL is calculated to be just long enough that a fully drained bucket would refill to capacity.</p>\n<p>Now that you have the Lua logic, the Java service below wires it into a callable interface. The key design choice here is passing the current timestamp from the application rather than reading it inside Lua — this keeps the script deterministic and easier to test.</p>\n<pre><code class=\"language-java\">@Service\npublic class TokenBucketRateLimiter {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    private final DefaultRedisScript&#x3C;List> luaScript;\n\n    public TokenBucketRateLimiter() {\n        this.luaScript = new DefaultRedisScript&#x3C;>();\n        this.luaScript.setScriptText(LUA_SCRIPT); // Load from classpath\n        this.luaScript.setResultType(List.class);\n    }\n\n    public RateLimitResult checkLimit(String clientId, RateLimitConfig config) {\n        String key = \"rate:\" + clientId;\n        double now = System.currentTimeMillis() / 1000.0;\n\n        List&#x3C;Long> result = redis.execute(\n            luaScript,\n            List.of(key),\n            String.valueOf(config.getMaxTokens()),\n            String.valueOf(config.getRefillRate()),\n            String.valueOf(now),\n            \"1\"\n        );\n\n        boolean allowed = result.get(0) == 1L;\n        long remainingTokens = result.get(1);\n\n        return new RateLimitResult(allowed, remainingTokens, config.getMaxTokens());\n    }\n}\n</code></pre>\n<h2>Spring Boot Integration: Rate Limit Filter</h2>\n<p>With the core limiter in place, you need to intercept every HTTP request before it reaches your business logic. A servlet filter runs before any controller code and can short-circuit the request with a <code>429</code> response without touching your application logic at all. This separation means you can add rate limiting to any endpoint without modifying it.</p>\n<pre><code class=\"language-java\">@Component\n@Order(Ordered.HIGHEST_PRECEDENCE)\npublic class RateLimitFilter extends OncePerRequestFilter {\n\n    @Autowired\n    private TokenBucketRateLimiter rateLimiter;\n\n    private static final Map&#x3C;String, RateLimitConfig> TIER_CONFIGS = Map.of(\n        \"free\",       new RateLimitConfig(60, 1.0),   // 60 burst, 1 RPS sustained\n        \"pro\",        new RateLimitConfig(600, 10.0),  // 600 burst, 10 RPS sustained\n        \"enterprise\", new RateLimitConfig(6000, 100.0) // 6000 burst, 100 RPS\n    );\n\n    @Override\n    protected void doFilterInternal(\n            HttpServletRequest request,\n            HttpServletResponse response,\n            FilterChain chain) throws ServletException, IOException {\n\n        String clientId = extractClientId(request); // From API key or JWT\n        String tier = extractTier(clientId);\n        RateLimitConfig config = TIER_CONFIGS.getOrDefault(tier, TIER_CONFIGS.get(\"free\"));\n\n        RateLimitResult result = rateLimiter.checkLimit(clientId, config);\n\n        // Always set rate limit headers (RFC 6585)\n        response.setHeader(\"X-RateLimit-Limit\", String.valueOf(config.getMaxTokens()));\n        response.setHeader(\"X-RateLimit-Remaining\", String.valueOf(result.getRemainingTokens()));\n        response.setHeader(\"X-RateLimit-Reset\", String.valueOf(System.currentTimeMillis() / 1000 + 1));\n\n        if (!result.isAllowed()) {\n            response.setStatus(HttpStatus.TOO_MANY_REQUESTS.value());\n            response.setHeader(\"Retry-After\", \"1\");\n            response.getWriter().write(\"{\\\"error\\\": \\\"Rate limit exceeded\\\", \\\"tier\\\": \\\"\" + tier + \"\\\"}\");\n            return;\n        }\n\n        chain.doFilter(request, response);\n    }\n\n    private String extractClientId(HttpServletRequest request) {\n        String apiKey = request.getHeader(\"X-API-Key\");\n        if (apiKey != null) return \"api:\" + apiKey;\n\n        // Fall back to IP-based limiting for unauthenticated requests\n        return \"ip:\" + request.getRemoteAddr();\n    }\n}\n</code></pre>\n<p>Always setting the rate limit headers — even for successful requests — is important because clients use <code>X-RateLimit-Remaining</code> to implement self-throttling. A well-behaved SDK will slow down proactively when remaining tokens run low, reducing the number of rejected requests and improving the overall user experience.</p>\n<h2>Distributed Challenges</h2>\n<p>The single-server implementation above is correct, but distributed systems introduce new problems. The following two challenges are the ones interviewers most commonly expect you to address.</p>\n<h3>Redis Cluster Consistency</h3>\n<p>In a Redis cluster, rate limit keys can sit on different shards. Ensure the key lands on the same shard using hash tags:</p>\n<pre><code class=\"language-java\">// Without hash tag: different shards for different clients (fine)\nString key = \"rate:\" + clientId;\n\n// With hash tag: force all rate limit keys to same shard (avoid for large deployments)\nString key = \"{rate}:\" + clientId;\n\n// Better: use consistent hashing at the application level\n// Partition clients by modulo and direct to specific Redis nodes\n</code></pre>\n<h3>Multi-Region Rate Limiting</h3>\n<p>Strict global rate limiting requires cross-region coordination — expensive. In practice, use <strong>local + global</strong> hybrid. Think of it like allocating traffic quotas across airport terminals: each terminal enforces its own cap, and the central authority adjusts allocations periodically rather than approving every single passenger.</p>\n<pre><code>Approach: Divide global limit across regions proportionally\n\nGlobal limit: 1000 requests/minute\n  Region us-east-1: 500 req/min (50% of traffic)\n  Region eu-west-1: 300 req/min (30%)\n  Region ap-south-1: 200 req/min (20%)\n\nEach region enforces its limit independently.\nSync global counters asynchronously every 10 seconds.\nA client can exceed global limit by up to 10 seconds × regional rate — acceptable for most use cases.\n</code></pre>\n<p>The 10-second sync window means a determined attacker can exceed the global limit by at most 10 seconds worth of regional traffic — a bounded and acceptable overshoot for most business requirements.</p>\n<h2>Where to Apply Rate Limiting</h2>\n<p>Rate limiting is most effective when applied at multiple layers simultaneously, creating defense in depth. Each layer handles a different threat model, and together they prevent both accidental and intentional overload.</p>\n<pre><code>Request path:\n  Client → CDN → API Gateway → Load Balancer → Service → Database\n\nRate limiting layers:\n  CDN:         IP-based blocking for known abusers (bot traffic)\n  API Gateway: Per-client limits (recommended primary enforcement point)\n  Service:     Per-endpoint limits (e.g., expensive endpoints get stricter limits)\n  Database:    Connection pool limits (implicit rate limiting)\n\nRecommendation:\n  - API Gateway for business rate limits (per API key, per tier)\n  - Service layer for resource protection (expensive endpoints)\n  - Both together for defense in depth\n</code></pre>\n<p>Rate limiting is a foundational API design concern. The token bucket algorithm with Redis Lua provides the best combination of correctness, performance, and operational simplicity. The boundary burst of fixed window algorithms has caused real production incidents — don't ship that in critical systems.</p>\n","tableOfContents":[{"id":"why-rate-limiting","text":"Why Rate Limiting?","level":2},{"id":"algorithm-comparison","text":"Algorithm Comparison","level":2},{"id":"fixed-window-counter","text":"Fixed Window Counter","level":3},{"id":"sliding-window-log","text":"Sliding Window Log","level":3},{"id":"sliding-window-counter","text":"Sliding Window Counter","level":3},{"id":"token-bucket","text":"Token Bucket","level":3},{"id":"leaky-bucket","text":"Leaky Bucket","level":3},{"id":"redis-implementation-token-bucket-with-lua","text":"Redis Implementation: Token Bucket with Lua","level":2},{"id":"spring-boot-integration-rate-limit-filter","text":"Spring Boot Integration: Rate Limit Filter","level":2},{"id":"distributed-challenges","text":"Distributed Challenges","level":2},{"id":"redis-cluster-consistency","text":"Redis Cluster Consistency","level":3},{"id":"multi-region-rate-limiting","text":"Multi-Region Rate Limiting","level":3},{"id":"where-to-apply-rate-limiting","text":"Where to Apply Rate Limiting","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}