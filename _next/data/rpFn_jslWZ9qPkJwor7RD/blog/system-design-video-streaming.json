{"pageProps":{"post":{"title":"System Design: Video Streaming Platform at Netflix Scale","description":"Design a video streaming platform handling 250M users and 15% of global internet traffic. Covers video transcoding pipeline, CDN architecture, adaptive bitrate streaming, and recommendation systems.","date":"2025-03-19","category":"System Design","tags":["system design","video streaming","cdn","aws","distributed systems","hls"],"featured":false,"affiliateSection":"system-design-courses","slug":"system-design-video-streaming","readingTime":"11 min read","excerpt":"Netflix accounts for 15% of global internet traffic. At peak, it serves 250 million concurrent streams — each stream adapting in real-time to network conditions, each segment served from servers 20ms away from the viewer…","contentHtml":"<p>Netflix accounts for 15% of global internet traffic. At peak, it serves 250 million concurrent streams — each stream adapting in real-time to network conditions, each segment served from servers 20ms away from the viewer. The architecture behind this solves problems that span encoding theory, distributed systems, network optimization, and machine learning.</p>\n<h2>Requirements</h2>\n<p><strong>Functional:</strong></p>\n<ul>\n<li>Users upload videos (creators) or watch licensed content</li>\n<li>Video plays within 2 seconds of click (fast start)</li>\n<li>Adaptive quality (auto-adjusts 360p → 1080p → 4K based on bandwidth)</li>\n<li>Resume playback across devices</li>\n<li>Offline download</li>\n<li>Personalized recommendations</li>\n</ul>\n<p><strong>Non-Functional:</strong></p>\n<ul>\n<li>250M DAU, 500M total subscribers</li>\n<li>15B streaming hours per month</li>\n<li>P99 start time &#x3C; 2 seconds</li>\n<li>Rebuffering rate &#x3C; 0.5% (buffer = video pauses to load = terrible UX)</li>\n<li>Upload: encode 1 hour of 4K video in &#x3C; 30 minutes</li>\n</ul>\n<h2>The Video Pipeline: Upload to Playback</h2>\n<p>Before going deep on any individual component, it helps to see the entire pipeline end to end. Video streaming has two distinct data flows that operate independently: the upload and transcoding path (happens once per video), and the playback path (happens billions of times per video). Understanding this asymmetry — encode once, serve forever — is the core architectural principle that makes the economics work.</p>\n<pre><code>Creator Upload                    Viewer Playback\n      │                                  │\n      ▼                                  ▼\nRaw video file              ┌─── CDN Edge (&#x3C; 20ms away)\n (H.264 4K 50GB)            │         │\n      │                     │         ▼\n      ▼                     │    Adaptive Bitrate\n Ingest Service ──► S3      │    Streaming Player\n      │           (raw)     │    (requests appropriate\n      ▼                     │     quality segment)\nTranscoding Farm ──────────►│\n (parallel workers)         │\n  - 360p @ 500kbps          └─── Origin servers (S3 + CDN origin)\n  - 720p @ 2.5Mbps\n  - 1080p @ 5Mbps\n  - 4K @ 25Mbps\n  - Audio: AAC, Dolby Atmos\n      │\n      ▼\n HLS/DASH manifest + segments\n stored in S3 → CDN\n</code></pre>\n<p>The right side of this diagram — CDN edge serving segments directly to viewers — is where nearly all viewer requests land. The origin servers (S3) exist primarily to fill the CDN cache on first access, not to serve ongoing traffic. Getting to a 98% CDN cache hit rate is what makes serving 250 million concurrent streams economically feasible.</p>\n<h2>Video Transcoding Pipeline</h2>\n<p>Transcoding is the most compute-intensive operation in the system, and it must be done before a video can be watched by anyone. A one-hour 4K video encodes into five quality levels plus multiple audio tracks, and each encoding job takes significant CPU time. The solution is to parallelize across many workers simultaneously rather than processing each quality level sequentially on one machine.</p>\n<pre><code class=\"language-java\">// Distributed transcoding: break 1 video into many parallel jobs\n@Service\npublic class VideoTranscodingOrchestrator {\n\n    @Autowired\n    private SqsClient sqs;\n\n    @Autowired\n    private S3Client s3;\n\n    public void transcodeVideo(VideoUploadedEvent event) {\n        String videoId = event.getVideoId();\n        String rawS3Key = event.getRawS3Key();\n\n        // Define all required output formats\n        List&#x3C;TranscodeJob> jobs = List.of(\n            new TranscodeJob(videoId, \"360p\",  500_000,  640,  360),\n            new TranscodeJob(videoId, \"480p\",  1_000_000, 854, 480),\n            new TranscodeJob(videoId, \"720p\",  2_500_000, 1280, 720),\n            new TranscodeJob(videoId, \"1080p\", 5_000_000, 1920, 1080),\n            new TranscodeJob(videoId, \"4k\",   25_000_000, 3840, 2160),\n            // Audio tracks\n            new TranscodeJob(videoId, \"audio_aac\",   128_000, 0, 0),\n            new TranscodeJob(videoId, \"audio_dolby\", 640_000, 0, 0)\n        );\n\n        // Send all jobs to SQS — transcoding workers pick up in parallel\n        jobs.forEach(job -> {\n            sqs.sendMessage(SendMessageRequest.builder()\n                .queueUrl(TRANSCODING_QUEUE_URL)\n                .messageBody(serialize(job))\n                .messageGroupId(videoId)  // FIFO queue: group by video\n                .build());\n        });\n\n        // Track completion — send HLS manifest when all renditions done\n        jobTracker.initializeVideoTracking(videoId, jobs.size());\n    }\n}\n\n// FFmpeg-based transcoding worker (runs on EC2 Spot instances — 90% cheaper)\n@Component\npublic class TranscodingWorker {\n\n    @SqsListener(queueNames = \"${transcoding.queue.url}\")\n    public void processJob(TranscodeJob job) {\n        // Download raw video chunk from S3\n        File rawFile = downloadFromS3(job.getRawS3Key());\n\n        // FFmpeg transcoding\n        ProcessBuilder pb = new ProcessBuilder(\n            \"ffmpeg\",\n            \"-i\", rawFile.getAbsolutePath(),\n            \"-vf\", \"scale=\" + job.getWidth() + \":\" + job.getHeight(),\n            \"-c:v\", \"libx264\",\n            \"-b:v\", job.getBitrate() + \"k\",\n            \"-preset\", \"slow\",          // Better compression, slower\n            \"-crf\", \"23\",               // Quality level\n            \"-c:a\", \"aac\",\n            \"-b:a\", \"128k\",\n            // HLS segmentation: 6-second segments\n            \"-f\", \"hls\",\n            \"-hls_time\", \"6\",\n            \"-hls_playlist_type\", \"vod\",\n            \"-hls_segment_filename\", outputDir + \"/%d.ts\",\n            outputDir + \"/playlist.m3u8\"\n        );\n\n        executeAndWait(pb);\n\n        // Upload segments and playlist to S3\n        uploadSegmentsToS3(outputDir, job.getVideoId(), job.getRendition());\n\n        // Notify orchestrator this rendition is complete\n        jobTracker.markComplete(job.getVideoId(), job.getRendition());\n    }\n}\n</code></pre>\n<p>Running transcoding workers on EC2 Spot instances rather than on-demand instances is a 90% cost reduction — and transcoding is a perfect fit for Spot because jobs are interruptible: if a Spot instance is reclaimed, SQS retains the unacknowledged job and another worker picks it up. The <code>-preset slow</code> flag in FFmpeg deliberately trades encoding speed for file size: a slower preset produces a smaller output file at the same quality level, which reduces S3 storage costs and CDN bandwidth for every view of the video.</p>\n<h2>HLS Manifest: Adaptive Bitrate Streaming</h2>\n<p>The output of transcoding is not just video files — it is a manifest structure that the player uses to navigate between quality levels on the fly. HLS (HTTP Live Streaming) organizes this into a two-level hierarchy: a master playlist that lists all available quality levels, and per-rendition playlists that list individual 6-second segments.</p>\n<pre><code># Master playlist (index.m3u8) — served to player\n#EXTM3U\n#EXT-X-VERSION:3\n\n# Renditions ordered by bandwidth\n#EXT-X-STREAM-INF:BANDWIDTH=500000,RESOLUTION=640x360\nhttps://cdn.example.com/videos/abc123/360p/playlist.m3u8\n\n#EXT-X-STREAM-INF:BANDWIDTH=2500000,RESOLUTION=1280x720\nhttps://cdn.example.com/videos/abc123/720p/playlist.m3u8\n\n#EXT-X-STREAM-INF:BANDWIDTH=5000000,RESOLUTION=1920x1080\nhttps://cdn.example.com/videos/abc123/1080p/playlist.m3u8\n\n#EXT-X-STREAM-INF:BANDWIDTH=25000000,RESOLUTION=3840x2160\nhttps://cdn.example.com/videos/abc123/4k/playlist.m3u8\n\n# 360p playlist (360p/playlist.m3u8)\n#EXTM3U\n#EXT-X-VERSION:3\n#EXT-X-TARGETDURATION:6\n#EXT-X-PLAYLIST-TYPE:VOD\n\n#EXTINF:6.0,\n0.ts\n#EXTINF:6.0,\n1.ts\n...\n#EXTINF:4.2,\n600.ts\n#EXT-X-ENDLIST\n</code></pre>\n<p>The HLS player measures download speed. If 1080p segment downloads in 2 seconds but should play in 6 seconds → ample buffer → stay at 1080p. If 1080p segment downloads in 7 seconds but should play in 6 seconds → falling behind → switch to 720p. This adaptation happens every 6 seconds.</p>\n<p>The 6-second segment duration is a deliberate engineering choice that balances two competing concerns: shorter segments (2-3 seconds) make quality switching more responsive but increase the number of HTTP requests and add overhead; longer segments (10-15 seconds) reduce HTTP overhead but make quality adaptation sluggish. Six seconds is the sweet spot that most major streaming platforms have converged on.</p>\n<h2>CDN Architecture</h2>\n<p>The CDN is where the \"encode once, serve forever\" principle pays off. A video segment is a fixed bytes-on-disk file that never changes after encoding, which means it can be cached indefinitely at every CDN edge node worldwide. Once a segment is cached at an edge location, every subsequent request for that segment from that region costs nothing in compute and delivers in under 5ms.</p>\n<pre><code>Global CDN (CloudFront / Akamai):\n  - 400+ edge locations worldwide\n  - Video segments cached at edge: 95% cache hit rate\n  - Cache hit: 5ms latency\n  - Cache miss → origin S3: 50-200ms (once)\n\nCache key: /videos/{videoId}/{rendition}/{segment}.ts\n\nCache strategy:\n  - Segments: immutable (never change), cache forever (Cache-Control: max-age=31536000)\n  - Manifests: short TTL during encoding (5 seconds), long TTL after (1 hour)\n  - Playlists: short TTL during live streams, long after VOD is complete\n  - Thumbnails: 1 day TTL\n\nCDN hit rate target: 98%\n  - Popular videos: 100% (100M+ viewers = every edge has it cached)\n  - Long-tail (indie content): 60-80% (less popular = cache misses)\n  - First 1% views: always miss (fill the cache)\n</code></pre>\n<p>The distinction between <code>max-age=31536000</code> (one year) for segments versus a 1-hour TTL for manifests reflects their different mutability: segments are truly immutable and can be cached forever, but manifests may need to be updated if metadata changes or if the encoding pipeline re-processes the video. If you served stale segments it would not matter, but a stale manifest pointing to old segments could break playback.</p>\n<h2>Video Start Time Optimization</h2>\n<p>Start time is the user experience metric that matters most before playback begins. Think of it like launching a rocket: every millisecond of delay before the first frame is felt more acutely than buffering that happens 10 minutes into a show. The optimizations below stack on top of each other, each shaving latency from a different part of the startup sequence.</p>\n<pre><code>P99 &#x3C; 2 seconds requires:\n\n1. Fast DNS resolution: Anycast DNS → nearest CDN PoP\n   Impact: 50ms → 5ms\n\n2. Video manifest prefetch: when user hovers on thumbnail,\n   fetch and cache the master playlist\n   Impact: 200ms → 0ms (already cached)\n\n3. Pre-buffered thumbnails: before user clicks play,\n   stream still frames (video preview on hover)\n   Impact: perceived start time → \"instant\"\n\n4. Start with lowest quality: request 360p first segment immediately,\n   then upgrade quality\n   Impact: first frame in 200ms (360p = 375KB), then upgrade\n\n5. ABR algorithm: start low, ramp up quickly\n   - First segment: lowest quality (fast start)\n   - Next 3 segments: upgrade if bandwidth allows\n   - After 30 seconds: stable quality\n\nStart time breakdown:\n  DNS lookup: 5ms\n  TCP + TLS handshake: 30ms\n  Manifest download: 20ms\n  First segment download: 100ms (360p, 375KB, 30Mbps)\n  Total: 155ms → well under 2 second target\n</code></pre>\n<p>The manifest prefetch on hover is worth highlighting as one of the highest-leverage optimizations available: users typically hover over a thumbnail for 200-500ms before clicking, and during that window you can silently fetch the manifest and cache it locally. By the time the user clicks play, the manifest round-trip is already done and the player can immediately request the first segment.</p>\n<h2>Recommendation System Architecture</h2>\n<p>Once a user finishes watching, the next challenge is showing them something they will want to watch next. The recommendation system is a two-stage pipeline designed around a fundamental tradeoff: generating the best possible 5 recommendations from 200 million titles is too slow to do in real time, but doing it entirely in batch misses real-time signals like what the user just watched 10 minutes ago.</p>\n<pre><code>Data pipeline:\n  User events (play, pause, skip, complete, rate) → Kafka → Feature store\n\nTwo-stage recommendation:\n  Stage 1: Candidate generation (fast, broad)\n    - Collaborative filtering: \"users like you watched X\"\n    - Content-based: \"you watched thrillers → more thrillers\"\n    - Trending: popular content in your region\n    - Output: 500-1000 candidates per user\n\n  Stage 2: Ranking (slower, precise)\n    - Neural network ranking model\n    - Features: user history, content metadata, time of day, device\n    - Output: top 20 ranked recommendations\n\nServing:\n  - Pre-compute recommendations nightly (batch job)\n  - Store in Redis: user_id → [ranked video IDs]\n  - Real-time adjustments: boost content just watched by friends\n  - A/B test: 20% users get new ranking model vs baseline\n</code></pre>\n<p>The two-stage architecture is a pattern you will see throughout large-scale ML systems: a fast, approximate retrieval stage narrows millions of candidates to hundreds, then a slow, precise ranking stage applies a sophisticated model to that smaller set. Trying to run the precise ranking model against all 200 million titles for every user every second would require thousands of GPUs; running it against 500-1000 pre-filtered candidates is economical.</p>\n<h2>Storage Cost Optimization</h2>\n<p>Video storage is where the economics of streaming become genuinely challenging. The numbers below reveal why a naive approach of storing all quality levels for all content is financially unsustainable, and why tiered storage and newer codecs are not just nice-to-haves but existential requirements for the business model.</p>\n<pre><code>Content stored at multiple quality levels:\n  720p (avg 2.5 Mbps, 2hr movie = 2.25GB) × 200M titles = 450 PB\n  + 4 more quality levels × 200M titles = 2.25 EB\n\nThat's impossibly expensive. Netflix's actual strategy:\n\n1. S3 Intelligent-Tiering:\n   - Hot tier (frequently accessed): $0.023/GB/month\n   - Cold tier (infrequently accessed, 90+ days): $0.0025/GB/month\n   - Long tail content automatically moves to cold tier\n\n2. Per-title quality decision:\n   - High-demand titles: store all quality levels at edge\n   - Low-demand titles: store only in origin (380p, 720p)\n   - Zero-demand titles: transcode on-demand\n\n3. HEVC (H.265) encoding for new content:\n   - 40-50% smaller files vs H.264 at same quality\n   - Requires hardware decoder (all modern devices support it)\n\n4. AV1 codec (open, royalty-free):\n   - 30% smaller than HEVC\n   - Google/Netflix co-developed\n   - Rolling deployment on supported devices\n</code></pre>\n<p>The combination of S3 Intelligent-Tiering and per-title quality decisions means the system is essentially self-optimizing on cost: popular titles migrate to hot-tier storage and get all quality levels at edge CDN, while obscure titles sit in cold storage and are only fetched on the rare occasion someone watches them. A 90% reduction in storage price for cold-tier content changes the math on keeping long-tail content in the catalog at all.</p>\n<p>The lessons from video streaming architecture apply broadly: pre-computation beats real-time computation (transcode once, serve forever), geographic distribution beats raw speed (CDN edge beats fast origin), and partial results beat waiting (low quality first, upgrade while playing). These principles show up in recommendation systems, image delivery, web performance — anywhere latency matters more than perfection.</p>\n","tableOfContents":[{"id":"requirements","text":"Requirements","level":2},{"id":"the-video-pipeline-upload-to-playback","text":"The Video Pipeline: Upload to Playback","level":2},{"id":"video-transcoding-pipeline","text":"Video Transcoding Pipeline","level":2},{"id":"hls-manifest-adaptive-bitrate-streaming","text":"HLS Manifest: Adaptive Bitrate Streaming","level":2},{"id":"cdn-architecture","text":"CDN Architecture","level":2},{"id":"video-start-time-optimization","text":"Video Start Time Optimization","level":2},{"id":"recommendation-system-architecture","text":"Recommendation System Architecture","level":2},{"id":"storage-cost-optimization","text":"Storage Cost Optimization","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}