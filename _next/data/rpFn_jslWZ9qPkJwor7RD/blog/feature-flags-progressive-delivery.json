{"pageProps":{"post":{"title":"Feature Flags and Progressive Delivery: Safe Releases at Scale","description":"Production feature flag implementation: flag evaluation architecture, percentage rollouts, user targeting, kill switches, flag lifecycle management, OpenFeature SDK, LaunchDarkly patterns, and how progressive delivery eliminates release fear.","date":"2025-05-19","category":"System Design","tags":["feature flags","progressive delivery","deployment","system design","spring boot","launchdarkly","openfeature"],"featured":false,"affiliateSection":"system-design-courses","slug":"feature-flags-progressive-delivery","readingTime":"7 min read","excerpt":"Feature flags — also called feature toggles or feature switches — decouple code deployment from feature release. You deploy code to production with the new feature disabled. When you're ready, you enable it for 1% of use…","contentHtml":"<p>Feature flags — also called feature toggles or feature switches — decouple code deployment from feature release. You deploy code to production with the new feature disabled. When you're ready, you enable it for 1% of users, watch metrics, enable for 10%, verify, then 100%. If something goes wrong, you flip a switch and it's gone — no rollback deploy, no database migration, no 2am deployment.</p>\n<p>At scale, feature flags become a core part of your deployment infrastructure. Companies like Facebook, LinkedIn, and Spotify deploy dozens of times per day, with every significant change behind a feature flag. This article covers the implementation patterns, not the philosophy.</p>\n<h2>Flag Types and Use Cases</h2>\n<pre><code>Release flags:\n  → Hide incomplete features in production (trunk-based development)\n  → \"newCheckoutFlow\": false in production, code exists but inaccessible\n\nKill switches:\n  → Emergency disablement of a feature causing incidents\n  → \"paymentService\": false → fallback to manual processing\n\nOps flags:\n  → Control infrastructure behavior (circuit breakers, cache TTLs)\n  → \"enableRedisCaching\": true/false\n\nExperiment flags:\n  → A/B testing: 50% users see variant A, 50% see variant B\n  → \"checkoutButtonColor\": { \"control\": \"blue\", \"treatment\": \"green\" }\n\nPermission flags:\n  → Enable features for specific users (beta, premium, internal)\n  → \"advancedAnalytics\": enabled for { tier: \"enterprise\" }\n</code></pre>\n<h2>Architecture: Evaluation and Storage</h2>\n<pre><code>Flag evaluation architecture:\n\nSDK (in application) → Local cache (in-memory)\n                              ↑\n                    Background polling/streaming\n                              ↑\n                    Flag service (LaunchDarkly / Flagsmith / internal)\n                              ↑\n                    Flag storage (database / config service)\n\nCritical design constraint: Flag evaluation must be SYNCHRONOUS and LOCAL.\nCalling a remote API for each flag evaluation adds latency to every request.\nThe SDK maintains a local in-memory copy of all flags, refreshed every 30s\nor via streaming (Server-Sent Events). Evaluation is a local lookup — &#x3C; 1ms.\n</code></pre>\n<h2>OpenFeature: Vendor-Neutral Flag SDK</h2>\n<p><a href=\"https://openfeature.dev\">OpenFeature</a> is a CNCF standard for feature flag evaluation. Use it to avoid vendor lock-in:</p>\n<pre><code class=\"language-java\">// Maven:\n// openfeature-java-sdk + provider (LaunchDarkly, Flagsmith, etc.)\n\n@Configuration\npublic class FeatureFlagConfig {\n\n    @Bean\n    public OpenFeatureAPI openFeatureAPI() {\n        // Provider can be swapped without changing application code:\n        FeatureProvider provider = new LaunchDarklyProvider(\n            new LDConfig.Builder()\n                .offline(false)\n                .build(),\n            new LDClient(System.getenv(\"LAUNCHDARKLY_SDK_KEY\"))\n        );\n\n        OpenFeatureAPI api = OpenFeatureAPI.getInstance();\n        api.setProvider(provider);\n        return api;\n    }\n\n    @Bean\n    public Client featureFlagClient(OpenFeatureAPI api) {\n        return api.getClient(\"order-service\");\n    }\n}\n\n// Flag evaluation in services:\n@Service\npublic class CheckoutService {\n\n    @Autowired\n    private Client featureFlags;\n\n    public CheckoutResult checkout(CartRequest cart, User user) {\n        // Create evaluation context from the user:\n        EvaluationContext context = new ImmutableContext(user.getId(), Map.of(\n            \"email\", Value.objectToValue(user.getEmail()),\n            \"tier\", Value.objectToValue(user.getTier()),\n            \"region\", Value.objectToValue(user.getRegion()),\n            \"betaUser\", Value.objectToValue(user.isBetaOptIn())\n        ));\n\n        // Boolean flag evaluation (with default):\n        boolean useNewCheckoutFlow = featureFlags.getBooleanValue(\n            \"new-checkout-flow\",\n            false,  // Default: old flow (fail-safe)\n            context\n        );\n\n        if (useNewCheckoutFlow) {\n            return newCheckoutService.process(cart);\n        } else {\n            return legacyCheckoutService.process(cart);\n        }\n    }\n\n    // Multivariate flag (A/B/C testing):\n    public String getRecommendationAlgorithm(User user) {\n        EvaluationContext ctx = buildContext(user);\n        return featureFlags.getStringValue(\n            \"recommendation-algorithm\",\n            \"collaborative-filtering\",  // Default\n            ctx\n        );\n        // Returns: \"collaborative-filtering\", \"content-based\", or \"hybrid\"\n        // based on targeting rules configured in the flag service\n    }\n}\n</code></pre>\n<h2>Percentage Rollouts</h2>\n<pre><code>Percentage rollout implementation:\n\nUser ID: \"user-12345\"\nFlag name: \"new-checkout-flow\"\nTarget percentage: 10%\n\nHash: SHA256(\"user-12345\" + \"new-checkout-flow\")\n    = a1b2c3d4e5... (deterministic)\n\nBucket: parseInt(hash[0:4], 16) % 10000 = 6521\n\n6521 / 10000 = 65.21% → User is NOT in 10% rollout (65% > 10%)\n\nProperties:\n- Same user always gets same result (consistent experience)\n- Increasing percentage from 10% → 20% adds new users, keeps existing 10% in\n- No server-side state needed — pure function of userId + flagName + percentage\n</code></pre>\n<pre><code class=\"language-java\">// Simple percentage rollout without external flag service:\n@Component\npublic class FeatureFlagEvaluator {\n\n    public boolean isEnabled(String flagName, String userId, int targetPercent) {\n        String input = userId + \":\" + flagName;\n        int hash = Math.abs(MurmurHash3.hash32(input.getBytes())) % 10000;\n        return hash &#x3C; targetPercent * 100;\n    }\n}\n\n// Usage:\nboolean showNewUI = flagEvaluator.isEnabled(\"new-ui\", user.getId(), 15);\n// 15% of users deterministically get the new UI\n</code></pre>\n<h2>Flag Lifecycle: Avoiding \"Flag Debt\"</h2>\n<p>Feature flags accumulate. A codebase with 200 flags — half of which are fully rolled out and forgotten — becomes unmaintainable. Each flag adds a branch in your code; 200 flags means thousands of untested combinations.</p>\n<pre><code>Flag lifecycle stages:\n1. Created     → default false, no targeting\n2. Testing     → enabled for QA/internal users only\n3. Canary      → 1-5% production users\n4. Rollout     → gradual increase: 10% → 25% → 50% → 100%\n5. Cleanup     → flag removed from code, flag config deleted\n</code></pre>\n<p><strong>When a flag reaches 100% rollout (or 0% = permanently disabled), it must be cleaned up.</strong> This means:</p>\n<ol>\n<li>Delete the flag from the flag service</li>\n<li>Remove the flag evaluation from code</li>\n<li>Delete the unused code path</li>\n</ol>\n<pre><code class=\"language-java\">// Code BEFORE cleanup (flag at 100%):\nif (featureFlags.getBooleanValue(\"new-checkout-flow\", false, context)) {\n    return newCheckoutService.process(cart);\n} else {\n    return legacyCheckoutService.process(cart);  // Dead code\n}\n\n// Code AFTER cleanup:\nreturn newCheckoutService.process(cart);  // Permanent — no flag check\n</code></pre>\n<p>Track flag cleanup as a first-class engineering task. Some teams use automatic expiry dates — flags that aren't cleaned up by their expiry date trigger alerts.</p>\n<h2>Kill Switches: Emergency Degradation</h2>\n<p>Kill switches are flags designed for emergency use — they should be evaluated extremely quickly and fail safe:</p>\n<pre><code class=\"language-java\">@Service\npublic class PaymentService {\n\n    @Autowired\n    private Client featureFlags;\n\n    @Autowired\n    private ManualPaymentService manualPaymentService;\n\n    public PaymentResult processPayment(PaymentRequest request) {\n        // Kill switch: if payment service is having issues, use manual fallback\n        boolean paymentServiceEnabled = featureFlags.getBooleanValue(\n            \"payment-service-enabled\",\n            true,   // Default TRUE — service is enabled by default\n            EvaluationContext.EMPTY  // No user context needed for kill switches\n        );\n\n        if (!paymentServiceEnabled) {\n            log.warn(\"Payment service kill switch active — using manual fallback\");\n            return manualPaymentService.queue(request);\n        }\n\n        return stripeService.charge(request);\n    }\n}\n</code></pre>\n<p>Kill switch defaults must be <strong>safe state</strong> (what behavior is acceptable during an incident):</p>\n<ul>\n<li><code>payment-service-enabled</code>: default <code>true</code> (payments work normally)</li>\n<li><code>new-search-algorithm</code>: default <code>false</code> (new algorithm is disabled by default)</li>\n</ul>\n<p>If the flag service itself is unavailable (network partition, outage), the SDK uses cached values. If no cache exists, it uses the SDK default. Design your defaults for the worst case.</p>\n<h2>Metrics and Flag Evaluation Tracking</h2>\n<pre><code class=\"language-java\">// Track flag evaluations for analysis:\n@Aspect\n@Component\npublic class FeatureFlagMetricsAspect {\n\n    @Autowired\n    private MeterRegistry meterRegistry;\n\n    @Around(\"@annotation(featureFlagCheck)\")\n    public Object trackFlagEvaluation(ProceedingJoinPoint joinPoint,\n                                       FeatureFlagCheck featureFlagCheck) throws Throwable {\n        String flagName = featureFlagCheck.flag();\n        Object result = joinPoint.proceed();\n\n        meterRegistry.counter(\"feature_flag.evaluation\",\n            \"flag\", flagName,\n            \"value\", result.toString()\n        ).increment();\n\n        return result;\n    }\n}\n\n// Use in OpenFeature hooks:\npublic class MetricsHook implements Hook {\n    @Override\n    public void after(HookContext ctx, FlagEvaluationDetails details, Map&#x3C;String, Object> hints) {\n        // Record every flag evaluation with its result and variant\n        metrics.record(\"feature_flag.evaluation\", 1,\n            \"flag\", details.getFlagKey(),\n            \"value\", details.getValue().toString(),\n            \"reason\", details.getReason()\n        );\n    }\n}\n</code></pre>\n<p>Track flag evaluations in Grafana/Datadog, correlated with:</p>\n<ul>\n<li>Error rate (did enabling this flag increase errors?)</li>\n<li>Latency (did the new code path change P99?)</li>\n<li>Business metrics (did the A/B test variant convert better?)</li>\n</ul>\n<p>This telemetry turns flag evaluation into a decision-making tool, not just a deployment switch.</p>\n<h2>Self-Hosted vs. Managed Flag Service</h2>\n<table>\n<thead>\n<tr>\n<th>Factor</th>\n<th>Self-Hosted (Flagsmith, Unleash)</th>\n<th>Managed (LaunchDarkly, Split.io)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cost</td>\n<td>Infrastructure only ($0-$200/mo)</td>\n<td>$0-$50k/year depending on tier</td>\n</tr>\n<tr>\n<td>Setup</td>\n<td>Moderate (deploy + maintain)</td>\n<td>None (SaaS)</td>\n</tr>\n<tr>\n<td>Data privacy</td>\n<td>All data stays in your infra</td>\n<td>Data sent to vendor</td>\n</tr>\n<tr>\n<td>Reliability</td>\n<td>Your responsibility</td>\n<td>Vendor SLA (99.99%+)</td>\n</tr>\n<tr>\n<td>Features</td>\n<td>Core + open source ecosystem</td>\n<td>Full-featured (A/B stats, etc.)</td>\n</tr>\n</tbody>\n</table>\n<p>Self-host Flagsmith or Unleash if: data residency requirements, budget constraints, or &#x3C; 50 flags. Use LaunchDarkly if: large A/B testing programs, many flags, and the engineering time cost of maintaining self-hosted outweighs the subscription cost.</p>\n<p>Feature flags are an investment in deployment safety. The teams that implement them stop having \"all hands on deck\" deployment nights. When something goes wrong, they turn a flag off instead of rolling back a deployment. The operational maturity that comes with progressive delivery — canary deployments, A/B testing, kill switches — is only possible when code changes can be separated from feature releases.</p>\n","tableOfContents":[{"id":"flag-types-and-use-cases","text":"Flag Types and Use Cases","level":2},{"id":"architecture-evaluation-and-storage","text":"Architecture: Evaluation and Storage","level":2},{"id":"openfeature-vendor-neutral-flag-sdk","text":"OpenFeature: Vendor-Neutral Flag SDK","level":2},{"id":"percentage-rollouts","text":"Percentage Rollouts","level":2},{"id":"flag-lifecycle-avoiding-flag-debt","text":"Flag Lifecycle: Avoiding \"Flag Debt\"","level":2},{"id":"kill-switches-emergency-degradation","text":"Kill Switches: Emergency Degradation","level":2},{"id":"metrics-and-flag-evaluation-tracking","text":"Metrics and Flag Evaluation Tracking","level":2},{"id":"self-hosted-vs-managed-flag-service","text":"Self-Hosted vs. Managed Flag Service","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}