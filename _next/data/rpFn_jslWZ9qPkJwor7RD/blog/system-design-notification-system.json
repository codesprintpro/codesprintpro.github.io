{"pageProps":{"post":{"title":"System Design: Building a Notification System for 100 Million Users","description":"Design a scalable notification system that delivers push, email, SMS, and in-app notifications reliably. Covers fan-out strategies, priority queues, delivery guarantees, and user preference management.","date":"2025-02-18","category":"System Design","tags":["system design","notifications","kafka","push notifications","distributed systems"],"featured":false,"affiliateSection":"system-design-courses","slug":"system-design-notification-system","readingTime":"11 min read","excerpt":"Notification systems are deceptively complex. Sending one notification is trivial. Sending 100 million notifications daily — with channel routing, user preferences, delivery tracking, retry logic, and rate limiting — req…","contentHtml":"<p>Notification systems are deceptively complex. Sending one notification is trivial. Sending 100 million notifications daily — with channel routing, user preferences, delivery tracking, retry logic, and rate limiting — requires careful architectural thinking.</p>\n<p>This article designs a notification system like the ones at LinkedIn, Twitter, or Uber.</p>\n<h2>Requirements</h2>\n<p><strong>Functional:</strong></p>\n<ul>\n<li>Send notifications via: push (iOS/Android), email, SMS, in-app</li>\n<li>User preferences per channel and notification type (opt-in/opt-out)</li>\n<li>Templated notifications with variable substitution</li>\n<li>Scheduled notifications (send at specific time)</li>\n<li>Delivery tracking (sent, delivered, read)</li>\n<li>Rate limiting per user (max N notifications per hour)</li>\n</ul>\n<p><strong>Non-Functional:</strong></p>\n<ul>\n<li>100M users, 50M daily active</li>\n<li>1B notifications/day = ~11,600 sends/sec</li>\n<li>Priority tiers: critical (OTP, alerts) &#x3C; 100ms delivery, standard &#x3C; 5 minutes</li>\n<li>At-least-once delivery guarantee</li>\n<li>Idempotent (no duplicates on retry)</li>\n</ul>\n<h2>Back-of-Envelope</h2>\n<p>Before designing the system, it is worth establishing the numbers that drive your architectural decisions. The peak volume of 115,000 notifications per second — ten times the average, during events like flash sales or breaking news — is the figure that determines how many Kafka partitions and worker instances you need.</p>\n<pre><code>Notification volume:\n  1B/day ÷ 86,400s = 11,574 notifications/sec average\n  Peak (10x): ~115,000/sec (e.g., breaking news, flash sale)\n\nStorage:\n  Per notification log: ~500 bytes\n  1B/day × 365 days × 3 years × 500 bytes = ~548 TB total\n  Use tiered storage: hot (recent 30 days) in Cassandra, archive in S3\n\nUser preferences:\n  100M users × 1 KB preference data = 100 GB → fits in Redis + DB\n\nTemplates:\n  ~1,000 notification templates, cached in memory\n</code></pre>\n<p>Notice that user preference data — at 100 GB — is small enough to fit entirely in Redis. This means every preference lookup can be served from memory without ever touching the database, which is critical when you are processing 11,000 notifications per second.</p>\n<h2>System Architecture</h2>\n<p>The architecture separates the system into three logical zones: ingestion (the API that receives requests), routing (Kafka topics that buffer and prioritize), and delivery (channel workers that call third-party providers). This separation means a slow email provider cannot block push notifications, and a surge in marketing messages cannot delay OTP codes.</p>\n<pre><code>Producers (API servers, event processors)\n    │\n    ▼\n┌────────────────────────────────────────────┐\n│         Notification Service API            │\n│  POST /api/v1/notify (single or batch)      │\n│  POST /api/v1/notify/schedule               │\n└────────────────┬───────────────────────────┘\n                 │\n    ┌────────────▼────────────┐\n    │  Validation &#x26; Enrichment │\n    │  - User preference check │\n    │  - Template resolution   │\n    │  - Rate limit check      │\n    └────────────┬────────────┘\n                 │\n    ┌────────────▼────────────────────────────┐\n    │     Priority Kafka Topics                │\n    │  notifications.critical  (P0 - OTP)      │\n    │  notifications.high      (P1 - alerts)   │\n    │  notifications.standard  (P2 - marketing)│\n    └──┬──────────┬──────────────┬────────────┘\n       │          │              │\n  ┌────▼──┐  ┌────▼──┐     ┌────▼────────┐\n  │ Push  │  │ Email │     │  SMS        │\n  │Worker │  │Worker │     │  Worker     │\n  └────┬──┘  └────┬──┘     └────┬────────┘\n       │          │              │\n  ┌────▼──┐  ┌────▼──┐     ┌────▼────────┐\n  │ FCM/  │  │SendGrid│    │  Twilio     │\n  │ APNs  │  │ SES   │     │  SNS        │\n  └───────┘  └───────┘     └────────────┘\n                 │\n    ┌────────────▼────────────┐\n    │   Delivery Tracker       │\n    │   Cassandra (logs)       │\n    │   Redis (real-time state)│\n    └─────────────────────────┘\n</code></pre>\n<p>Using separate Kafka topics for each priority level is what guarantees low latency for critical notifications. Kafka consumers for <code>notifications.critical</code> can be scaled independently and given more CPU resources than the marketing topic, so a backlog of promotional emails never delays an OTP message.</p>\n<h2>Core Service: Notification API</h2>\n<p>The API itself is intentionally thin — it validates the request, checks preferences, resolves the template, and publishes to Kafka. All actual sending happens asynchronously downstream. This design means your API can respond to the caller in milliseconds regardless of how long it takes to deliver the notification, and it decouples the caller from any third-party provider failures.</p>\n<pre><code class=\"language-java\">@RestController\n@RequestMapping(\"/api/v1/notify\")\npublic class NotificationController {\n\n    @Autowired\n    private NotificationOrchestrator orchestrator;\n\n    @PostMapping\n    public ResponseEntity&#x3C;NotificationResponse> sendNotification(\n            @RequestBody @Valid NotificationRequest request) {\n\n        // Enqueue — return immediately, process async\n        String notificationId = orchestrator.enqueue(request);\n\n        return ResponseEntity.accepted()\n            .body(new NotificationResponse(notificationId, \"QUEUED\"));\n    }\n\n    @PostMapping(\"/batch\")\n    public ResponseEntity&#x3C;BatchNotificationResponse> sendBatch(\n            @RequestBody @Valid BatchNotificationRequest request) {\n        // Max 1000 per batch\n        if (request.getRecipients().size() > 1000) {\n            return ResponseEntity.badRequest().build();\n        }\n        List&#x3C;String> ids = orchestrator.enqueueBatch(request);\n        return ResponseEntity.accepted().body(new BatchNotificationResponse(ids));\n    }\n}\n\n@Service\npublic class NotificationOrchestrator {\n\n    @Autowired\n    private UserPreferenceService preferenceService;\n\n    @Autowired\n    private TemplateService templateService;\n\n    @Autowired\n    private RateLimiter rateLimiter;\n\n    @Autowired\n    private KafkaTemplate&#x3C;String, NotificationEvent> kafka;\n\n    public String enqueue(NotificationRequest request) {\n        String notificationId = UUID.randomUUID().toString();\n\n        for (String userId : request.getRecipientIds()) {\n            // 1. Check user preferences\n            UserPreferences prefs = preferenceService.get(userId);\n            List&#x3C;Channel> enabledChannels = prefs.getEnabledChannels(request.getType());\n\n            if (enabledChannels.isEmpty()) continue; // User opted out\n\n            // 2. Rate limit check\n            if (!rateLimiter.isAllowed(userId, request.getPriority())) {\n                log.info(\"Rate limited notification to user {}\", userId);\n                continue;\n            }\n\n            // 3. Resolve template\n            String content = templateService.render(request.getTemplateId(), request.getVariables());\n\n            // 4. Publish to priority-appropriate topic\n            String topic = \"notifications.\" + request.getPriority().name().toLowerCase();\n            NotificationEvent event = NotificationEvent.builder()\n                .id(notificationId)\n                .userId(userId)\n                .channels(enabledChannels)\n                .content(content)\n                .subject(request.getSubject())\n                .createdAt(Instant.now())\n                .build();\n\n            kafka.send(topic, userId, event); // Key = userId → consistent partition\n        }\n\n        return notificationId;\n    }\n}\n</code></pre>\n<p>The Kafka partition key <code>userId</code> is a subtle but important detail: it ensures that all notifications for the same user land on the same partition, which preserves ordering. A user will always see their notifications arrive in the order they were sent, rather than in the order workers happened to process them.</p>\n<h2>Fan-Out Strategy: Push vs Pull</h2>\n<p>With the API and routing in place, the next challenge is how to expand a single event (like a breaking news alert) into millions of per-user notifications efficiently. This is called fan-out, and the right strategy depends on how many recipients an event has.</p>\n<p>For large events (breaking news sent to 50M users), push-to-all is too slow. Use <strong>fan-out on write</strong> for small follower counts, <strong>fan-out on read</strong> for celebrities/broadcasts.</p>\n<pre><code>Fan-out on write (for most notifications):\n  Event occurs → expand recipient list immediately → queue per-user notifications\n  Pro: Fast delivery, simple consumers\n  Con: Hot users/events cause write amplification (celebrity with 10M followers)\n\nFan-out on read (for broadcast/marketing):\n  Store one notification record → users fetch on next app open\n  Pro: No write amplification\n  Con: Delivery delay, users must poll\n\nHybrid (recommended for large scale):\n  - Personal notifications (friend request, order update): fan-out on write\n  - Marketing/broadcast: fan-out on read with batch job\n  - System alerts: fan-out on write with priority queue\n</code></pre>\n<p>Think of the hybrid approach like postal mail versus a newspaper: personal letters are addressed and delivered to each recipient individually (fan-out on write), while newspapers are printed once and picked up at the newsstand by whoever wants one (fan-out on read). The right model depends entirely on the nature of the message.</p>\n<h2>Channel Workers</h2>\n<p>Channel workers are the component that bridges your internal system to third-party providers. Each worker subscribes to all priority topics, handles the platform-specific API calls, and manages failure modes like expired tokens. The most important design principle here is that channel workers must be idempotent — if a worker crashes after successfully sending but before acknowledging the Kafka message, the message will be reprocessed. Tracking the provider's <code>messageId</code> in the delivery tracker is what prevents the user from receiving a duplicate notification.</p>\n<pre><code class=\"language-java\">@Component\npublic class PushNotificationWorker {\n\n    @Autowired\n    private FcmService fcm;\n\n    @Autowired\n    private ApnsService apns;\n\n    @Autowired\n    private DeviceTokenRepository deviceTokens;\n\n    @Autowired\n    private DeliveryTracker tracker;\n\n    @KafkaListener(topics = {\"notifications.critical\", \"notifications.high\", \"notifications.standard\"},\n                   groupId = \"push-worker\",\n                   containerFactory = \"priorityKafkaListenerFactory\")\n    public void process(NotificationEvent event) {\n        List&#x3C;DeviceToken> tokens = deviceTokens.findByUserId(event.getUserId());\n\n        for (DeviceToken token : tokens) {\n            try {\n                String messageId = switch (token.getPlatform()) {\n                    case ANDROID -> fcm.send(token.getToken(), event.getContent(), event.getTitle());\n                    case IOS -> apns.send(token.getToken(), event.getContent(), event.getTitle());\n                };\n\n                tracker.markDelivered(event.getId(), token.getDeviceId(), messageId);\n\n            } catch (TokenExpiredException e) {\n                deviceTokens.deactivate(token.getId());\n            } catch (RateLimitException e) {\n                // Requeue with delay\n                throw new RetryableException(\"FCM rate limited\", e);\n            }\n        }\n    }\n}\n</code></pre>\n<p>Deactivating expired tokens immediately on <code>TokenExpiredException</code> is both a correctness fix and a performance optimization: it prevents future notifications from attempting delivery to a device that has been wiped or uninstalled the app, saving FCM/APNs quota.</p>\n<h2>Delivery Tracking with Cassandra</h2>\n<p>Now that notifications are being sent, you need to track their state. Cassandra is ideal for notification logs: write-heavy, time-series, high cardinality. The choice of Cassandra over PostgreSQL here comes down to the write pattern: at 11,000 notifications per second, every send triggers a status update. PostgreSQL's row-level locking and B-tree indexes do not scale to this write rate without expensive sharding, while Cassandra was designed from the ground up for exactly this kind of append-heavy, partition-key-based access.</p>\n<pre><code class=\"language-sql\">-- Cassandra schema\nCREATE TABLE notification_events (\n    notification_id  UUID,\n    user_id          TEXT,\n    channel          TEXT,    -- push, email, sms, in_app\n    status           TEXT,    -- queued, sent, delivered, read, failed\n    created_at       TIMESTAMP,\n    updated_at       TIMESTAMP,\n    error_message    TEXT,\n    PRIMARY KEY ((user_id), created_at, notification_id)\n) WITH CLUSTERING ORDER BY (created_at DESC)\n  AND default_time_to_live = 7776000;  -- 90 days TTL\n\n-- Query: Get user's recent notifications\nSELECT * FROM notification_events\nWHERE user_id = 'user123'\n  AND created_at > '2025-01-01'\nLIMIT 50;\n</code></pre>\n<p>The <code>CLUSTERING ORDER BY (created_at DESC)</code> means the most recent notifications are physically stored first on disk, so the common \"show me recent activity\" query reads the minimum amount of data without any sorting step.</p>\n<h2>User Preference Management</h2>\n<p>User preferences are the gateway that determines whether a notification is ever sent at all. The Redis-first approach here is critical: checking preferences is on the hot path for every notification, and a database call for each of 11,000 notifications per second would saturate your PostgreSQL cluster immediately.</p>\n<pre><code class=\"language-java\">// Preferences stored in Redis (hot path) + PostgreSQL (source of truth)\n@Service\npublic class UserPreferenceService {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    @Autowired\n    private PreferenceRepository repo;\n\n    public UserPreferences get(String userId) {\n        String cacheKey = \"prefs:\" + userId;\n        String cached = redis.opsForValue().get(cacheKey);\n\n        if (cached != null) {\n            return objectMapper.readValue(cached, UserPreferences.class);\n        }\n\n        UserPreferences prefs = repo.findByUserId(userId)\n            .orElse(UserPreferences.defaults());\n\n        redis.opsForValue().set(cacheKey, objectMapper.writeValueAsString(prefs), Duration.ofHours(1));\n        return prefs;\n    }\n\n    public void update(String userId, UpdatePreferenceRequest request) {\n        repo.upsert(userId, request);\n        redis.delete(\"prefs:\" + userId); // Invalidate cache\n    }\n}\n</code></pre>\n<p>Notice that the <code>update</code> method invalidates the Redis key rather than writing the new value directly. This cache-aside pattern avoids a race condition where two simultaneous updates could leave stale data in Redis — by deleting the key, the next read is guaranteed to fetch the freshest value from the database.</p>\n<h2>Handling Third-Party Rate Limits</h2>\n<p>FCM, APNs, SendGrid, and Twilio all have rate limits. Handle them gracefully. Without this throttle, a burst of outgoing notifications could exhaust your provider's quota within seconds, causing your entire notification system to fail for the rest of the billing period. Think of it like a water tap with a flow restrictor: you control the output rate to stay within the pipe's capacity, even when the demand upstream is higher.</p>\n<pre><code class=\"language-java\">@Component\npublic class AdaptiveRateLimiter {\n\n    private final Map&#x3C;String, TokenBucket> providerBuckets = Map.of(\n        \"FCM\",      new TokenBucket(600_000, 600_000), // 600K/min\n        \"APNs\",     new TokenBucket(300_000, 300_000),\n        \"SendGrid\", new TokenBucket(100,  100),         // 100/sec\n        \"Twilio\",   new TokenBucket(1,    1)            // 1/sec (varies by plan)\n    );\n\n    public void throttle(String provider) {\n        TokenBucket bucket = providerBuckets.get(provider);\n        if (!bucket.tryConsume()) {\n            long waitMs = bucket.getMillisToNextToken();\n            try { Thread.sleep(waitMs); } catch (InterruptedException e) { Thread.currentThread().interrupt(); }\n        }\n    }\n}\n</code></pre>\n<h2>Scheduled Notifications</h2>\n<p>The final capability to add is scheduling, where a notification should be delivered at a specific future time rather than immediately. Redis sorted sets are the perfect data structure for this: the score is the delivery timestamp in milliseconds, so a range query from 0 to \"now\" always returns exactly the notifications that are due, in order.</p>\n<pre><code class=\"language-java\">// Store scheduled notifications in a sorted set (score = delivery timestamp)\npublic void schedule(NotificationRequest request, Instant deliveryTime) {\n    String key = \"scheduled:notifications\";\n    String payload = objectMapper.writeValueAsString(request);\n    redis.opsForZSet().add(key, payload, deliveryTime.toEpochMilli());\n}\n\n// Scheduler: poll every second, process due notifications\n@Scheduled(fixedDelay = 1000)\npublic void processScheduled() {\n    long now = Instant.now().toEpochMilli();\n    Set&#x3C;String> due = redis.opsForZSet().rangeByScore(\"scheduled:notifications\", 0, now);\n\n    for (String payload : due) {\n        NotificationRequest request = objectMapper.readValue(payload, NotificationRequest.class);\n        enqueue(request);\n        redis.opsForZSet().remove(\"scheduled:notifications\", payload);\n    }\n}\n</code></pre>\n<p>The one-second polling interval gives you a delivery accuracy of ±1 second, which is sufficient for virtually all scheduling use cases. If you needed sub-second precision, you would replace the scheduler with a dedicated delay queue backed by a time-wheel data structure.</p>\n<p>The notification system's core challenge is not the happy path — it's the edge cases: user preferences changing mid-delivery, third-party provider outages, duplicate suppression on retry, and fan-out storms during viral events. Design each component to handle failure gracefully, and your notification system will be invisible to users (which is exactly what you want).</p>\n","tableOfContents":[{"id":"requirements","text":"Requirements","level":2},{"id":"back-of-envelope","text":"Back-of-Envelope","level":2},{"id":"system-architecture","text":"System Architecture","level":2},{"id":"core-service-notification-api","text":"Core Service: Notification API","level":2},{"id":"fan-out-strategy-push-vs-pull","text":"Fan-Out Strategy: Push vs Pull","level":2},{"id":"channel-workers","text":"Channel Workers","level":2},{"id":"delivery-tracking-with-cassandra","text":"Delivery Tracking with Cassandra","level":2},{"id":"user-preference-management","text":"User Preference Management","level":2},{"id":"handling-third-party-rate-limits","text":"Handling Third-Party Rate Limits","level":2},{"id":"scheduled-notifications","text":"Scheduled Notifications","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}