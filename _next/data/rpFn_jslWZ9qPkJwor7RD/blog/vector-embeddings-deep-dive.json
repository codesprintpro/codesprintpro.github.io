{"pageProps":{"post":{"title":"Vector Embeddings: The Foundation of Modern AI Applications","description":"Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.","date":"2025-03-11","category":"AI/ML","tags":["ai","embeddings","vector database","semantic search","rag","pgvector","pinecone"],"featured":false,"affiliateSection":"ai-ml-books","slug":"vector-embeddings-deep-dive","readingTime":"11 min read","excerpt":"Every modern AI application — semantic search, RAG, recommendations, duplicate detection — is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantic…","contentHtml":"<p>Every modern AI application — semantic search, RAG, recommendations, duplicate detection — is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantically similar items are geometrically close. This geometric property is what powers \"find me articles about machine learning\" returning results that match the concept, not the exact words.</p>\n<h2>What Are Embeddings?</h2>\n<p>Before diving into code, here is the intuition: imagine placing every piece of text you have ever written onto a map, where texts that mean similar things end up in the same neighborhood. \"Machine learning\" and \"neural networks\" would be a few blocks apart; \"machine learning\" and \"cooking recipes\" would be in different cities. An embedding model learns to draw this map by training on billions of examples of which texts are semantically related. The diagram below shows concretely what changes between traditional keyword search and this map-based approach.</p>\n<pre><code>Traditional keyword search:\n  Query: \"machine learning\"\n  Matches: \"machine learning\", \"Machine Learning\", \"MACHINE LEARNING\"\n  Does NOT match: \"neural networks\", \"deep learning\", \"AI algorithms\"\n\nEmbedding-based semantic search:\n  Query: \"machine learning\"\n  Matches: \"machine learning\", \"neural networks\", \"deep learning\",\n           \"AI algorithms\", \"gradient descent\", \"model training\"\n  Based on: meaning, not string matching\n\nHow:\n  \"machine learning\" → [0.23, -0.45, 0.12, 0.89, ...] (1536 dimensions)\n  \"neural networks\"  → [0.21, -0.43, 0.15, 0.87, ...] (similar vector!)\n  \"cooking recipes\"  → [-0.67, 0.34, -0.89, 0.12, ...] (very different vector)\n\n  Cosine similarity(\"machine learning\", \"neural networks\") = 0.94 (very similar)\n  Cosine similarity(\"machine learning\", \"cooking recipes\") = 0.12 (unrelated)\n</code></pre>\n<p>Each dimension captures some aspect of meaning — not interpretable individually, but the ensemble encodes semantic relationships learned from billions of text examples.</p>\n<h2>Generating Embeddings</h2>\n<p>Now you can see how to generate these vectors in practice. The <code>cosine_similarity</code> function at the bottom is the mathematical equivalent of measuring how close two points are on your semantic map — a score near 1.0 means the texts are neighbors, near 0 means they are unrelated.</p>\n<pre><code class=\"language-python\"># OpenAI text-embedding-3-small (best cost/performance ratio)\nfrom openai import OpenAI\nimport numpy as np\n\nclient = OpenAI()\n\ndef embed(text: str) -> list[float]:\n    \"\"\"Generate embedding for a single text.\"\"\"\n    response = client.embeddings.create(\n        model=\"text-embedding-3-small\",\n        input=text,\n        encoding_format=\"float\"\n    )\n    return response.data[0].embedding\n\ndef embed_batch(texts: list[str]) -> list[list[float]]:\n    \"\"\"Generate embeddings for multiple texts in one API call.\"\"\"\n    # Batch up to 2048 texts per request\n    response = client.embeddings.create(\n        model=\"text-embedding-3-small\",\n        input=texts\n    )\n    return [item.embedding for item in sorted(response.data, key=lambda x: x.index)]\n\n# Dimensions: text-embedding-3-small = 1536, text-embedding-3-large = 3072\n# Cost: $0.02 per million tokens (very cheap)\n\n# Cosine similarity\ndef cosine_similarity(a: list[float], b: list[float]) -> float:\n    a, b = np.array(a), np.array(b)\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Example:\ne1 = embed(\"Java virtual threads\")\ne2 = embed(\"Java lightweight concurrency\")\ne3 = embed(\"Python web scraping\")\n\nprint(cosine_similarity(e1, e2))  # ~0.92 (highly similar)\nprint(cosine_similarity(e1, e3))  # ~0.45 (unrelated)\n</code></pre>\n<p>The results show exactly why embeddings are useful: \"Java virtual threads\" and \"Java lightweight concurrency\" score 0.92 even though they share only the word \"Java\" — the model understood they describe the same concept. A score of 0.45 for the Python web scraping comparison confirms they are semantically unrelated despite being in the same programming domain.</p>\n<h2>Vector Databases: Efficient Similarity Search</h2>\n<p>Brute-force cosine similarity over 1M vectors takes seconds. Vector databases use ANN (Approximate Nearest Neighbor) algorithms to answer \"find 10 most similar vectors\" in milliseconds.</p>\n<p>The tradeoff here is precision for speed: an ANN index might miss the single most-similar vector 1-2% of the time, but it answers in under 10ms instead of several seconds. For search applications, that tradeoff is almost always worth it — your users will not notice the occasional near-miss, but they will absolutely notice a slow response.</p>\n<h3>Option 1: pgvector (PostgreSQL Extension)</h3>\n<p>Best for: existing PostgreSQL users, &#x3C; 10M vectors, want SQL joins with similarity search.</p>\n<p>If you are already running PostgreSQL, pgvector is a compelling choice because it lets you combine similarity search with all the power of SQL — you can filter by category, join against user tables, or run aggregations alongside your vector queries without managing a separate service.</p>\n<pre><code class=\"language-sql\">-- Install pgvector extension\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Table: blog articles with embeddings\nCREATE TABLE articles (\n    id          BIGSERIAL PRIMARY KEY,\n    slug        VARCHAR(200) UNIQUE NOT NULL,\n    title       TEXT NOT NULL,\n    content     TEXT NOT NULL,\n    category    VARCHAR(50),\n    embedding   vector(1536),    -- OpenAI text-embedding-3-small dimensions\n    created_at  TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- IVFFlat index: fast approximate search\n-- lists = sqrt(total_rows) is a good starting point\nCREATE INDEX idx_articles_embedding ON articles\n  USING ivfflat (embedding vector_cosine_ops)\n  WITH (lists = 100);\n\n-- HNSW index (PostgreSQL 15+): better recall, more memory\nCREATE INDEX idx_articles_embedding_hnsw ON articles\n  USING hnsw (embedding vector_cosine_ops)\n  WITH (m = 16, ef_construction = 64);\n\n-- Semantic search query\nSELECT\n    id,\n    title,\n    category,\n    1 - (embedding &#x3C;=> '[0.23, -0.45, 0.12, ...]'::vector) AS similarity\nFROM articles\nWHERE category = 'System Design'           -- Filter BEFORE similarity (important!)\nORDER BY embedding &#x3C;=> '[0.23, ...]'::vector  -- &#x3C;=> = cosine distance\nLIMIT 10;\n\n-- Operators:\n-- &#x3C;=>  cosine distance (use for text — normalized vectors)\n-- &#x3C;->  Euclidean distance\n-- &#x3C;#>  negative inner product (use if vectors are normalized)\n</code></pre>\n<p>Notice the comment \"Filter BEFORE similarity (important!)\". Applying your <code>WHERE category = 'System Design'</code> filter before the vector search dramatically reduces the number of vectors the index needs to scan, giving you both faster queries and more relevant results.</p>\n<pre><code class=\"language-python\"># Python + pgvector\nimport psycopg2\nimport numpy as np\n\ndef search_similar_articles(\n    query: str,\n    category: str | None = None,\n    limit: int = 5\n) -> list[dict]:\n    query_embedding = embed(query)\n\n    sql = \"\"\"\n        SELECT id, title, category, slug,\n               1 - (embedding &#x3C;=> %s::vector) AS similarity\n        FROM articles\n        WHERE 1=1\n    \"\"\"\n    params = [str(query_embedding)]\n\n    if category:\n        sql += \" AND category = %s\"\n        params.append(category)\n\n    sql += \" ORDER BY embedding &#x3C;=> %s::vector LIMIT %s\"\n    params.extend([str(query_embedding), limit])\n\n    with psycopg2.connect(DATABASE_URL) as conn:\n        with conn.cursor() as cur:\n            cur.execute(sql, params)\n            return [\n                {\"id\": r[0], \"title\": r[1], \"category\": r[2],\n                 \"slug\": r[3], \"similarity\": r[4]}\n                for r in cur.fetchall()\n            ]\n</code></pre>\n<h3>Option 2: Pinecone (Managed Vector Database)</h3>\n<p>Best for: > 10M vectors, need managed scaling, serverless pricing.</p>\n<p>When your vector count grows into the tens of millions or you need zero-ops infrastructure, Pinecone removes the burden of tuning index parameters, managing shards, and handling node failures. The code below shows the same semantic search capability as pgvector, but backed by a fully managed service.</p>\n<pre><code class=\"language-python\">from pinecone import Pinecone, ServerlessSpec\n\npc = Pinecone(api_key=\"your-api-key\")\n\n# Create index\npc.create_index(\n    name=\"articles\",\n    dimension=1536,\n    metric=\"cosine\",\n    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n)\n\nindex = pc.Index(\"articles\")\n\n# Upsert vectors with metadata\ndef index_article(article: dict):\n    embedding = embed(article[\"title\"] + \"\\n\\n\" + article[\"content\"])\n    index.upsert(vectors=[{\n        \"id\": article[\"slug\"],\n        \"values\": embedding,\n        \"metadata\": {\n            \"title\": article[\"title\"],\n            \"category\": article[\"category\"],\n            \"created_at\": article[\"created_at\"]\n        }\n    }])\n\n# Query with metadata filtering\ndef search(query: str, category: str | None = None, top_k: int = 5):\n    query_embedding = embed(query)\n\n    filter_dict = {}\n    if category:\n        filter_dict[\"category\"] = {\"$eq\": category}\n\n    results = index.query(\n        vector=query_embedding,\n        top_k=top_k,\n        filter=filter_dict if filter_dict else None,\n        include_metadata=True\n    )\n\n    return [\n        {\n            \"id\": match.id,\n            \"score\": match.score,\n            **match.metadata\n        }\n        for match in results.matches\n    ]\n</code></pre>\n<h2>Hybrid Search: Combining Keyword and Semantic</h2>\n<p>Pure semantic search misses exact keyword matches. Pure keyword search misses semantic matches. Hybrid search combines both.</p>\n<p>Consider a user searching for a specific API endpoint like <code>POST /api/v2/users/reset-password</code>. Pure semantic search might return conceptually related content about authentication but miss this exact path. Pure keyword search finds the path but misses related documentation. Reciprocal Rank Fusion (RRF) below solves this by merging the two ranked lists into a single score that rewards items that rank well in both.</p>\n<pre><code class=\"language-python\"># Reciprocal Rank Fusion (RRF) — combine two result lists\ndef hybrid_search(query: str, k: int = 5) -> list[dict]:\n    # 1. Semantic search via vector similarity\n    semantic_results = search_similar_articles(query, limit=20)\n\n    # 2. Keyword search via PostgreSQL full-text search\n    keyword_results = keyword_search(query, limit=20)\n\n    # 3. Merge using RRF: score = sum(1 / (rank + 60))\n    scores: dict[str, float] = {}\n    RRF_K = 60\n\n    for rank, result in enumerate(semantic_results):\n        scores[result[\"slug\"]] = scores.get(result[\"slug\"], 0) + 1 / (rank + RRF_K)\n\n    for rank, result in enumerate(keyword_results):\n        scores[result[\"slug\"]] = scores.get(result[\"slug\"], 0) + 1 / (rank + RRF_K)\n\n    # Sort by combined score\n    all_slugs = {r[\"slug\"]: r for r in semantic_results + keyword_results}\n    return sorted(\n        [all_slugs[slug] for slug in scores],\n        key=lambda x: scores[x[\"slug\"]],\n        reverse=True\n    )[:k]\n</code></pre>\n<p>The constant <code>RRF_K = 60</code> dampens the influence of rank position — an item ranked 1st gets score <code>1/61 ≈ 0.016</code>, while an item ranked 61st gets <code>1/121 ≈ 0.008</code>. This prevents a very high-ranked result in one list from dominating the combined score and means results that appear in both lists consistently float to the top.</p>\n<h2>Embeddings for RAG (Retrieval-Augmented Generation)</h2>\n<p>With embeddings and similarity search in place, you have all the pieces needed to build a complete RAG pipeline. The four steps below — embed the question, retrieve matching chunks, assemble context, and answer with an LLM — are the core loop that powers every document Q&#x26;A system. Notice that the LLM is explicitly instructed to only use the retrieved context and to cite sources, which is what prevents hallucination.</p>\n<pre><code class=\"language-python\"># Complete RAG pipeline: question → retrieve context → answer with LLM\nfrom anthropic import Anthropic\n\nanthropic_client = Anthropic()\n\ndef rag_answer(question: str) -> str:\n    # Step 1: Embed the question\n    question_embedding = embed(question)\n\n    # Step 2: Retrieve relevant chunks from knowledge base\n    relevant_chunks = search_similar_articles(\n        question,\n        limit=5  # Top 5 most relevant articles/chunks\n    )\n\n    # Step 3: Build context from retrieved chunks\n    context = \"\\n\\n---\\n\\n\".join([\n        f\"Title: {chunk['title']}\\n{chunk['content']}\"\n        for chunk in relevant_chunks\n    ])\n\n    # Step 4: Answer with LLM using retrieved context\n    response = anthropic_client.messages.create(\n        model=\"claude-opus-4-6\",\n        max_tokens=1024,\n        system=\"\"\"You are a technical assistant. Answer questions using ONLY\n        the provided context. If the answer isn't in the context, say so.\n        Cite the specific articles you're drawing from.\"\"\",\n        messages=[{\n            \"role\": \"user\",\n            \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n        }]\n    )\n\n    return response.content[0].text\n</code></pre>\n<h2>Chunking Strategy: Critical for RAG Quality</h2>\n<p>How you split documents before embedding them is arguably more important than which embedding model you choose. A chunk that cuts a sentence in half, or that lumps together five unrelated paragraphs, produces an embedding that represents nothing clearly — and no similarity search can recover useful signal from a bad embedding.</p>\n<pre><code class=\"language-python\"># Bad: chunk by fixed character count (breaks mid-sentence)\ndef bad_chunking(text: str, chunk_size: int = 1000) -> list[str]:\n    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n\n# Good: chunk by semantic units (paragraphs, sections)\ndef good_chunking(markdown_text: str, max_chunk_size: int = 800) -> list[str]:\n    chunks = []\n    current_chunk = []\n    current_size = 0\n\n    for paragraph in markdown_text.split(\"\\n\\n\"):\n        paragraph = paragraph.strip()\n        if not paragraph:\n            continue\n\n        if current_size + len(paragraph) > max_chunk_size and current_chunk:\n            chunks.append(\"\\n\\n\".join(current_chunk))\n            current_chunk = [paragraph]\n            current_size = len(paragraph)\n        else:\n            current_chunk.append(paragraph)\n            current_size += len(paragraph)\n\n    if current_chunk:\n        chunks.append(\"\\n\\n\".join(current_chunk))\n\n    return chunks\n\n# Best: sliding window with overlap (maintains context at boundaries)\ndef sliding_window_chunks(text: str, chunk_size: int = 800, overlap: int = 200) -> list[str]:\n    words = text.split()\n    chunks = []\n    i = 0\n    while i &#x3C; len(words):\n        chunk_words = words[i:i + chunk_size]\n        chunks.append(\" \".join(chunk_words))\n        i += chunk_size - overlap  # Overlap keeps context between chunks\n    return chunks\n</code></pre>\n<p>The <code>overlap</code> parameter in the sliding window approach is doing critical work: it ensures that a sentence split across the boundary between chunk N and chunk N+1 appears fully in at least one of them. Without overlap, every boundary is a potential context loss point that will silently degrade your retrieval quality.</p>\n<h2>Dimensionality Reduction for Visualization</h2>\n<p>Once you have a collection of embeddings, visualizing them is one of the fastest ways to build intuition about your data and validate that your embedding model is working correctly. If the visualization shows random scatter with no clustering by category, that is a signal your embeddings are not capturing the semantic distinctions you care about.</p>\n<pre><code class=\"language-python\"># Visualize your embedding space to understand clustering\nimport plotly.express as px\nfrom sklearn.manifold import TSNE\n\n# Generate embeddings for a set of articles\narticles = fetch_articles()\nembeddings = embed_batch([a[\"title\"] for a in articles])\n\n# Reduce 1536D → 2D for visualization (t-SNE)\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\ncoords_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot\ndf = pd.DataFrame({\n    \"x\": coords_2d[:, 0],\n    \"y\": coords_2d[:, 1],\n    \"title\": [a[\"title\"] for a in articles],\n    \"category\": [a[\"category\"] for a in articles]\n})\n\nfig = px.scatter(df, x=\"x\", y=\"y\", color=\"category\",\n                 hover_data=[\"title\"], title=\"Article Embedding Space\")\nfig.show()\n# You'll see: System Design articles cluster together, Java articles cluster,\n# AI/ML articles cluster — semantic proximity is visible\n</code></pre>\n<p>The insight that unlocks vector embeddings: you're not searching text anymore — you're searching a semantic space where proximity equals meaning. Once you build intuition for what's close vs far in embedding space, you'll see applications everywhere: deduplication, content recommendations, anomaly detection, and anything that requires \"find similar things.\" The math is straightforward; the power comes from applying it to the right problems.</p>\n","tableOfContents":[{"id":"what-are-embeddings","text":"What Are Embeddings?","level":2},{"id":"generating-embeddings","text":"Generating Embeddings","level":2},{"id":"vector-databases-efficient-similarity-search","text":"Vector Databases: Efficient Similarity Search","level":2},{"id":"option-1-pgvector-postgresql-extension","text":"Option 1: pgvector (PostgreSQL Extension)","level":3},{"id":"option-2-pinecone-managed-vector-database","text":"Option 2: Pinecone (Managed Vector Database)","level":3},{"id":"hybrid-search-combining-keyword-and-semantic","text":"Hybrid Search: Combining Keyword and Semantic","level":2},{"id":"embeddings-for-rag-retrieval-augmented-generation","text":"Embeddings for RAG (Retrieval-Augmented Generation)","level":2},{"id":"chunking-strategy-critical-for-rag-quality","text":"Chunking Strategy: Critical for RAG Quality","level":2},{"id":"dimensionality-reduction-for-visualization","text":"Dimensionality Reduction for Visualization","level":2}]},"relatedPosts":[{"title":"Fine-Tuning LLMs: When to Fine-Tune, When to Prompt","description":"Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.","date":"2025-03-27","category":"AI/ML","tags":["ai","llm","fine-tuning","lora","hugging face","openai","machine learning"],"featured":false,"affiliateSection":"ai-ml-books","slug":"fine-tuning-llms","readingTime":"10 min read","excerpt":"Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning — fo…"},{"title":"Building AI Agents with Tool Use: From Chatbot to Autonomous Agent","description":"Build production AI agents using Claude's tool use API. Learn the agentic loop, error handling, multi-step reasoning, human-in-the-loop patterns, and how to build reliable autonomous systems.","date":"2025-03-23","category":"AI/ML","tags":["ai","agents","claude","tool use","llm","autonomous systems","python"],"featured":false,"affiliateSection":"ai-ml-books","slug":"llm-agents-tool-use","readingTime":"10 min read","excerpt":"A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio…"},{"title":"Prompt Engineering: Advanced Techniques for Production LLMs","description":"Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications.","date":"2025-02-26","category":"AI/ML","tags":["ai","llm","prompt engineering","gpt","claude","production"],"featured":false,"affiliateSection":"ai-ml-books","slug":"prompt-engineering-production","readingTime":"11 min read","excerpt":"Most prompt engineering tutorials stop at \"be specific and provide context.\" That's necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-gra…"}]},"__N_SSG":true}