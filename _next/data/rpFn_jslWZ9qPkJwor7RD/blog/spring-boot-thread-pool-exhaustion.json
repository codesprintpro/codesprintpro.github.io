{"pageProps":{"post":{"title":"Thread Pool Exhaustion in Spring Boot: Diagnosis, Prevention, and Recovery","description":"How Tomcat thread pools work, why blocking I/O kills throughput, and the production patterns that prevent thread pool exhaustion in Spring Boot services. Includes real outage scenario and JVM tuning.","date":"2025-04-08","category":"Java","tags":["spring boot","java","tomcat","thread pool","async","performance","resilience"],"featured":false,"affiliateSection":"java-courses","slug":"spring-boot-thread-pool-exhaustion","readingTime":"9 min read","excerpt":"Thread pool exhaustion is one of the most deceptive production failures in Spring Boot services. The service is technically running — JVM process alive, health endpoint returning 200, no OutOfMemoryError in logs — but re…","contentHtml":"<p>Thread pool exhaustion is one of the most deceptive production failures in Spring Boot services. The service is technically running — JVM process alive, health endpoint returning 200, no OutOfMemoryError in logs — but requests pile up, latencies spike to 30 seconds, and then everything times out. On-call gets paged. The fix is usually a restart, which masks the root cause until it happens again.</p>\n<p>This article explains the mechanism precisely, shows you the math, and gives you the production patterns to prevent it.</p>\n<h2>How Tomcat Thread Pools Work</h2>\n<p>Spring Boot's default embedded server is Tomcat. Tomcat uses a fixed thread pool to process HTTP requests.</p>\n<pre><code>HTTP Request\n     │\n     ▼\n┌────────────────────┐\n│  Acceptor Thread   │  (accepts TCP connections, non-blocking)\n└────────┬───────────┘\n         │\n         ▼\n┌────────────────────┐\n│  Connection Queue  │  (bounded, default maxConnections=8192)\n└────────┬───────────┘\n         │\n         ▼\n┌────────────────────────────────────┐\n│  Tomcat Thread Pool                │\n│  min: 10 threads (minSpareThreads) │\n│  max: 200 threads (maxThreads)     │\n└────────────────────────────────────┘\n         │\n         ▼\n    @Controller method executes on this thread\n    (BLOCKS until method returns)\n</code></pre>\n<p>The critical constraint: <strong>each active HTTP request holds exactly one Tomcat thread</strong>. The thread is occupied for the entire duration of request processing — including all database calls, external HTTP calls, and I/O. The default maximum is 200 threads.</p>\n<h2>The Blocking I/O Impact: The Math</h2>\n<p>Consider a Spring Boot service making a database call. Assume:</p>\n<ul>\n<li>Database query latency: 100ms average</li>\n<li>Tomcat max threads: 200</li>\n<li>Incoming request rate: 1,000 requests/second</li>\n</ul>\n<p>Under steady state, how many threads are occupied?</p>\n<pre><code>Threads occupied = Request rate × Average response time\n                 = 1,000 req/s × 0.1 s\n                 = 100 threads occupied\n</code></pre>\n<p>100 threads occupied out of 200 — we're at 50% capacity with headroom. Now the database slows down to 500ms due to a slow query:</p>\n<pre><code>Threads occupied = 1,000 req/s × 0.5 s = 500 threads\n</code></pre>\n<p>500 threads required, only 200 available. The thread pool exhausts in milliseconds. New requests queue, then time out. This is the cascade.</p>\n<p>The dangerous property: <strong>a 5× increase in downstream latency causes a 5× increase in required threads</strong>. Under load, systems don't degrade linearly — they collapse.</p>\n<h2>Async vs Sync Controller Comparison</h2>\n<p>The conventional Spring MVC model is synchronous. Every request blocks a thread:</p>\n<pre><code class=\"language-java\">// SYNC - holds Tomcat thread for entire duration\n@GetMapping(\"/order/{id}\")\npublic OrderResponse getOrder(@PathVariable String id) {\n    Order order = orderRepository.findById(id).orElseThrow(); // blocks 20ms\n    List&#x3C;Item> items = itemService.getItems(order.getId());   // blocks 50ms\n    PriceResult price = pricingService.calculate(items);      // blocks 30ms\n    return OrderResponse.from(order, items, price);\n    // Total: ~100ms holding 1 Tomcat thread\n}\n</code></pre>\n<p>Spring MVC supports <code>DeferredResult</code> and <code>Callable</code> for asynchronous processing, which releases the Tomcat thread while work proceeds on another thread:</p>\n<pre><code class=\"language-java\">// ASYNC with DeferredResult - releases Tomcat thread immediately\n@GetMapping(\"/order/{id}\")\npublic DeferredResult&#x3C;OrderResponse> getOrder(@PathVariable String id) {\n    DeferredResult&#x3C;OrderResponse> result = new DeferredResult&#x3C;>(5000L);\n\n    CompletableFuture\n        .supplyAsync(() -> orderRepository.findById(id).orElseThrow(), asyncExecutor)\n        .thenApplyAsync(order -> {\n            List&#x3C;Item> items = itemService.getItems(order.getId());\n            return Pair.of(order, items);\n        }, asyncExecutor)\n        .thenAcceptAsync(pair -> {\n            PriceResult price = pricingService.calculate(pair.getSecond());\n            result.setResult(OrderResponse.from(pair.getFirst(), pair.getSecond(), price));\n        }, asyncExecutor)\n        .exceptionally(ex -> {\n            result.setErrorResult(ex);\n            return null;\n        });\n\n    return result; // Tomcat thread is FREE after this return\n}\n</code></pre>\n<p>Spring WebFlux (Reactor-based) takes this further with a reactive pipeline that uses a small number of event loop threads to handle many concurrent requests without blocking:</p>\n<pre><code class=\"language-java\">// WebFlux - non-blocking from top to bottom\n@GetMapping(\"/order/{id}\")\npublic Mono&#x3C;OrderResponse> getOrder(@PathVariable String id) {\n    return orderRepository.findById(id) // reactive repo\n        .flatMap(order -> itemService.getItemsReactive(order.getId())\n            .flatMap(items -> pricingService.calculateReactive(items)\n                .map(price -> OrderResponse.from(order, items, price))\n            )\n        );\n}\n</code></pre>\n<p>WebFlux can handle 10,000+ concurrent requests with 8 threads — but it requires your entire stack to be non-blocking. A single blocking call inside a reactive chain pins an event loop thread and destroys your throughput.</p>\n<h2>Connection Pool Exhaustion</h2>\n<p>Thread pool exhaustion and database connection pool exhaustion are different problems that often arrive together. HikariCP defaults:</p>\n<pre><code class=\"language-yaml\">spring:\n  datasource:\n    hikari:\n      maximum-pool-size: 10      # default - dangerously low\n      minimum-idle: 10\n      connection-timeout: 30000  # 30s - too high for production\n      idle-timeout: 600000\n      max-lifetime: 1800000\n</code></pre>\n<p>The right formula for HikariCP pool size:</p>\n<pre><code>pool_size = (core_count * 2) + effective_spindle_count\n\nFor a 4-core server with SSD:\npool_size = (4 * 2) + 1 = 9 connections\n</code></pre>\n<p>This seems counterintuitively small. The reason: more connections than the database can process concurrently causes context switching at the DB server level that makes everything slower. HikariCP's own research shows ~10 connections often outperforms 100.</p>\n<p>Set <code>connection-timeout</code> to match your SLA minus overhead — if your API must respond in 2 seconds and a query takes up to 1 second, your connection timeout should be under 500ms. A 30-second connection timeout means threads wait 30 seconds for a connection before failing — during which they hold Tomcat threads.</p>\n<pre><code class=\"language-java\">// Explicit HikariCP config for production\n@Bean\npublic DataSource dataSource() {\n    HikariConfig config = new HikariConfig();\n    config.setJdbcUrl(\"jdbc:postgresql://db:5432/mydb\");\n    config.setMaximumPoolSize(10);\n    config.setMinimumIdle(5);\n    config.setConnectionTimeout(2000);   // Fail fast: 2s timeout\n    config.setIdleTimeout(300000);       // 5 minutes\n    config.setMaxLifetime(900000);       // 15 minutes\n    config.setValidationTimeout(1000);   // 1s validation\n    config.addDataSourceProperty(\"cachePrepStmts\", \"true\");\n    config.addDataSourceProperty(\"prepStmtCacheSize\", \"250\");\n    return new HikariDataSource(config);\n}\n</code></pre>\n<h2>Backpressure Strategy</h2>\n<p>When your thread pool is full, Tomcat queues requests in the <code>acceptCount</code> queue (default: 100). When that fills, new TCP connections are refused. This is Tomcat's built-in backpressure — it's crude but it works.</p>\n<p>You can shape it:</p>\n<pre><code class=\"language-properties\">server.tomcat.threads.max=200\nserver.tomcat.threads.min-spare=20\nserver.tomcat.accept-count=50       # Keep queue short - fail fast\nserver.tomcat.max-connections=2000\nserver.connection-timeout=5000      # 5s connection timeout\n</code></pre>\n<p>A short <code>accept-count</code> means you fail fast when overloaded — clients see a connection refused immediately rather than waiting 30 seconds in queue. Failing fast is almost always better than hanging.</p>\n<h2>Circuit Breaker Integration</h2>\n<p>Thread pool exhaustion almost always traces to a slow downstream dependency. Wrap external calls with Resilience4j circuit breakers:</p>\n<pre><code class=\"language-java\">@Service\npublic class PaymentGatewayClient {\n\n    private final CircuitBreaker circuitBreaker;\n    private final TimeLimiter timeLimiter;\n\n    public PaymentGatewayClient(CircuitBreakerRegistry registry,\n                                 TimeLimiterRegistry timeLimiterRegistry) {\n        CircuitBreakerConfig config = CircuitBreakerConfig.custom()\n            .slidingWindowSize(20)\n            .failureRateThreshold(50)        // Open after 50% failure rate\n            .waitDurationInOpenState(Duration.ofSeconds(10))\n            .permittedNumberOfCallsInHalfOpenState(5)\n            .slowCallDurationThreshold(Duration.ofMillis(500)) // 500ms = slow\n            .slowCallRateThreshold(80)        // Open if 80% calls are slow\n            .build();\n\n        this.circuitBreaker = registry.circuitBreaker(\"payment-gateway\", config);\n\n        TimeLimiterConfig tlConfig = TimeLimiterConfig.custom()\n            .timeoutDuration(Duration.ofMillis(800))\n            .cancelRunningFuture(true)\n            .build();\n        this.timeLimiter = timeLimiterRegistry.timeLimiter(\"payment-gateway\", tlConfig);\n    }\n\n    public PaymentResult charge(PaymentRequest req) {\n        Supplier&#x3C;CompletableFuture&#x3C;PaymentResult>> futureSupplier =\n            () -> CompletableFuture.supplyAsync(() -> callExternalGateway(req), asyncExecutor);\n\n        Callable&#x3C;PaymentResult> restrictedCall =\n            TimeLimiter.decorateFutureSupplier(timeLimiter, futureSupplier);\n\n        Callable&#x3C;PaymentResult> circuitBreakerCall =\n            CircuitBreaker.decorateCallable(circuitBreaker, restrictedCall);\n\n        return Try.ofCallable(circuitBreakerCall)\n            .recover(CallNotPermittedException.class, e -> PaymentResult.circuitOpen())\n            .recover(TimeoutException.class, e -> PaymentResult.timeout())\n            .get();\n    }\n}\n</code></pre>\n<p>The <code>slowCallDurationThreshold</code> is critical. A circuit breaker that only trips on errors won't protect you from a slow dependency that eventually exhausts your thread pool while technically succeeding.</p>\n<h2>Real Production Outage Scenario</h2>\n<p><strong>System:</strong> Order processing service, Spring Boot 2.7, Tomcat 200 threads, HikariCP 10 connections, PostgreSQL RDS.</p>\n<p><strong>Timeline:</strong></p>\n<ul>\n<li>14:00: RDS replica promotion during maintenance window, brief failover</li>\n<li>14:03: During 30-second DB reconnect window, all 10 HikariCP connections time out waiting</li>\n<li>14:03: Requests pile up waiting for DB connections (30s <code>connectionTimeout</code>)</li>\n<li>14:04: All 200 Tomcat threads occupied, waiting for DB connections</li>\n<li>14:04: Tomcat accept queue fills (50 requests), new connections refused</li>\n<li>14:04: API gateway marks service unhealthy, starts shedding load</li>\n<li>14:05: DB reconnects. HikariCP establishes connections. But...</li>\n<li>14:05–14:08: Backlog of 200+ in-flight requests completes, some after 30s timeout</li>\n<li>14:08: Service recovers on its own — but 5 minutes of downtime and thousands of errors</li>\n</ul>\n<p><strong>Root cause:</strong> A 30-second <code>connection-timeout</code> created a 30-second thread holding duration during DB unavailability. The fix was reducing <code>connection-timeout</code> to 2000ms and adding exponential backoff retry logic for transient DB failures.</p>\n<h2>JVM Tuning for Thread-Heavy Applications</h2>\n<p>Each thread reserves stack memory. Default stack size is 512KB on most JVMs:</p>\n<pre><code>200 threads × 512KB = 100MB of stack memory reserved\n</code></pre>\n<p>For applications with many threads, reduce stack size if your call stacks are shallow:</p>\n<pre><code class=\"language-bash\">-Xss256k   # Reduce thread stack from 512K to 256K\n</code></pre>\n<p>G1GC settings for latency-sensitive services:</p>\n<pre><code class=\"language-bash\">-XX:+UseG1GC\n-XX:MaxGCPauseMillis=100        # Target 100ms max GC pause\n-XX:G1HeapRegionSize=16m        # Larger regions for big heaps\n-XX:InitiatingHeapOccupancyPercent=35\n-XX:+ParallelRefProcEnabled\n-XX:+DisableExplicitGC          # Prevent System.gc() calls\n</code></pre>\n<h2>Monitoring with Prometheus and Grafana</h2>\n<p>Key metrics to track thread pool health:</p>\n<pre><code class=\"language-yaml\"># application.properties\nmanagement.endpoints.web.exposure.include=prometheus,health,metrics\nmanagement.metrics.enable.tomcat=true\n</code></pre>\n<pre><code class=\"language-java\">// Custom metric: track thread pool utilization\n@Component\npublic class ThreadPoolMetrics {\n\n    private final ThreadPoolTaskExecutor asyncExecutor;\n    private final MeterRegistry meterRegistry;\n\n    @PostConstruct\n    public void registerMetrics() {\n        Gauge.builder(\"app.thread_pool.active_threads\", asyncExecutor,\n                      executor -> executor.getActiveCount())\n            .description(\"Active threads in async executor\")\n            .register(meterRegistry);\n\n        Gauge.builder(\"app.thread_pool.queue_size\", asyncExecutor,\n                      executor -> executor.getThreadPoolExecutor().getQueue().size())\n            .description(\"Async executor queue depth\")\n            .register(meterRegistry);\n    }\n}\n</code></pre>\n<p>Grafana alert rules:</p>\n<pre><code># Alert: Thread pool near exhaustion\ntomcat_threads_busy_threads / tomcat_threads_config_max_threads > 0.85\n\n# Alert: Connection pool exhausted\nhikaricp_connections_pending > 0 for 30s\n\n# Alert: High response latency (symptom of thread starvation)\nhttp_server_requests_seconds_p99 > 2.0\n</code></pre>\n<h2>Thread Dump Analysis</h2>\n<p>When a service is hanging, take a thread dump immediately:</p>\n<pre><code class=\"language-bash\">jstack &#x3C;pid> > thread-dump.txt\n# Or via JMX:\nkill -3 &#x3C;pid>   # Sends SIGQUIT, dumps to stdout\n</code></pre>\n<p>Patterns that indicate thread pool exhaustion:</p>\n<pre><code># Thread blocked waiting for DB connection:\n\"http-nio-8080-exec-47\" WAITING on com.zaxxer.hikari.util.ConcurrentBag$1\n    at sun.misc.Unsafe.park(Native Method)\n    at HikariPool.getConnection(HikariPool.java:213)\n    at OrderController.getOrder(OrderController.java:45)\n\n# Count blocked threads:\ngrep -c \"WAITING on com.zaxxer.hikari\" thread-dump.txt\n</code></pre>\n<p>If you see 50+ threads waiting on HikariCP, your connection pool is the bottleneck. If threads are waiting on external HTTP calls, your downstream is slow.</p>\n<h2>Tomcat Configuration for Production</h2>\n<pre><code class=\"language-java\">@Bean\npublic TomcatServletWebServerFactory tomcatFactory() {\n    TomcatServletWebServerFactory factory = new TomcatServletWebServerFactory();\n    factory.addConnectorCustomizers(connector -> {\n        ProtocolHandler handler = connector.getProtocolHandler();\n        if (handler instanceof AbstractProtocol&#x3C;?> protocol) {\n            protocol.setMaxThreads(200);\n            protocol.setMinSpareThreads(20);\n            protocol.setAcceptCount(50);\n            protocol.setConnectionTimeout(5000);\n            protocol.setKeepAliveTimeout(20000);\n            protocol.setMaxKeepAliveRequests(200);\n        }\n    });\n    return factory;\n}\n</code></pre>\n<h2>Lessons Learned from Production</h2>\n<p><strong>1. Short timeouts everywhere.</strong> Every blocking operation — DB query, HTTP call, lock acquisition — must have a timeout shorter than your SLA. A missing timeout is a thread leak waiting to happen.</p>\n<p><strong>2. Size thread pools to match downstream capacity.</strong> If your DB can handle 10 concurrent queries, having 200 Tomcat threads is counterproductive — they'll all race for 10 connections. Align pool sizes across the call chain.</p>\n<p><strong>3. Instrument thread pool utilization, not just request latency.</strong> P99 latency spikes are a lagging indicator. Thread pool utilization at 80% is an early warning.</p>\n<p><strong>4. Fail fast under load.</strong> A 50-request <code>accept-count</code> and 2-second <code>connection-timeout</code> mean failures are visible in 2 seconds, not 30. Operators can respond; monitoring can alert. Silent accumulation is far worse.</p>\n<p><strong>5. One slow downstream can take down your service.</strong> Circuit breakers are not optional in microservice architectures. Every external call that can be slow must be wrapped.</p>\n<p>Thread pool exhaustion is entirely preventable once you understand the mechanics. The failure mode is almost always: slow downstream + missing timeout + no circuit breaker = cascading thread starvation. Fix any one of those three and the cascade stops.</p>\n","tableOfContents":[{"id":"how-tomcat-thread-pools-work","text":"How Tomcat Thread Pools Work","level":2},{"id":"the-blocking-io-impact-the-math","text":"The Blocking I/O Impact: The Math","level":2},{"id":"async-vs-sync-controller-comparison","text":"Async vs Sync Controller Comparison","level":2},{"id":"connection-pool-exhaustion","text":"Connection Pool Exhaustion","level":2},{"id":"backpressure-strategy","text":"Backpressure Strategy","level":2},{"id":"circuit-breaker-integration","text":"Circuit Breaker Integration","level":2},{"id":"real-production-outage-scenario","text":"Real Production Outage Scenario","level":2},{"id":"jvm-tuning-for-thread-heavy-applications","text":"JVM Tuning for Thread-Heavy Applications","level":2},{"id":"monitoring-with-prometheus-and-grafana","text":"Monitoring with Prometheus and Grafana","level":2},{"id":"thread-dump-analysis","text":"Thread Dump Analysis","level":2},{"id":"tomcat-configuration-for-production","text":"Tomcat Configuration for Production","level":2},{"id":"lessons-learned-from-production","text":"Lessons Learned from Production","level":2}]},"relatedPosts":[{"title":"Java Concurrency Patterns: CompletableFuture, Structured Concurrency, and Thread-Safe Design","description":"Production Java concurrency: CompletableFuture pipelines, handling exceptions in async chains, Java 21 structured concurrency, thread-safe collection patterns, and the concurrency bugs that cause data corruption.","date":"2025-07-08","category":"Java","tags":["java","concurrency","completablefuture","virtual threads","java21","thread-safe","async"],"featured":false,"affiliateSection":"java-courses","slug":"java-concurrency-patterns","readingTime":"7 min read","excerpt":"Java concurrency has three eras: raw  and  (Java 1-4), the  framework (Java 5+), and the virtual thread/structured concurrency era (Java 21+). Each era's patterns still exist in production codebases. Understanding all th…"},{"title":"Java Memory Management Deep Dive: Heap, GC, and Production Tuning","description":"How the JVM allocates memory, how G1GC and ZGC work under the hood, heap analysis with JVM tools, and the GC tuning decisions that eliminate latency spikes in production Java services.","date":"2025-06-13","category":"Java","tags":["java","jvm","garbage collection","g1gc","zgc","heap","memory management","performance"],"featured":false,"affiliateSection":"java-courses","slug":"java-memory-management-deep-dive","readingTime":"7 min read","excerpt":"Java's garbage collector is the single biggest source of unexplained latency spikes in production services. A GC pause of 2 seconds is invisible in most logs but visible to every user who happened to make a request durin…"},{"title":"Spring Security OAuth2 and JWT: Production Implementation Guide","description":"Complete Spring Security OAuth2 implementation: JWT token validation, Resource Server configuration, method-level security, custom UserDetailsService, refresh token rotation, and the security pitfalls that lead to authentication bypasses.","date":"2025-06-03","category":"Java","tags":["spring security","oauth2","jwt","spring boot","authentication","authorization","java","security"],"featured":false,"affiliateSection":"java-courses","slug":"spring-security-oauth2-jwt","readingTime":"7 min read","excerpt":"Spring Security is one of the most powerful and most misunderstood frameworks in the Java ecosystem. Its flexibility is its strength — and its complexity. Misconfigured security is worse than no security, because it give…"}]},"__N_SSG":true}