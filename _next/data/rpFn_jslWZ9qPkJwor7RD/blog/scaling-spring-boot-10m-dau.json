{"pageProps":{"post":{"title":"Scaling Spring Boot Applications to Handle 10 Million Daily Active Users","description":"A practical performance engineering guide: load balancing, horizontal scaling, database tuning, JVM optimization, autoscaling, and the observability stack to find and fix bottlenecks before they page you.","date":"2025-05-28","category":"Java","tags":["spring boot","java","scaling","performance","jvm","kubernetes","prometheus","grafana"],"featured":false,"affiliateSection":"java-courses","slug":"scaling-spring-boot-10m-dau","readingTime":"10 min read","excerpt":"10 million daily active users is not an exotic scale — it's where a successful mid-stage startup or a growing enterprise service lands. At this scale, the things that worked for 100,000 users start breaking in interestin…","contentHtml":"<p>10 million daily active users is not an exotic scale — it's where a successful mid-stage startup or a growing enterprise service lands. At this scale, the things that worked for 100,000 users start breaking in interesting ways. Your single database instance is gasping. Your JVM is GC-pausing under heap pressure. Your API response times have developed a long tail you can't explain. Your Kubernetes pods are scaling but latency isn't improving.</p>\n<p>This article is a practical guide to getting a Spring Boot application to that scale — and keeping it there.</p>\n<h2>Sizing the Problem</h2>\n<p>First, understand what 10M DAU actually means in request terms:</p>\n<pre><code>10,000,000 DAU\nTraffic distribution: 20% of users active in peak 4-hour window\nPeak concurrent users: 10M × 0.20 / 4h = 500K users at peak hour\n\nAverage requests per active session: 50 (browsing, search, actions)\nPeak RPS: 500,000 users × 50 requests / 3,600s = ~7,000 RPS sustained peak\n\nP95 session: 200 requests in 30 minutes = 6.7 req/s per user\nPeak concurrent requests: 500,000 × 0.1s avg response time = 50,000 inflight\n\nRead/write ratio: typically 80:20 for content apps, 95:5 for social feeds\n</code></pre>\n<p>This gives you the numbers to size your infrastructure, not guesses.</p>\n<h2>Load Balancing Strategy</h2>\n<pre><code>Traffic distribution architecture:\n\n                         DNS (Route 53)\n                              │\n                    ┌─────────┴──────────┐\n                    │   Global Load Bal   │  (AWS Global Accelerator)\n                    └────────────┬────────┘\n                    ┌────────────┴────────────┐\n               us-east-1                  eu-west-1\n           ┌────────┴────────┐        ┌────────┴────────┐\n           │   ALB (Layer 7) │        │   ALB (Layer 7) │\n           └────────┬────────┘        └────────┬────────┘\n               ┌────┼────┐\n      ┌─────┐ ┌┴───┐ ┌──┴──┐\n      │ Pod │ │ Pod│ │ Pod │  (Spring Boot instances)\n      └─────┘ └────┘ └─────┘\n</code></pre>\n<p>ALB configuration for Spring Boot:</p>\n<ul>\n<li><code>idle_timeout</code>: 60s (match your Spring Boot <code>server.connection-timeout</code>)</li>\n<li><code>slow_start</code>: 30s (new instances get gradually increasing traffic during warmup — avoids cold-start spikes)</li>\n<li>Health check: <code>/actuator/health</code> with 2 healthy checks to mark healthy, 3 unhealthy to mark unhealthy</li>\n<li>Stickiness: off (stateless Spring Boot shouldn't need session affinity)</li>\n</ul>\n<h2>Horizontal Scaling</h2>\n<p>Design your Spring Boot service to be completely stateless before scaling horizontally. Common statefulness that breaks horizontal scaling:</p>\n<p><strong>In-memory caches:</strong> <code>@Cacheable</code> with <code>ConcurrentHashMap</code> is per-instance. 10 instances = 10 different caches, each potentially stale. Replace with distributed Redis cache.</p>\n<p><strong>File uploads to local disk:</strong> Instance receives file, processes, stores. On a different instance, the file doesn't exist. Replace with S3 + presigned URLs or shared EFS.</p>\n<p><strong>In-process event queues:</strong> A background job queue built on <code>ThreadPoolTaskExecutor</code> is lost on restart. Replace with persistent queue (SQS, Redis Queue, or Kafka).</p>\n<pre><code class=\"language-java\">// Stateful - breaks horizontal scaling:\n@Cacheable(value = \"products\")  // Default: in-memory\npublic Product getProduct(String id) { ... }\n\n// Stateless - scales horizontally:\n@Cacheable(value = \"products\", cacheManager = \"redisCacheManager\")\npublic Product getProduct(String id) { ... }\n</code></pre>\n<p><strong>Kubernetes HPA configuration:</strong></p>\n<pre><code class=\"language-yaml\">apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: api-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-service\n  minReplicas: 10\n  maxReplicas: 100\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 65      # Scale at 65% CPU, not 80%\n  - type: Pods\n    pods:\n      metric:\n        name: http_requests_in_progress  # Custom metric from Prometheus\n      target:\n        type: AverageValue\n        averageValue: \"200\"         # Scale when avg 200 inflight requests/pod\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 30   # Aggressive scale-up\n      policies:\n      - type: Pods\n        value: 5                        # Add max 5 pods per 30s\n        periodSeconds: 30\n    scaleDown:\n      stabilizationWindowSeconds: 300  # Conservative scale-down (5 min)\n</code></pre>\n<p>Scale up fast (avoid underprovisioning), scale down slow (avoid thrashing). Never scale below your minimum at peak hours — use a scheduled <code>CronJob</code> to increase <code>minReplicas</code> before known traffic events.</p>\n<h2>Database Tuning</h2>\n<p>At 7,000 RPS with 80:20 read/write, your read traffic is 5,600 RPS. A single PostgreSQL instance handles about 10,000 simple queries/second, but at 5,600 RPS with complex queries, JOINs, and index scans, you'll be at capacity.</p>\n<p><strong>Read replicas:</strong> Route all read queries (SELECT without FOR UPDATE) to replicas. Spring Boot + Hikari + AbstractRoutingDataSource:</p>\n<pre><code class=\"language-java\">@Configuration\npublic class DataSourceConfig {\n\n    @Bean\n    @Primary\n    public DataSource routingDataSource(\n            @Qualifier(\"primaryDataSource\") DataSource primary,\n            @Qualifier(\"replicaDataSource\") DataSource replica) {\n\n        ReadWriteRoutingDataSource routing = new ReadWriteRoutingDataSource();\n        routing.setDefaultTargetDataSource(primary);\n        routing.setTargetDataSources(Map.of(\n            DataSourceType.PRIMARY, primary,\n            DataSourceType.REPLICA, replica\n        ));\n        return routing;\n    }\n}\n\n// AOP: route @Transactional(readOnly=true) to replica\n@Aspect\n@Component\npublic class DataSourceRoutingAspect {\n    @Before(\"@annotation(transactional)\")\n    public void setDataSource(JoinPoint point, Transactional transactional) {\n        DataSourceContextHolder.set(\n            transactional.readOnly() ? DataSourceType.REPLICA : DataSourceType.PRIMARY\n        );\n    }\n}\n</code></pre>\n<p><strong>Connection pool sizing per instance:</strong></p>\n<pre><code>Per Spring Boot pod:\n  Primary pool: 5 connections (writes only)\n  Replica pool: 15 connections (reads)\n  Total: 20 connections per pod\n\nAt 50 pods:\n  Primary: 50 × 5 = 250 connections to primary DB\n  Replica: 50 × 15 = 750 connections per replica\n\nPostgreSQL max_connections = 500 for primary\n→ Need PgBouncer (connection pooler) in front of primary\nPgBouncer pools 250 app connections into 50 actual DB connections\n</code></pre>\n<p><strong>Index strategy for high-traffic queries:</strong></p>\n<pre><code class=\"language-sql\">-- Slow query from APM: product search by category + price range\nEXPLAIN ANALYZE\nSELECT p.*, COUNT(r.id) as review_count\nFROM products p\nLEFT JOIN reviews r ON p.id = r.product_id\nWHERE p.category_id = 5\n  AND p.price BETWEEN 10.00 AND 50.00\n  AND p.status = 'active'\nORDER BY p.created_at DESC\nLIMIT 20;\n\n-- Add composite index covering the WHERE and ORDER BY:\nCREATE INDEX CONCURRENTLY idx_products_category_price_created\nON products (category_id, status, price, created_at DESC)\nWHERE status = 'active';  -- Partial index: only active products\n\n-- Separate index for JOIN:\nCREATE INDEX CONCURRENTLY idx_reviews_product_id ON reviews (product_id);\n</code></pre>\n<h2>Connection Pool Sizing</h2>\n<p>Wrong HikariCP sizing is the most common Spring Boot performance mistake:</p>\n<pre><code class=\"language-yaml\">spring:\n  datasource:\n    hikari:\n      # For a write-heavy service (20% writes):\n      maximum-pool-size: 20\n      minimum-idle: 10\n      connection-timeout: 2000     # Fail fast: 2 seconds\n      idle-timeout: 300000         # 5 minutes\n      max-lifetime: 900000         # 15 minutes\n      keepalive-time: 60000        # Test idle connections every 60s\n      validation-timeout: 1000     # 1s connection validation\n      # Performance properties:\n      data-source-properties:\n        cachePrepStmts: true\n        prepStmtCacheSize: 250\n        prepStmtCacheSqlLimit: 2048\n        useServerPrepStmts: true    # Server-side prepared statements\n</code></pre>\n<p>Monitor <code>hikaricp_connections_pending</code> in Prometheus. If this metric is ever > 0, your pool is undersized.</p>\n<h2>Caching Layer Design</h2>\n<p>Three tiers:</p>\n<p><strong>Tier 1: Application-level cache (Caffeine)</strong> — for extremely hot, tiny data (config, feature flags):</p>\n<pre><code class=\"language-java\">@Bean\npublic CacheManager localCacheManager() {\n    CaffeineCacheManager manager = new CaffeineCacheManager();\n    manager.setCaffeine(Caffeine.newBuilder()\n        .maximumSize(1_000)\n        .expireAfterWrite(Duration.ofSeconds(30))\n        .recordStats()\n    );\n    return manager;\n}\n</code></pre>\n<p><strong>Tier 2: Distributed Redis cache</strong> — for hot data shared across pods:</p>\n<pre><code class=\"language-java\">@Cacheable(value = \"products\", key = \"#id\", cacheManager = \"redisCacheManager\")\npublic ProductDTO getProduct(String id) {\n    return productRepository.findById(id).map(ProductDTO::from).orElseThrow();\n}\n\n@CacheEvict(value = \"products\", key = \"#product.id\")\npublic void updateProduct(Product product) {\n    productRepository.save(product);\n}\n</code></pre>\n<p><strong>Tier 3: CDN (CloudFront)</strong> — for public, user-agnostic responses (category pages, search results, product pages):</p>\n<pre><code class=\"language-java\">@GetMapping(\"/products/{id}\")\npublic ResponseEntity&#x3C;ProductDTO> getProduct(@PathVariable String id) {\n    ProductDTO product = productService.getProduct(id);\n    return ResponseEntity.ok()\n        .cacheControl(CacheControl.maxAge(Duration.ofMinutes(5))\n            .cachePublic())              // Cache in CDN for 5 minutes\n        .eTag(product.getVersion())      // ETag for conditional requests\n        .body(product);\n}\n</code></pre>\n<h2>Async Processing</h2>\n<p>Move non-critical work off the request path:</p>\n<pre><code class=\"language-java\">// BEFORE: Synchronous - holds request thread for 800ms\n@PostMapping(\"/orders\")\npublic OrderResponse createOrder(@RequestBody OrderRequest request) {\n    Order order = orderService.create(request);\n    emailService.sendConfirmation(order);    // 200ms — blocks thread\n    analyticsService.track(order);           // 300ms — blocks thread\n    recommendationEngine.update(order);      // 300ms — blocks thread\n    return OrderResponse.from(order);\n}\n\n// AFTER: Async - request completes in 50ms\n@PostMapping(\"/orders\")\npublic OrderResponse createOrder(@RequestBody OrderRequest request) {\n    Order order = orderService.create(request);       // 50ms\n    eventPublisher.publishEvent(new OrderCreated(order)); // non-blocking\n    return OrderResponse.from(order);\n}\n\n@Async(\"asyncTaskExecutor\")\n@EventListener\npublic void handleOrderCreated(OrderCreated event) {\n    emailService.sendConfirmation(event.order());\n    analyticsService.track(event.order());\n    recommendationEngine.update(event.order());\n}\n\n@Bean(\"asyncTaskExecutor\")\npublic TaskExecutor asyncTaskExecutor() {\n    ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n    executor.setCorePoolSize(20);\n    executor.setMaxPoolSize(50);\n    executor.setQueueCapacity(500);\n    executor.setThreadNamePrefix(\"async-\");\n    executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());\n    executor.initialize();\n    return executor;\n}\n</code></pre>\n<h2>JVM Tuning</h2>\n<p>For a 10M DAU service running on 4-core, 16GB instances:</p>\n<pre><code class=\"language-bash\"># Heap sizing: start with 70% of available RAM\n-Xms8g -Xmx8g          # Fixed heap (avoids resizing pauses)\n\n# G1GC for low-latency (default in JDK 11+):\n-XX:+UseG1GC\n-XX:MaxGCPauseMillis=100      # Target 100ms max GC pause\n-XX:G1HeapRegionSize=16m      # 16MB regions for large heap\n-XX:InitiatingHeapOccupancyPercent=40  # Start concurrent GC earlier\n-XX:+ParallelRefProcEnabled   # Parallel reference processing\n-XX:ConcGCThreads=4           # Concurrent GC threads (= CPU cores)\n-XX:G1RSetUpdatingPauseTimePercent=10\n\n# ZGC (JDK 15+) for sub-millisecond GC pauses:\n-XX:+UseZGC\n-XX:SoftMaxHeapSize=7g        # ZGC leaves headroom above this\n-Xmx8g\n\n# Metaspace (class metadata):\n-XX:MetaspaceSize=256m\n-XX:MaxMetaspaceSize=512m\n\n# JIT compilation:\n-XX:+TieredCompilation\n-XX:ReservedCodeCacheSize=256m\n\n# Thread stack size (reduce for many threads):\n-Xss256k               # 256KB vs default 512KB\n\n# Diagnostics (non-prod):\n-XX:+PrintGCDetails\n-XX:+PrintGCDateStamps\n-Xloggc:/var/log/app/gc.log\n-XX:+UseGCLogFileRotation\n-XX:NumberOfGCLogFiles=5\n-XX:GCLogFileSize=20m\n</code></pre>\n<h2>Autoscaling Strategy</h2>\n<p>Custom metrics-based autoscaling performs better than CPU-only:</p>\n<pre><code class=\"language-yaml\"># KEDA ScaledObject using Kafka consumer lag:\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: order-processor\nspec:\n  scaleTargetRef:\n    name: order-processor-deployment\n  minReplicaCount: 5\n  maxReplicaCount: 100\n  triggers:\n  - type: kafka\n    metadata:\n      bootstrapServers: kafka:9092\n      consumerGroup: order-processor\n      topic: orders\n      lagThreshold: \"1000\"        # Scale up when lag > 1000 per partition\n      activationLagThreshold: \"10\"\n</code></pre>\n<h2>Bottleneck Identification</h2>\n<p>When P99 latency is rising, find the bottleneck systematically:</p>\n<pre><code class=\"language-bash\"># 1. CPU vs I/O bound?\n# CPU bound: top shows high CPU%, thread dump shows threads RUNNING\n# I/O bound: top shows low CPU%, thread dump shows threads WAITING\n\n# 2. Which endpoint is slow?\n# Spring Boot Actuator + Prometheus:\nhttp_server_requests_seconds_p99{uri=\"/api/products/{id}\"} > 2.0\n\n# 3. Database slow queries?\n# PostgreSQL slow query log:\nSET log_min_duration_statement = 1000;  # Log queries > 1 second\n\n# 4. Thread pool exhaustion?\n# Actuator thread dump:\ncurl http://localhost:8080/actuator/threaddump | python3 -c \"\nimport json, sys\ndata = json.load(sys.stdin)\nstates = {}\nfor t in data['threads']:\n    state = t['threadState']\n    states[state] = states.get(state, 0) + 1\nprint(states)\"\n\n# 5. GC pressure?\n# GC pauses causing latency spikes:\njstat -gcutil &#x3C;pid> 1000 10\n# If GC_TIME > 10% = GC is a bottleneck\n</code></pre>\n<h2>Observability Stack</h2>\n<pre><code class=\"language-yaml\"># docker-compose.yml for local observability:\nservices:\n  prometheus:\n    image: prom/prometheus\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n\n  grafana:\n    image: grafana/grafana\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n\n  elasticsearch:\n    image: elasticsearch:8.12.0\n\n  logstash:\n    image: logstash:8.12.0\n\n  kibana:\n    image: kibana:8.12.0\n\n  jaeger:\n    image: jaegertracing/all-in-one\n</code></pre>\n<p>Spring Boot Micrometer configuration:</p>\n<pre><code class=\"language-java\">@Bean\npublic MeterRegistryCustomizer&#x3C;MeterRegistry> commonTags(\n        @Value(\"${spring.application.name}\") String appName) {\n    return registry -> registry.config()\n        .commonTags(\"app\", appName, \"region\", System.getenv(\"AWS_REGION\"));\n}\n</code></pre>\n<p>Key dashboards:</p>\n<ol>\n<li><strong>RED dashboard:</strong> Rate, Errors, Duration per endpoint</li>\n<li><strong>USE dashboard:</strong> Utilization, Saturation, Errors per resource (CPU, memory, DB connections)</li>\n<li><strong>Business metrics:</strong> Orders/second, payment success rate, cart conversion</li>\n</ol>\n<pre><code>Critical alerts for production:\n- http_server_requests_seconds_p99 > 2s for 5 minutes → page\n- hikaricp_connections_pending > 0 for 2 minutes → page\n- jvm_gc_pause_seconds_max > 2s → page\n- process_cpu_usage > 0.9 for 5 minutes → scale-up trigger\n- kafka_consumer_lag > 10000 → page\n</code></pre>\n<h2>Production Debugging Strategy</h2>\n<p>When you get paged at 3 AM:</p>\n<pre><code class=\"language-bash\"># 1. Is it a recent deploy? Check:\nkubectl rollout history deployment/api-service\n# If yes: kubectl rollout undo deployment/api-service\n\n# 2. Is it a traffic spike?\n# Check CloudWatch/Prometheus: requests/second vs baseline\n\n# 3. Heap/GC issue?\nkubectl exec -it &#x3C;pod> -- jcmd 1 GC.heap_info\nkubectl exec -it &#x3C;pod> -- jcmd 1 VM.flags | grep Xmx\n\n# 4. Thread dump:\nkubectl exec -it &#x3C;pod> -- jstack 1 > /tmp/threaddump.txt\n# Look for BLOCKED threads, threads waiting on locks/DB\n\n# 5. Heap dump (if OOM suspected):\nkubectl exec -it &#x3C;pod> -- jcmd 1 GC.heap_dump /tmp/heap.hprof\nkubectl cp &#x3C;pod>:/tmp/heap.hprof ./heap.hprof\n# Analyze with Eclipse Memory Analyzer (MAT)\n\n# 6. Check downstream dependencies:\ncurl http://pod:8080/actuator/health  # Spring Boot health indicators\n# Checks DB, Redis, Kafka connectivity\n</code></pre>\n<p>The path from 100K to 10M DAU is not a single optimization — it's a sequence of bottleneck-find-fix cycles. Each scale milestone reveals a new bottleneck: first the database, then the cache, then the JVM, then the network, then the application code itself. The teams that navigate this successfully are the ones who instrument everything, build systematic diagnosis tools, and respond to metrics before they become incidents.</p>\n","tableOfContents":[{"id":"sizing-the-problem","text":"Sizing the Problem","level":2},{"id":"load-balancing-strategy","text":"Load Balancing Strategy","level":2},{"id":"horizontal-scaling","text":"Horizontal Scaling","level":2},{"id":"database-tuning","text":"Database Tuning","level":2},{"id":"connection-pool-sizing","text":"Connection Pool Sizing","level":2},{"id":"caching-layer-design","text":"Caching Layer Design","level":2},{"id":"async-processing","text":"Async Processing","level":2},{"id":"jvm-tuning","text":"JVM Tuning","level":2},{"id":"autoscaling-strategy","text":"Autoscaling Strategy","level":2},{"id":"bottleneck-identification","text":"Bottleneck Identification","level":2},{"id":"observability-stack","text":"Observability Stack","level":2},{"id":"production-debugging-strategy","text":"Production Debugging Strategy","level":2}]},"relatedPosts":[{"title":"Java Concurrency Patterns: CompletableFuture, Structured Concurrency, and Thread-Safe Design","description":"Production Java concurrency: CompletableFuture pipelines, handling exceptions in async chains, Java 21 structured concurrency, thread-safe collection patterns, and the concurrency bugs that cause data corruption.","date":"2025-07-08","category":"Java","tags":["java","concurrency","completablefuture","virtual threads","java21","thread-safe","async"],"featured":false,"affiliateSection":"java-courses","slug":"java-concurrency-patterns","readingTime":"7 min read","excerpt":"Java concurrency has three eras: raw  and  (Java 1-4), the  framework (Java 5+), and the virtual thread/structured concurrency era (Java 21+). Each era's patterns still exist in production codebases. Understanding all th…"},{"title":"Java Memory Management Deep Dive: Heap, GC, and Production Tuning","description":"How the JVM allocates memory, how G1GC and ZGC work under the hood, heap analysis with JVM tools, and the GC tuning decisions that eliminate latency spikes in production Java services.","date":"2025-06-13","category":"Java","tags":["java","jvm","garbage collection","g1gc","zgc","heap","memory management","performance"],"featured":false,"affiliateSection":"java-courses","slug":"java-memory-management-deep-dive","readingTime":"7 min read","excerpt":"Java's garbage collector is the single biggest source of unexplained latency spikes in production services. A GC pause of 2 seconds is invisible in most logs but visible to every user who happened to make a request durin…"},{"title":"Spring Security OAuth2 and JWT: Production Implementation Guide","description":"Complete Spring Security OAuth2 implementation: JWT token validation, Resource Server configuration, method-level security, custom UserDetailsService, refresh token rotation, and the security pitfalls that lead to authentication bypasses.","date":"2025-06-03","category":"Java","tags":["spring security","oauth2","jwt","spring boot","authentication","authorization","java","security"],"featured":false,"affiliateSection":"java-courses","slug":"spring-security-oauth2-jwt","readingTime":"7 min read","excerpt":"Spring Security is one of the most powerful and most misunderstood frameworks in the Java ecosystem. Its flexibility is its strength — and its complexity. Misconfigured security is worse than no security, because it give…"}]},"__N_SSG":true}