{"pageProps":{"post":{"title":"Sync vs Async in Java: CompletableFuture, Reactive Streams, and Virtual Threads","description":"Master Java's concurrency toolkit — from blocking calls and thread pools to CompletableFuture chains, Project Reactor, and the new virtual thread model. Know when each is the right tool.","date":"2025-01-26","category":"Java","tags":["java","async","concurrency","reactive","spring boot","webflux"],"featured":false,"affiliateSection":"java-courses","slug":"sync-async-patterns-java","readingTime":"11 min read","excerpt":"Java has accumulated three distinct paradigms for handling concurrency over the past 15 years: traditional blocking threads, CompletableFuture-based async composition, and reactive programming with Project Reactor/RxJava…","contentHtml":"<p>Java has accumulated three distinct paradigms for handling concurrency over the past 15 years: traditional blocking threads, CompletableFuture-based async composition, and reactive programming with Project Reactor/RxJava. Now Java 21 adds virtual threads. Each solves a real problem — but choosing the wrong one for your use case introduces unnecessary complexity or leaves performance on the table.</p>\n<p>This article gives you the mental model to choose correctly.</p>\n<h2>The Core Problem: Threads Are Expensive</h2>\n<p>Before comparing the options, it helps to understand precisely why concurrency is a challenge in the first place. A Java thread waiting on IO (database query, HTTP call, disk read) <strong>holds an OS thread</strong> that could be serving other requests. With a thread pool of 200 (Tomcat default), you can handle 200 concurrent IO-bound requests before requests start queuing. The following breakdown shows how quickly that limit becomes a bottleneck as concurrency grows:</p>\n<pre><code>200 concurrent users, each waits 100ms for DB:\n  Sequential (1 thread): 200 × 100ms = 20 seconds\n  200 threads (Tomcat default): all 200 run concurrently → 100ms total\n  1,000 concurrent users: 800 queue behind 200 threads → queueing latency\n\nSolution options:\n  1. Bigger thread pool (200 → 2000): High memory usage, GC pressure\n  2. Non-blocking async: Release thread during IO wait → serve more with fewer threads\n  3. Virtual threads (Java 21): OS-level non-blocking, write blocking code\n</code></pre>\n<h2>Option 1: Blocking I/O with Bounded Thread Pool</h2>\n<p>The simplest model. Still correct for most applications with moderate concurrency. If your service handles fewer than a few hundred concurrent requests and all of them are short CRUD operations, this is likely all you need — additional complexity buys you nothing:</p>\n<pre><code class=\"language-java\">@Service\npublic class OrderService {\n\n    @Autowired\n    private OrderRepository repository;  // Blocking JDBC\n\n    // Runs on Tomcat thread pool (default 200 threads)\n    // Blocking — holds thread during DB wait\n    public Order getOrder(String orderId) {\n        return repository.findById(orderId)  // Blocks thread here\n            .orElseThrow(() -> new OrderNotFoundException(orderId));\n    }\n}\n</code></pre>\n<p><strong>When this is fine:</strong></p>\n<ul>\n<li>&#x3C; 200 concurrent requests that involve IO</li>\n<li>Simple CRUD operations</li>\n<li>Teams unfamiliar with async patterns (simplicity wins)</li>\n</ul>\n<p><strong>When this breaks:</strong></p>\n<ul>\n<li>\n<blockquote>\n<p>1000 concurrent requests with IO wait</p>\n</blockquote>\n</li>\n<li>Long-polling, WebSockets, streaming endpoints</li>\n<li>Services calling 5+ downstream APIs per request</li>\n</ul>\n<h2>Option 2: CompletableFuture — Async Composition</h2>\n<p>CompletableFuture (Java 8+) runs tasks asynchronously and composes their results without blocking threads. It is most useful when you need to execute multiple independent IO operations in parallel and combine their results — turning sequential waits into concurrent ones. The following example builds a dashboard by fetching a user profile, recent orders, and notifications at the same time rather than one after another:</p>\n<pre><code class=\"language-java\">@Service\npublic class DashboardService {\n\n    // Run three API calls concurrently — don't wait for each one serially\n    public CompletableFuture&#x3C;Dashboard> buildDashboard(String userId) {\n        Executor executor = ForkJoinPool.commonPool(); // Or custom executor\n\n        CompletableFuture&#x3C;UserProfile> profileFuture =\n            CompletableFuture.supplyAsync(() -> fetchProfile(userId), executor);\n\n        CompletableFuture&#x3C;List&#x3C;Order>> ordersFuture =\n            CompletableFuture.supplyAsync(() -> fetchRecentOrders(userId, 10), executor);\n\n        CompletableFuture&#x3C;List&#x3C;Notification>> notifFuture =\n            CompletableFuture.supplyAsync(() -> fetchNotifications(userId), executor);\n\n        // Combine all three: continue only when all complete\n        return CompletableFuture.allOf(profileFuture, ordersFuture, notifFuture)\n            .thenApply(__ -> new Dashboard(\n                profileFuture.join(),   // .join() here is safe — allOf guarantees completion\n                ordersFuture.join(),\n                notifFuture.join()\n            ))\n            .exceptionally(e -> {\n                log.error(\"Dashboard build failed for user {}: {}\", userId, e.getMessage());\n                return Dashboard.empty(); // Graceful degradation\n            });\n    }\n}\n</code></pre>\n<h3>CompletableFuture Chaining</h3>\n<p>Once you have a single async result, you can chain subsequent operations using <code>thenApply</code> and <code>thenCompose</code>. The distinction between these two methods is the most important thing to understand: use <code>thenApply</code> when the next step is synchronous, and <code>thenCompose</code> when the next step is itself asynchronous and returns a <code>CompletableFuture</code>. Mixing them up leads to nested <code>CompletableFuture&#x3C;CompletableFuture&#x3C;T>></code> types that are difficult to unwrap correctly:</p>\n<pre><code class=\"language-java\">public CompletableFuture&#x3C;String> processOrder(String orderId) {\n    return CompletableFuture\n        .supplyAsync(() -> orderRepository.findById(orderId))    // Fetch order\n        .thenApply(order -> validateOrder(order))                // Validate (sync)\n        .thenCompose(order -> inventoryService.reserveAsync(order)) // Reserve (async)\n        .thenCompose(reserved -> paymentService.chargeAsync(reserved)) // Charge (async)\n        .thenApply(result -> result.getConfirmationId())        // Extract ID (sync)\n        .whenComplete((result, error) -> {\n            if (error != null) {\n                auditLog.logFailure(orderId, error);\n            } else {\n                auditLog.logSuccess(orderId, result);\n            }\n        });\n}\n</code></pre>\n<p><strong>Key CompletableFuture methods:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Input</th>\n<th>Output</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>thenApply</code></td>\n<td>sync function</td>\n<td>CF of result</td>\n<td>Transform result synchronously</td>\n</tr>\n<tr>\n<td><code>thenCompose</code></td>\n<td>async function returning CF</td>\n<td>CF of result</td>\n<td>Chain async operations (flatMap)</td>\n</tr>\n<tr>\n<td><code>thenCombine</code></td>\n<td>two CFs</td>\n<td>CF of combined</td>\n<td>Merge two concurrent results</td>\n</tr>\n<tr>\n<td><code>allOf</code></td>\n<td>N CFs</td>\n<td>CF<Void></td>\n<td>Wait for all</td>\n</tr>\n<tr>\n<td><code>anyOf</code></td>\n<td>N CFs</td>\n<td>CF<Object></td>\n<td>First to complete wins</td>\n</tr>\n<tr>\n<td><code>exceptionally</code></td>\n<td>exception handler</td>\n<td>CF of fallback</td>\n<td>Handle errors</td>\n</tr>\n<tr>\n<td><code>whenComplete</code></td>\n<td>BiConsumer</td>\n<td>CF of result</td>\n<td>Side effect on completion</td>\n</tr>\n</tbody>\n</table>\n<h3>CompletableFuture Pitfalls</h3>\n<p>CompletableFuture is easy to misuse in ways that silently defeat its purpose. The two most common mistakes involve either blocking inside the async chain, or using the wrong thread pool for IO-bound work:</p>\n<pre><code class=\"language-java\">// WRONG: Blocking inside async chain — wastes the thread\nCompletableFuture.supplyAsync(() -> {\n    return httpClient.get(url).get(); // .get() BLOCKS the thread!\n    // Defeats the purpose of async\n});\n\n// WRONG: Using ForkJoinPool for blocking IO\nCompletableFuture.supplyAsync(() -> jdbcTemplate.queryForList(sql));\n// ForkJoinPool is for CPU-bound work — blocking IO starves it\n// Use a dedicated IO thread pool instead\n\n// RIGHT: Separate executor for IO-bound async tasks\nExecutor ioExecutor = Executors.newFixedThreadPool(50);\n\nCompletableFuture.supplyAsync(() -> jdbcTemplate.queryForList(sql), ioExecutor);\n</code></pre>\n<p>The reason <code>ForkJoinPool</code> is wrong for IO work is subtle: <code>ForkJoinPool</code> is designed to keep all threads busy with CPU work by work-stealing. If your tasks block on IO, those threads sit idle and the pool cannot compensate by spinning up new ones — you end up with all threads blocked and new tasks queuing up behind them.</p>\n<h2>Option 3: Project Reactor (Spring WebFlux)</h2>\n<p>Reactor provides a fully non-blocking reactive pipeline using <code>Mono</code> (0-1 elements) and <code>Flux</code> (0-N elements). Unlike <code>CompletableFuture</code>, which models a single eventual value, Reactor can model streams of values over time — making it the right choice for server-sent events, WebSocket feeds, and any scenario where the producer generates data faster than the consumer can process it. The entire call stack must be non-blocking — including database drivers (R2DBC) and HTTP clients (WebClient):</p>\n<pre><code class=\"language-java\">@RestController\npublic class ReactiveOrderController {\n\n    @Autowired\n    private R2dbcOrderRepository repository;  // Non-blocking R2DBC\n\n    @Autowired\n    private WebClient inventoryClient;\n\n    // Flux: stream of events (Server-Sent Events)\n    @GetMapping(value = \"/orders/stream\", produces = MediaType.TEXT_EVENT_STREAM_VALUE)\n    public Flux&#x3C;Order> streamOrders() {\n        return repository.findAll()\n            .delayElements(Duration.ofMillis(100)) // Throttle for streaming\n            .doOnError(e -> log.error(\"Stream error\", e));\n    }\n\n    // Mono: single order with enrichment\n    @GetMapping(\"/orders/{id}\")\n    public Mono&#x3C;OrderDetail> getOrderDetail(@PathVariable String id) {\n        return repository.findById(id)\n            .switchIfEmpty(Mono.error(new OrderNotFoundException(id)))\n            .flatMap(order ->\n                inventoryClient.get()\n                    .uri(\"/items/{id}\", order.getItemId())\n                    .retrieve()\n                    .bodyToMono(Item.class)\n                    .map(item -> new OrderDetail(order, item))\n            )\n            .timeout(Duration.ofSeconds(3))\n            .onErrorResume(TimeoutException.class, e -> {\n                log.warn(\"Inventory timeout for order {}\", id);\n                return repository.findById(id).map(o -> new OrderDetail(o, Item.unknown()));\n            });\n    }\n}\n</code></pre>\n<p>The <code>onErrorResume</code> block at the end demonstrates one of Reactor's strengths over <code>CompletableFuture</code>: typed error handling. You can match on the specific exception type (<code>TimeoutException</code>) and provide a degraded response — in this case returning the order with an <code>Item.unknown()</code> placeholder rather than failing the entire request.</p>\n<h3>subscribeOn vs publishOn</h3>\n<p>The most confusing part of Reactor is understanding which thread executes which part of your pipeline. Reactor uses a scheduler model where you explicitly control thread assignment. The key mental model is that <code>subscribeOn</code> affects the entire upstream (where subscription starts), while <code>publishOn</code> is a one-way switch that affects only the operators that come after it:</p>\n<pre><code class=\"language-java\">Flux.fromIterable(largeList)\n    .subscribeOn(Schedulers.boundedElastic())  // Which thread SUBSCRIBES (starts) the chain\n    .map(item -> expensiveComputation(item))   // Runs on subscribeOn thread\n    .publishOn(Schedulers.parallel())          // Switch thread for downstream ops\n    .map(result -> transformResult(result))    // Runs on parallel scheduler\n    .subscribe(result -> log.info(result));    // Runs on parallel scheduler\n\n// subscribeOn: affects where the entire upstream runs\n// publishOn: switches scheduler for operations AFTER it in the chain\n</code></pre>\n<p><strong>Reactor schedulers:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Scheduler</th>\n<th>Use For</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>Schedulers.parallel()</code></td>\n<td>CPU-bound work, sized to CPU cores</td>\n</tr>\n<tr>\n<td><code>Schedulers.boundedElastic()</code></td>\n<td>Blocking IO wrappers, sized dynamically (max 10×CPU)</td>\n</tr>\n<tr>\n<td><code>Schedulers.immediate()</code></td>\n<td>Current thread (no context switch)</td>\n</tr>\n<tr>\n<td><code>Schedulers.single()</code></td>\n<td>Sequential background tasks, 1 thread</td>\n</tr>\n</tbody>\n</table>\n<h3>Wrapping Blocking IO in Reactor</h3>\n<p>If you are migrating an existing application to WebFlux incrementally, you will likely have some JDBC or legacy code that cannot be made reactive immediately. The correct approach is to wrap it in <code>Mono.fromCallable</code> and offload it to <code>boundedElastic</code>, which is specifically designed to handle a dynamic number of blocking IO tasks:</p>\n<pre><code class=\"language-java\">// Database calls (without R2DBC), legacy APIs — wrap in boundedElastic\npublic Mono&#x3C;Order> getOrder(String id) {\n    return Mono.fromCallable(() -> jdbcOrderRepository.findById(id)) // Blocking\n        .subscribeOn(Schedulers.boundedElastic()) // Run on IO-capable scheduler\n        .doOnError(e -> log.error(\"DB error\", e));\n}\n\n// DON'T: Call blocking code on parallel() scheduler — starves CPU threads\npublic Mono&#x3C;Order> broken(String id) {\n    return Mono.fromCallable(() -> jdbcOrderRepository.findById(id))\n        .subscribeOn(Schedulers.parallel()); // WRONG for IO\n}\n</code></pre>\n<h2>Option 4: Virtual Threads (Java 21)</h2>\n<p>Virtual threads write like blocking code but scale like reactive code. If you are starting a new project on Java 21, virtual threads should be your first consideration for IO-bound services — they give you the readability of blocking code without the scalability ceiling of platform threads and without the learning curve of reactive programming:</p>\n<pre><code class=\"language-java\">// Same blocking code — but runs on a virtual thread\n// No CompletableFuture composition, no subscribeOn, no reactive operators\n@GetMapping(\"/orders/{id}\")\npublic OrderDetail getOrderDetail(@PathVariable String id) {\n    // All these block the current virtual thread — not the OS carrier thread\n    Order order = orderRepository.findById(id)  // Blocks VT\n        .orElseThrow(() -> new OrderNotFoundException(id));\n\n    Item item = inventoryClient.getItem(order.getItemId()); // Blocks VT\n\n    return new OrderDetail(order, item); // Simple, readable, debuggable\n}\n</code></pre>\n<p>Compare this to the equivalent <code>Mono</code>-based version above. The virtual thread version is shorter, has full stack traces, works naturally with debuggers, and is immediately readable to anyone who knows Java — without sacrificing the ability to handle thousands of concurrent requests. See the Java Virtual Threads article for full details on configuration and pitfalls.</p>\n<h2>Decision Tree</h2>\n<p>With all four options covered, use this decision tree to choose the right approach for your situation. The single most impactful question is whether you are on Java 21 or later — if you are, virtual threads eliminate most of the reasons to reach for <code>CompletableFuture</code> or Reactor for IO-bound work:</p>\n<pre><code>Is your workload IO-bound (DB, HTTP, files)?\n├── YES: How many concurrent requests?\n│   ├── &#x3C; 500: Platform threads (blocking), simple, fast to develop\n│   ├── 500-10K: Virtual threads (Java 21) OR CompletableFuture\n│   └── > 10K: Virtual threads (Java 21) OR Reactive (WebFlux + R2DBC)\n│\n└── NO (CPU-bound: sorting, compression, ML inference):\n    └── ForkJoinPool / parallel streams (thread-per-core)\n\nUsing Java 21+?\n├── YES: Virtual threads for most IO cases — simple and scalable\n└── NO:  CompletableFuture (fan-out) or Reactor (streaming, backpressure)\n\nNeed backpressure (consumer slower than producer)?\n└── YES: Project Reactor Flux — built-in backpressure via demand signals\n\nNeed to stream data to client (SSE, WebSocket)?\n└── YES: Project Reactor Flux with streaming media type\n</code></pre>\n<h2>Performance Comparison (IO-bound, 50ms wait per request)</h2>\n<p>The numbers in the table below are the most important takeaway from this article. They quantify what the decision tree above implies: at low concurrency every approach performs similarly, but the differences become dramatic as you scale past the platform thread pool limit. Notice how blocking threads hit a hard wall while the other approaches continue to scale:</p>\n<pre><code>Concurrency │ Blocking (200 threads) │ CompletableFuture │ Reactor │ Virtual Threads\n────────────┼────────────────────────┼────────────────────┼─────────┼────────────────\n       200  │ 3,900 rps, p99: 52ms   │ 3,950 rps, 51ms   │ 3,980   │ 3,960 rps, 51ms\n     1,000  │   980 rps, p99: 1.02s  │ 9,700 rps, 103ms  │ 9,800   │ 9,800 rps, 52ms\n    10,000  │ timeout (queue full)   │ 47K rps, 210ms    │ 96K rps  │ 97K rps, 53ms\n    50,000  │ OOM                    │ 50K rps, OOM risk │ 97K rps  │ 96K rps, 56ms\n</code></pre>\n<p><strong>Takeaways:</strong></p>\n<ul>\n<li>CompletableFuture helps but still requires thread pool management</li>\n<li>Reactor and virtual threads achieve similar throughput for IO-bound workloads</li>\n<li>Reactor has better backpressure for streaming; virtual threads are simpler to write and maintain</li>\n</ul>\n<p>The right choice depends on your Java version, team familiarity, and streaming requirements — not on which paradigm is theoretically \"best\".</p>\n","tableOfContents":[{"id":"the-core-problem-threads-are-expensive","text":"The Core Problem: Threads Are Expensive","level":2},{"id":"option-1-blocking-io-with-bounded-thread-pool","text":"Option 1: Blocking I/O with Bounded Thread Pool","level":2},{"id":"option-2-completablefuture-async-composition","text":"Option 2: CompletableFuture — Async Composition","level":2},{"id":"completablefuture-chaining","text":"CompletableFuture Chaining","level":3},{"id":"completablefuture-pitfalls","text":"CompletableFuture Pitfalls","level":3},{"id":"option-3-project-reactor-spring-webflux","text":"Option 3: Project Reactor (Spring WebFlux)","level":2},{"id":"subscribeon-vs-publishon","text":"subscribeOn vs publishOn","level":3},{"id":"wrapping-blocking-io-in-reactor","text":"Wrapping Blocking IO in Reactor","level":3},{"id":"option-4-virtual-threads-java-21","text":"Option 4: Virtual Threads (Java 21)","level":2},{"id":"decision-tree","text":"Decision Tree","level":2},{"id":"performance-comparison-io-bound-50ms-wait-per-request","text":"Performance Comparison (IO-bound, 50ms wait per request)","level":2}]},"relatedPosts":[{"title":"Java Concurrency Patterns: CompletableFuture, Structured Concurrency, and Thread-Safe Design","description":"Production Java concurrency: CompletableFuture pipelines, handling exceptions in async chains, Java 21 structured concurrency, thread-safe collection patterns, and the concurrency bugs that cause data corruption.","date":"2025-07-08","category":"Java","tags":["java","concurrency","completablefuture","virtual threads","java21","thread-safe","async"],"featured":false,"affiliateSection":"java-courses","slug":"java-concurrency-patterns","readingTime":"7 min read","excerpt":"Java concurrency has three eras: raw  and  (Java 1-4), the  framework (Java 5+), and the virtual thread/structured concurrency era (Java 21+). Each era's patterns still exist in production codebases. Understanding all th…"},{"title":"Java Memory Management Deep Dive: Heap, GC, and Production Tuning","description":"How the JVM allocates memory, how G1GC and ZGC work under the hood, heap analysis with JVM tools, and the GC tuning decisions that eliminate latency spikes in production Java services.","date":"2025-06-13","category":"Java","tags":["java","jvm","garbage collection","g1gc","zgc","heap","memory management","performance"],"featured":false,"affiliateSection":"java-courses","slug":"java-memory-management-deep-dive","readingTime":"7 min read","excerpt":"Java's garbage collector is the single biggest source of unexplained latency spikes in production services. A GC pause of 2 seconds is invisible in most logs but visible to every user who happened to make a request durin…"},{"title":"Spring Security OAuth2 and JWT: Production Implementation Guide","description":"Complete Spring Security OAuth2 implementation: JWT token validation, Resource Server configuration, method-level security, custom UserDetailsService, refresh token rotation, and the security pitfalls that lead to authentication bypasses.","date":"2025-06-03","category":"Java","tags":["spring security","oauth2","jwt","spring boot","authentication","authorization","java","security"],"featured":false,"affiliateSection":"java-courses","slug":"spring-security-oauth2-jwt","readingTime":"7 min read","excerpt":"Spring Security is one of the most powerful and most misunderstood frameworks in the Java ecosystem. Its flexibility is its strength — and its complexity. Misconfigured security is worse than no security, because it give…"}]},"__N_SSG":true}