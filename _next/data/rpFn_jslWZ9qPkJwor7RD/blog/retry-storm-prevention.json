{"pageProps":{"post":{"title":"Designing a Retry System Without Causing a Retry Storm","description":"Exponential backoff with jitter, circuit breakers, bulkhead isolation, Kafka retry topics, and the retry amplification problem — with Java implementations and a real outage postmortem.","date":"2025-05-22","category":"System Design","tags":["retry","circuit breaker","resilience","spring boot","kafka","distributed systems","java"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"retry-storm-prevention","readingTime":"10 min read","excerpt":"Retry logic is the second most dangerous code in a distributed system, after \"delete all records.\" The intent is to improve reliability by recovering from transient failures. The actual effect, when implemented naively, …","contentHtml":"<p>Retry logic is the second most dangerous code in a distributed system, after \"delete all records.\" The intent is to improve reliability by recovering from transient failures. The actual effect, when implemented naively, is to turn a brief service degradation into a cascading system-wide outage.</p>\n<p>The pattern is predictable: a downstream service slows down, client retries pile up, the downstream service is now handling 3× the original load while already struggling, it slows down more, more retries, complete failure. A retry storm.</p>\n<p>This article covers every layer of the retry stack — from jitter algorithms to Kafka DLQ topology — and the production outage that reshaped how our team thinks about retry.</p>\n<h2>Exponential Backoff: Why Linear Retry Is Wrong</h2>\n<p>Linear retry: wait 1 second, then try again. If the service is down for 60 seconds and you have 1,000 clients, each retrying every second, that's 1,000 × 60 = 60,000 retry requests during the outage. When the service recovers, it receives all 1,000 pending retries simultaneously.</p>\n<p>Exponential backoff: each retry waits 2× longer than the previous:</p>\n<pre><code>Attempt 1: wait 1s\nAttempt 2: wait 2s\nAttempt 3: wait 4s\nAttempt 4: wait 8s\nAttempt 5: wait 16s → give up\n</code></pre>\n<p>Total client load during a 60-second outage with exponential backoff: 5 retries per client × 1,000 clients = 5,000 requests — 12× fewer than linear retry.</p>\n<p>But exponential backoff alone still causes a thundering herd on recovery: all 1,000 clients synchronized on the same backoff schedule will all retry at t=1s, t=2s, t=4s simultaneously.</p>\n<h2>Jitter: The Fix for Synchronized Retries</h2>\n<p>Jitter adds randomness to the wait time, spreading retries across a time window:</p>\n<pre><code class=\"language-java\">public class ExponentialBackoffWithJitter {\n\n    private final int maxAttempts;\n    private final long baseDelayMs;\n    private final long maxDelayMs;\n    private final double jitterFactor;\n\n    // Full jitter: random(0, min(maxDelay, baseDelay * 2^attempt))\n    public long computeDelay(int attempt) {\n        long exponentialDelay = (long) (baseDelayMs * Math.pow(2, attempt));\n        long cappedDelay = Math.min(maxDelayMs, exponentialDelay);\n        return ThreadLocalRandom.current().nextLong(0, cappedDelay);\n    }\n\n    // Equal jitter: split between base and random portion\n    // Guarantees minimum wait while still spreading load\n    public long computeEqualJitterDelay(int attempt) {\n        long exponentialDelay = (long) (baseDelayMs * Math.pow(2, attempt));\n        long capped = Math.min(maxDelayMs, exponentialDelay);\n        return (capped / 2) + ThreadLocalRandom.current().nextLong(0, capped / 2);\n    }\n\n    // Decorrelated jitter (AWS recommendation): harder to reason about but best distribution\n    public long computeDecorrelatedDelay(int attempt, long previousDelay) {\n        return Math.min(maxDelayMs,\n            ThreadLocalRandom.current().nextLong(baseDelayMs, previousDelay * 3));\n    }\n}\n</code></pre>\n<p><strong>Which jitter to use:</strong></p>\n<ul>\n<li>Full jitter: best load distribution, minimum guaranteed wait is 0 (acceptable for most cases)</li>\n<li>Equal jitter: ensures some minimum delay, good for avoiding hammering on immediate retry</li>\n<li>Decorrelated jitter: AWS's recommended approach, best statistical distribution</li>\n</ul>\n<h2>Circuit Breaker</h2>\n<p>The circuit breaker prevents retrying a service that's known to be down. Instead of each client independently discovering the service is down, the circuit breaker shares this knowledge:</p>\n<pre><code>Circuit Breaker States:\n\nCLOSED (normal):\n  Requests flow through\n  Failure rate monitored\n  If failure rate > threshold → open circuit\n         │\n         ▼\nOPEN (service is down):\n  All requests rejected immediately (fail fast)\n  No requests sent to downstream\n  After timeout → move to half-open\n         │\n         ▼\nHALF-OPEN (testing recovery):\n  Limited requests allowed through\n  If they succeed → close circuit\n  If they fail → re-open circuit\n</code></pre>\n<pre><code class=\"language-java\">@Bean\npublic CircuitBreaker paymentCircuitBreaker(CircuitBreakerRegistry registry) {\n    CircuitBreakerConfig config = CircuitBreakerConfig.custom()\n        .slidingWindowType(SlidingWindowType.TIME_BASED)\n        .slidingWindowSize(30)                           // 30 seconds window\n        .minimumNumberOfCalls(10)                        // Min calls before evaluating\n        .failureRateThreshold(50)                        // Open at 50% failure rate\n        .slowCallRateThreshold(80)                       // Also open at 80% slow calls\n        .slowCallDurationThreshold(Duration.ofSeconds(2))\n        .waitDurationInOpenState(Duration.ofSeconds(30)) // Stay open 30s\n        .permittedNumberOfCallsInHalfOpenState(5)\n        .recordExceptions(IOException.class, TimeoutException.class,\n                         ServiceUnavailableException.class)\n        .ignoreExceptions(ValidationException.class,    // Don't count business logic failures\n                         AuthenticationException.class)\n        .build();\n\n    return registry.circuitBreaker(\"payment-service\", config);\n}\n\n// Usage:\npublic PaymentResult charge(PaymentRequest request) {\n    return circuitBreaker.executeSupplier(() -> {\n        return paymentClient.charge(request);\n    });\n}\n</code></pre>\n<p>The <code>slowCallDurationThreshold</code> is critical and often missed. A service that responds in 5 seconds is not \"failing\" — the exception-based circuit breaker stays closed. But 5-second calls are exhausting your thread pool. Trip the circuit breaker on slow calls, not just errors.</p>\n<h2>Bulkhead Pattern</h2>\n<p>Bulkheads isolate failure domains. Without bulkheads, a slow downstream service exhausts all your thread pool capacity, taking down unrelated functionality.</p>\n<pre><code>Without bulkhead:\nAll requests share 200 Tomcat threads\nPayment service slow → 200 threads busy waiting for payments\nAll other endpoints (search, profile, cart) → timeout\n\nWith bulkhead:\n┌─────────────────────────────────────────────┐\n│ Tomcat (200 total threads)                  │\n│                                             │\n│ ┌──────────────┐  ┌──────────────┐          │\n│ │ Payment pool │  │ Search pool  │          │\n│ │ 30 threads   │  │ 50 threads   │          │\n│ └──────────────┘  └──────────────┘          │\n│ ┌──────────────┐  ┌──────────────┐          │\n│ │ Profile pool │  │ Cart pool    │          │\n│ │ 20 threads   │  │ 20 threads   │          │\n│ └──────────────┘  └──────────────┘          │\n└─────────────────────────────────────────────┘\n</code></pre>\n<pre><code class=\"language-java\">// Resilience4j ThreadPoolBulkhead:\n@Bean\npublic ThreadPoolBulkhead paymentBulkhead(ThreadPoolBulkheadRegistry registry) {\n    ThreadPoolBulkheadConfig config = ThreadPoolBulkheadConfig.custom()\n        .maxThreadPoolSize(30)\n        .coreThreadPoolSize(15)\n        .queueCapacity(20)          // Queue depth before rejecting\n        .keepAliveDuration(Duration.ofSeconds(20))\n        .build();\n\n    return registry.bulkhead(\"payment\", config);\n}\n\npublic CompletableFuture&#x3C;PaymentResult> chargeAsync(PaymentRequest request) {\n    return paymentBulkhead.executeSupplier(() ->\n        CompletableFuture.supplyAsync(() -> paymentClient.charge(request))\n    );\n}\n</code></pre>\n<p>When the payment service is slow, only the 30 payment threads are affected. Search, profile, and cart keep running with their own thread pools.</p>\n<h2>Dead Letter Queues</h2>\n<p>Messages that consistently fail need to go somewhere that isn't \"try again forever.\" DLQ design:</p>\n<pre><code>DLQ requirements:\n1. Not lost (durable storage)\n2. Observable (alert on DLQ message arrival)\n3. Reprocessable (ability to replay after fix)\n4. Auditable (track when message failed and why)\n\nDLQ record schema:\n┌────────────────────────────────────────┐\n│ original_payload: &#x3C;message bytes>      │\n│ original_topic: \"payments\"             │\n│ original_partition: 7                  │\n│ original_offset: 10034567              │\n│ failure_count: 4                       │\n│ last_failure_time: 2025-04-15T14:32:00 │\n│ last_error: \"GatewayTimeoutException\"  │\n│ last_error_trace: \"...\"               │\n└────────────────────────────────────────┘\n</code></pre>\n<h2>Kafka Retry Topics</h2>\n<p>Kafka's at-least-once delivery means consumers must handle retries. The naive approach — retry in the consumer loop — holds the partition and blocks other messages. Use a retry topic pattern instead:</p>\n<pre><code>Kafka Retry Topology:\n\npayments (main)\n    │\n    ▼\nConsumer (payments-group)\n    │\n    ├── Success → ack, continue\n    │\n    ├── Retryable failure (1st time)\n    │       └── Publish to payments-retry-30s\n    │\n    └── Non-retryable → payments-dlq (immediately)\n\npayments-retry-30s\n    │  Consumer pauses 30s before consuming\n    ▼\nConsumer (retry-group-30s)\n    │\n    ├── Success → ack\n    ├── Retryable → payments-retry-5m\n    └── Max retries → payments-dlq\n\npayments-retry-5m → payments-retry-30m → payments-dlq\n</code></pre>\n<pre><code class=\"language-java\">@RetryableTopic(\n    attempts = \"4\",\n    backoff = @Backoff(\n        delay = 30_000,           // 30 seconds initial\n        multiplier = 6,           // × 6 each attempt: 30s, 3m, 18m\n        maxDelay = 1_800_000      // Cap at 30 minutes\n    ),\n    dltStrategy = DltStrategy.FAIL_ON_ERROR,\n    autoCreateTopics = \"false\",   // Create topics via IaC, not code\n    include = {\n        RetryablePaymentException.class,\n        GatewayTimeoutException.class\n    },\n    exclude = {\n        NonRetryablePaymentException.class,\n        InvalidRequestException.class\n    }\n)\n@KafkaListener(topics = \"payments\", groupId = \"payment-processor\")\npublic void process(ConsumerRecord&#x3C;String, PaymentEvent> record) {\n    paymentService.process(record.value());\n}\n\n@DltHandler\npublic void handleDeadLetter(\n        PaymentEvent event,\n        @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,\n        @Header(KafkaHeaders.EXCEPTION_FQCN) String exceptionFqcn) {\n    deadLetterService.record(event, topic, exceptionFqcn);\n    alertingService.notifyDltArrival(event);\n}\n</code></pre>\n<h2>Idempotency Considerations</h2>\n<p>Retries without idempotent consumers cause duplicate processing. Every message that might be retried must be safe to process twice:</p>\n<pre><code class=\"language-java\">@Transactional\npublic void processPayment(PaymentEvent event) {\n    // Idempotency check upfront\n    if (processedPayments.existsByEventId(event.getEventId())) {\n        log.info(\"Duplicate event skipped: {}\", event.getEventId());\n        return;\n    }\n\n    // Mark as processing (atomic insert, fails on duplicate)\n    processedPayments.markProcessing(event.getEventId());\n\n    try {\n        PaymentResult result = gateway.charge(event);\n        processedPayments.markComplete(event.getEventId(), result);\n    } catch (Exception e) {\n        processedPayments.markFailed(event.getEventId(), e.getMessage());\n        throw e; // Re-throw for retry\n    }\n}\n</code></pre>\n<h2>Retry Amplification Problem</h2>\n<p>Each service in a call chain that retries independently amplifies the total request count:</p>\n<pre><code>Service A calls B calls C calls D\nEach service retries 3 times on failure\n\nD fails:\nC retries D 3 times → 3 calls to D\nB retries C 3 times → 3 × 3 = 9 calls to D\nA retries B 3 times → 3 × 3 × 3 = 27 calls to D\n\n27 calls to D for 1 user request\nAt 1,000 concurrent users: 27,000 calls to D\n</code></pre>\n<p>The fix: <strong>retry at the edge, not in the interior of a call chain</strong>. Services B and C should not retry — they should propagate errors back to A. A (the edge service, closest to the user) retries the full request.</p>\n<p>Alternatively, use idempotency keys in the retry headers so interior services can deduplicate:</p>\n<pre><code class=\"language-java\">@GetMapping(\"/order\")\npublic ResponseEntity&#x3C;OrderResponse> createOrder(@RequestBody OrderRequest request) {\n    String idempotencyKey = UUID.randomUUID().toString();\n\n    return retryTemplate.execute(context -> {\n        // Same idempotency key on all retries\n        return orderService.createOrder(request, idempotencyKey);\n    });\n}\n</code></pre>\n<h2>Real Outage Scenario</h2>\n<p><strong>System:</strong> Payment processing service, Spring Boot, calls external payment gateway.</p>\n<p><strong>Timeline:</strong></p>\n<ul>\n<li>09:00: Payment gateway experiences degraded performance (30% of requests timing out at 10 seconds)</li>\n<li>09:01: Payment service's <code>RestTemplate</code> has <code>connectTimeout=10s, readTimeout=10s</code></li>\n<li>09:01: Retry logic: 3 retries with 1-second delay (linear, no jitter)</li>\n<li>09:02: 1,000 concurrent payment requests × 3 retries × 10s timeout = 30,000 seconds of thread holding. 200 Tomcat threads exhausted within 2 minutes.</li>\n<li>09:03: Payment service appears down. API gateway returns 503. Alert fires.</li>\n<li>09:03: On-call restarts payment service. Gateway still degraded. Service exhausts threads again in 90 seconds.</li>\n<li>09:04: Retry on restart causes 3,000 requests to hit the still-struggling gateway simultaneously (retry storm on service startup)</li>\n<li>09:15: Gateway recovers. Payment service recovers.</li>\n<li><strong>Total downtime:</strong> 15 minutes. Payment service fully available for only 10 minutes of that window.</li>\n</ul>\n<p><strong>Root causes:</strong></p>\n<ol>\n<li>10-second timeout was too long — threads held too long during degradation</li>\n<li>Linear retry with no jitter created synchronized load spikes</li>\n<li>No circuit breaker — service kept sending requests to a known-degraded gateway</li>\n<li>No bulkhead — payment calls exhausted the shared thread pool</li>\n</ol>\n<p><strong>Fixes applied:</strong></p>\n<pre><code class=\"language-java\">// Before:\nrestTemplate.setConnectTimeout(10_000);\nrestTemplate.setReadTimeout(10_000);\n// 3 retries, 1-second delay\n\n// After:\nrestTemplate.setConnectTimeout(2_000);   // 2 seconds - fail fast\nrestTemplate.setReadTimeout(5_000);      // 5 seconds max read\n\nCircuitBreakerConfig config = CircuitBreakerConfig.custom()\n    .slowCallDurationThreshold(Duration.ofSeconds(3))\n    .slowCallRateThreshold(50)\n    .failureRateThreshold(30)\n    .waitDurationInOpenState(Duration.ofSeconds(20))\n    .build();\n\nRetryConfig retryConfig = RetryConfig.custom()\n    .maxAttempts(3)\n    .intervalFunction(IntervalFunction.ofExponentialRandomBackoff(\n        Duration.ofMillis(500),  // base delay\n        2.0,                     // multiplier\n        Duration.ofSeconds(10)   // max delay\n    ))\n    .build();\n</code></pre>\n<p>Result: During the next gateway degradation event (4 weeks later), the circuit breaker opened after 30 seconds of degraded performance, fast-failing requests instead of holding threads, payment service remained available (returning \"payment gateway temporarily unavailable\" to users), gateway recovered, circuit breaker closed, normal operations resumed. Total user-visible downtime: 30 seconds.</p>\n<h2>Monitoring Retry Rates</h2>\n<pre><code class=\"language-java\">@Component\npublic class RetryMetrics {\n\n    @EventListener\n    public void onRetry(RetryOnRetryEvent event) {\n        meterRegistry.counter(\"retry.attempts\",\n            \"service\", event.getName(),\n            \"attempt\", String.valueOf(event.getNumberOfRetryAttempts())\n        ).increment();\n    }\n\n    @EventListener\n    public void onError(RetryOnErrorEvent event) {\n        meterRegistry.counter(\"retry.failures\",\n            \"service\", event.getName(),\n            \"exception\", event.getLastThrowable().getClass().getSimpleName()\n        ).increment();\n    }\n}\n</code></pre>\n<p>Grafana alert: <code>rate(retry.attempts[5m]) > 100</code> — indicates upstream degradation in progress before it becomes an outage.</p>\n<h2>Architecture Diagram</h2>\n<pre><code>Resilient Retry Architecture:\n\nUser Request\n     │\n     ▼\nAPI Gateway\n(rate limiting, auth)\n     │\n     ▼\nEdge Service\n     │\n     ├── Bulkhead: Payment pool (30 threads)\n     │       │\n     │       ├── Circuit Breaker (open/closed/half-open)\n     │       │       │\n     │       │       ▼\n     │       │   Retry (3 attempts, exponential + jitter)\n     │       │       │\n     │       │       ▼\n     │       │   Payment Gateway (external)\n     │       │\n     │       └── Circuit open → Return cached/degraded response\n     │\n     ├── Bulkhead: Inventory pool (20 threads)\n     │       └── [same pattern]\n     │\n     └── Bulkhead: User profile pool (15 threads)\n             └── [same pattern]\n\nAsync Kafka path:\nEvent → payments topic → Consumer\n                              │\n                              ├── Success → payments-processed\n                              ├── Retry   → payments-retry-* topics\n                              └── Max retry → payments-dlq → alert\n</code></pre>\n<p>Retry logic is not a feature — it's infrastructure. It needs the same rigor as your deployment pipeline. An untested retry strategy will fail exactly when you need it most: during an outage, when the retry storm amplifies the problem it was meant to solve.</p>\n","tableOfContents":[{"id":"exponential-backoff-why-linear-retry-is-wrong","text":"Exponential Backoff: Why Linear Retry Is Wrong","level":2},{"id":"jitter-the-fix-for-synchronized-retries","text":"Jitter: The Fix for Synchronized Retries","level":2},{"id":"circuit-breaker","text":"Circuit Breaker","level":2},{"id":"bulkhead-pattern","text":"Bulkhead Pattern","level":2},{"id":"dead-letter-queues","text":"Dead Letter Queues","level":2},{"id":"kafka-retry-topics","text":"Kafka Retry Topics","level":2},{"id":"idempotency-considerations","text":"Idempotency Considerations","level":2},{"id":"retry-amplification-problem","text":"Retry Amplification Problem","level":2},{"id":"real-outage-scenario","text":"Real Outage Scenario","level":2},{"id":"monitoring-retry-rates","text":"Monitoring Retry Rates","level":2},{"id":"architecture-diagram","text":"Architecture Diagram","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}