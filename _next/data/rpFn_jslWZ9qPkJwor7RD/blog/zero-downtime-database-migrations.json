{"pageProps":{"post":{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible — rollback a bad deploy and your code is back to the previous version. Database schema changes are st…","contentHtml":"<p>Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible — rollback a bad deploy and your code is back to the previous version. Database schema changes are stateful and often irreversible — a dropped column is gone, a renamed column leaves old code broken, an index added with a table lock takes your service down.</p>\n<p>The root cause of downtime during migrations is running application code that makes assumptions about schema that don't yet hold (or no longer hold). The solution is a pattern called <strong>expand-contract</strong> combined with backward-compatible intermediate states.</p>\n<h2>The Expand-Contract Pattern</h2>\n<p>Most schema changes can be decomposed into three phases that can each be deployed independently:</p>\n<pre><code>Phase 1: EXPAND — Add new schema alongside old (both versions of app work)\nPhase 2: MIGRATE — Backfill data, transition traffic to new schema\nPhase 3: CONTRACT — Remove old schema (only new app version exists)\n</code></pre>\n<p>This works because at any moment during a rolling deployment, some pods run the old code and some run the new code. Both must work against the same database. Backward-compatible intermediate states ensure both versions work simultaneously.</p>\n<h2>Pattern 1: Renaming a Column</h2>\n<p><strong>The naive approach (causes downtime):</strong></p>\n<pre><code class=\"language-sql\">-- This breaks old code immediately:\nALTER TABLE orders RENAME COLUMN customer_id TO user_id;\n-- Old code writing to customer_id → column not found → 500 errors\n</code></pre>\n<p><strong>The expand-contract approach:</strong></p>\n<p><strong>Phase 1: Expand — Add new column</strong></p>\n<pre><code class=\"language-sql\">-- Migration (deploy with old application code still running):\nALTER TABLE orders ADD COLUMN user_id BIGINT;\n\n-- Application code change (deploy after migration):\n-- Write to BOTH old and new column\n-- Read from old column (primary), fall back to new\nINSERT INTO orders (customer_id, user_id, amount) VALUES (?, ?, ?);\nSELECT COALESCE(user_id, customer_id) AS user_id FROM orders WHERE ...;\n</code></pre>\n<p><strong>Phase 2: Migrate — Backfill data</strong></p>\n<pre><code class=\"language-sql\">-- Run in batches (don't lock the table):\nUPDATE orders SET user_id = customer_id\nWHERE user_id IS NULL\nAND id BETWEEN ? AND ?;  -- Process in chunks of 10,000 rows\n\n-- Repeat until no NULL user_id remain:\n-- SELECT COUNT(*) FROM orders WHERE user_id IS NULL; → 0\n</code></pre>\n<p><strong>Phase 3: Contract — Remove old column</strong></p>\n<pre><code class=\"language-sql\">-- Application code: read from new column only (deployed first)\n-- Then drop old column:\nALTER TABLE orders DROP COLUMN customer_id;\n</code></pre>\n<p>Three separate deployments, zero downtime at each step. The intermediate state (both columns exist, both written) is ugly but safe.</p>\n<h2>Pattern 2: Adding a NOT NULL Column</h2>\n<p>Adding a NOT NULL column with no default to an existing table fails immediately (existing rows don't satisfy the constraint). Even with a default, PostgreSQL pre-14 rewrites the entire table to set the default, causing a long lock.</p>\n<p><strong>PostgreSQL 11+ approach:</strong></p>\n<pre><code class=\"language-sql\">-- Step 1: Add nullable column (fast, no table rewrite):\nALTER TABLE orders ADD COLUMN shipping_address TEXT;\n\n-- Step 2: Application starts writing to new column (deploy new code)\n\n-- Step 3: Backfill existing rows:\nUPDATE orders SET shipping_address = 'Unknown' WHERE shipping_address IS NULL;\n-- (Run in batches: WHERE id BETWEEN ? AND ?)\n\n-- Step 4: Add NOT NULL constraint (PostgreSQL validates, fast if all rows are set):\nALTER TABLE orders ALTER COLUMN shipping_address SET NOT NULL;\n-- Or use a CHECK constraint validated later:\nALTER TABLE orders ADD CONSTRAINT shipping_address_not_null\n    CHECK (shipping_address IS NOT NULL) NOT VALID;\n-- Then validate in background (doesn't lock writes):\nALTER TABLE orders VALIDATE CONSTRAINT shipping_address_not_null;\n-- Then convert to NOT NULL:\nALTER TABLE orders ALTER COLUMN shipping_address SET NOT NULL;\nALTER TABLE orders DROP CONSTRAINT shipping_address_not_null;\n</code></pre>\n<p><code>NOT VALID</code> constraint + <code>VALIDATE CONSTRAINT</code> is the PostgreSQL pattern for adding constraints on large tables without locking. The <code>NOT VALID</code> constraint applies only to new rows (immediate). <code>VALIDATE</code> scans old rows using a weaker lock (ShareUpdateExclusiveLock) that allows reads and writes to continue.</p>\n<h2>Pattern 3: Index Creation Without Locking</h2>\n<p>Standard <code>CREATE INDEX</code> acquires a lock that blocks all writes until the index is built. On a large table (100M rows), this can take minutes.</p>\n<p><strong>PostgreSQL:</strong></p>\n<pre><code class=\"language-sql\">-- WRONG: Blocks writes for duration of build (potentially hours):\nCREATE INDEX ON orders (customer_id);\n\n-- RIGHT: Concurrent build — reads and writes continue, 2× longer build time:\nCREATE INDEX CONCURRENTLY ON orders (customer_id);\n\n-- If CONCURRENTLY fails (crash, etc.), it leaves an INVALID index:\nSELECT schemaname, tablename, indexname, indisvalid\nFROM pg_indexes\nWHERE indisvalid = false;\n-- → Drop the invalid index and retry\n\nDROP INDEX CONCURRENTLY orders_customer_id_idx;\n</code></pre>\n<p><code>CREATE INDEX CONCURRENTLY</code> takes about 2-3× longer than regular index creation but never blocks reads or writes. Always use it in production.</p>\n<p><strong>Adding a unique constraint concurrently:</strong></p>\n<pre><code class=\"language-sql\">-- Unique constraint directly → table lock\n-- Instead: create unique index first, then add constraint using the index\nCREATE UNIQUE INDEX CONCURRENTLY orders_external_id_unique ON orders (external_id);\n\n-- Then create constraint using the pre-built index (fast):\nALTER TABLE orders ADD CONSTRAINT orders_external_id_unique\n    UNIQUE USING INDEX orders_external_id_unique;\n</code></pre>\n<h2>Pattern 4: Large Table Alterations</h2>\n<p>Some alterations trigger full table rewrites — <code>ALTER COLUMN TYPE</code>, adding a column with a volatile default, enabling encryption. In PostgreSQL, these block all reads and writes for the duration.</p>\n<p><strong>Strategy: Shadow table swap</strong></p>\n<pre><code class=\"language-sql\">-- Create new table with desired schema:\nCREATE TABLE orders_new (\n    id BIGINT PRIMARY KEY,\n    user_id BIGINT NOT NULL,         -- new: renamed from customer_id\n    amount NUMERIC(10,2) NOT NULL,\n    currency CHAR(3) NOT NULL DEFAULT 'USD',  -- new: added column\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Copy data in batches (reads from old, writes to new):\nINSERT INTO orders_new (id, user_id, amount, created_at)\nSELECT id, customer_id, amount, created_at\nFROM orders\nWHERE id BETWEEN ? AND ?;\n\n-- Once backfill is complete, swap tables atomically:\nBEGIN;\n  LOCK TABLE orders IN ACCESS EXCLUSIVE MODE;  -- Brief lock — just for rename\n  ALTER TABLE orders RENAME TO orders_old;\n  ALTER TABLE orders_new RENAME TO orders;\n  -- Update sequences, foreign keys, etc.\nCOMMIT;\n\n-- Dual-write during backfill period:\n-- Application writes to both orders (old) and orders_new simultaneously\n-- After swap, drops orders_old\n</code></pre>\n<p>This pattern is what tools like pt-online-schema-change (Percona Toolkit) and gh-ost (GitHub) automate for MySQL. For PostgreSQL, pglogical-based migration tools do the same.</p>\n<h2>Managing Migrations with Flyway/Liquibase</h2>\n<p><strong>Flyway versioned migration structure:</strong></p>\n<pre><code>db/migration/\n  V1__create_orders_table.sql\n  V2__add_customer_id_index.sql\n  V3__add_user_id_column.sql          ← expand\n  V4__backfill_user_id.sql            ← migrate (run separately or in batches)\n  V5__add_user_id_not_null.sql        ← contract step 1\n  V6__drop_customer_id_column.sql     ← contract step 2\n</code></pre>\n<p><strong>Critical Flyway rules for zero-downtime:</strong></p>\n<ol>\n<li>\n<p><strong>Never modify a migration after it's been applied</strong> — Flyway checksums every migration; modification causes startup failure. Create a new migration instead.</p>\n</li>\n<li>\n<p><strong>Separate schema migrations from data migrations</strong> — Schema migrations (V3) run at deploy time. Data migrations (V4 — backfill) should run as background jobs, not blocking app startup.</p>\n</li>\n<li>\n<p><strong>Idempotency for repeatable migrations</strong> — Flyway's R__ prefix for repeatable migrations (views, stored procedures) runs them on every change. Schema migrations (V__) run once.</p>\n</li>\n</ol>\n<pre><code class=\"language-java\">// Spring Boot Flyway config:\n@Configuration\npublic class FlywayConfig {\n\n    @Bean\n    public FlywayMigrationStrategy flywayMigrationStrategy() {\n        return flyway -> {\n            // Run baseline repair if needed\n            flyway.repair();\n            flyway.migrate();\n        };\n    }\n}\n\n// application.properties:\nspring.flyway.locations=classpath:db/migration\nspring.flyway.baseline-on-migrate=true\nspring.flyway.out-of-order=false  // Enforce sequential migration order\nspring.flyway.validate-on-migrate=true\n</code></pre>\n<p><strong>Liquibase for multi-database compatibility:</strong></p>\n<pre><code class=\"language-yaml\"># liquibase/changelog/0003-add-user-id.yaml\ndatabaseChangeLog:\n  - changeSet:\n      id: \"0003-add-user-id-column\"\n      author: \"engineering\"\n      runOnChange: false\n      failOnError: true\n      changes:\n        - addColumn:\n            tableName: orders\n            columns:\n              - column:\n                  name: user_id\n                  type: BIGINT\n                  constraints:\n                    nullable: true  # Start nullable — NOT NULL added later\n      rollback:\n        - dropColumn:\n            tableName: orders\n            columnName: user_id\n</code></pre>\n<h2>Rolling Deployments: The Application Side</h2>\n<p>During a rolling deployment, both old and new pod versions run simultaneously against the same database. Write application code to tolerate this:</p>\n<pre><code class=\"language-java\">// Old code: reads customer_id\n// New code: reads user_id (with fallback during transition)\n\n// Repository method during Phase 1 (both columns exist, old code still deployed):\npublic Long getUserId(Order order) {\n    // New code reads new column, falls back to old if null\n    return order.getUserId() != null ? order.getUserId() : order.getCustomerId();\n}\n\n// Writes to both during transition:\n@Transactional\npublic Order createOrder(OrderRequest request) {\n    Order order = new Order();\n    order.setUserId(request.getUserId());      // New column\n    order.setCustomerId(request.getUserId());  // Old column (backward compat)\n    order.setAmount(request.getAmount());\n    return orderRepository.save(order);\n}\n</code></pre>\n<p>This dual-write period is the most fragile moment. Keep it short — ideally one deployment cycle (hours, not days). Remove the backward-compatible code in the next deployment.</p>\n<h2>Testing Migrations</h2>\n<p>Never run migrations only in production. Test them in staging with production-scale data:</p>\n<pre><code class=\"language-bash\"># Clone production data (anonymized) to staging:\npg_dump --no-acl --no-owner production_db | psql staging_db\n\n# Run migration and time it:\ntime psql staging_db -f V5__add_user_id_not_null.sql\n\n# Check for locks during migration:\npsql staging_db -c \"\nSELECT pid, wait_event_type, wait_event, state, query\nFROM pg_stat_activity\nWHERE state != 'idle'\nORDER BY duration DESC;\"\n\n# Verify no table rewrites (should show no sequential scans on large tables):\npsql staging_db -c \"EXPLAIN ANALYZE &#x3C;your migration SQL>;\"\n</code></pre>\n<p><strong>The pre-flight checklist:</strong></p>\n<ul>\n<li>Is this migration backward-compatible? (Can old app code run with the new schema?)</li>\n<li>Will it acquire a lock? For how long? (Check PostgreSQL lock documentation for each <code>ALTER TABLE</code> variant)</li>\n<li>Can it be made concurrent? (<code>CREATE INDEX CONCURRENTLY</code>, <code>ADD CONSTRAINT ... NOT VALID</code>)</li>\n<li>Is there a rollback path? (For each phase of expand-contract)</li>\n<li>Has it been tested on production-scale data?</li>\n</ul>\n<p>Database migrations are the rare operations where going slow saves time. A 5-minute outage for a botched migration is worth a week's worth of careful planning. The expand-contract pattern is more work than a single ALTER statement, but it's the difference between a boring deploy and an incident postmortem.</p>\n","tableOfContents":[{"id":"the-expand-contract-pattern","text":"The Expand-Contract Pattern","level":2},{"id":"pattern-1-renaming-a-column","text":"Pattern 1: Renaming a Column","level":2},{"id":"pattern-2-adding-a-not-null-column","text":"Pattern 2: Adding a NOT NULL Column","level":2},{"id":"pattern-3-index-creation-without-locking","text":"Pattern 3: Index Creation Without Locking","level":2},{"id":"pattern-4-large-table-alterations","text":"Pattern 4: Large Table Alterations","level":2},{"id":"managing-migrations-with-flywayliquibase","text":"Managing Migrations with Flyway/Liquibase","level":2},{"id":"rolling-deployments-the-application-side","text":"Rolling Deployments: The Application Side","level":2},{"id":"testing-migrations","text":"Testing Migrations","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring — every node is equal, there's no primary, and data placement is determined by partit…"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you…"},{"title":"PostgreSQL Performance Tuning: From Slow Queries to Sub-Millisecond Reads","description":"A production guide to PostgreSQL query optimization: EXPLAIN ANALYZE, index design, VACUUM tuning, connection pooling with PgBouncer, partitioning, and the configuration changes that actually move the needle.","date":"2025-06-03","category":"Databases","tags":["postgresql","databases","performance","sql","indexing","query optimization"],"featured":false,"affiliateSection":"database-resources","slug":"postgresql-performance-tuning","readingTime":"9 min read","excerpt":"PostgreSQL ships with defaults tuned for a 512MB machine from 2005. Every production deployment needs to be re-tuned. Beyond that, most slow queries are not a PostgreSQL problem — they're a query design problem that Post…"}]},"__N_SSG":true}