{"pageProps":{"post":{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…","contentHtml":"<p>Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sound. The practice involves snapshot strategies, projection rebuilding, schema evolution, and operational tooling that most tutorials skip entirely.</p>\n<p>This article is about what comes after you've decided to use event sourcing.</p>\n<h2>The Event Store: Core Design</h2>\n<p>An event store is an append-only log of domain events. Every state change is expressed as an immutable event:</p>\n<pre><code class=\"language-sql\">-- Event store schema (PostgreSQL):\nCREATE TABLE domain_events (\n    id              BIGSERIAL PRIMARY KEY,\n    aggregate_id    UUID NOT NULL,\n    aggregate_type  VARCHAR(100) NOT NULL,       -- 'Order', 'Account', 'Shipment'\n    event_type      VARCHAR(100) NOT NULL,        -- 'OrderPlaced', 'OrderShipped'\n    event_version   INT NOT NULL DEFAULT 1,       -- Schema version for evolution\n    sequence_number BIGINT NOT NULL,              -- Position within aggregate\n    data            JSONB NOT NULL,               -- Event payload\n    metadata        JSONB,                        -- Correlation ID, user ID, etc.\n    occurred_at     TIMESTAMPTZ DEFAULT NOW(),\n    UNIQUE (aggregate_id, sequence_number)        -- No gaps in sequence per aggregate\n);\n\nCREATE INDEX idx_events_aggregate ON domain_events (aggregate_id, sequence_number);\nCREATE INDEX idx_events_type_occurred ON domain_events (event_type, occurred_at DESC);\n</code></pre>\n<p>Loading an aggregate's current state requires replaying its events:</p>\n<pre><code class=\"language-java\">public class OrderRepository {\n\n    public Order load(UUID orderId) {\n        List&#x3C;DomainEvent> events = eventStore.loadEvents(orderId);\n        if (events.isEmpty()) {\n            throw new AggregateNotFoundException(orderId);\n        }\n        return Order.reconstitute(events);\n    }\n\n    public void save(Order order) {\n        List&#x3C;DomainEvent> newEvents = order.getUncommittedEvents();\n        long expectedSequence = order.getSequenceNumber();\n\n        // Optimistic concurrency: if another process saved between our load and save,\n        // sequence number won't match → conflict detected\n        eventStore.append(order.getId(), newEvents, expectedSequence);\n        order.clearUncommittedEvents();\n    }\n}\n\n// Order aggregate:\npublic class Order {\n    private UUID id;\n    private OrderStatus status;\n    private List&#x3C;OrderItem> items;\n    private long sequenceNumber;\n\n    // Reconstitute from events\n    public static Order reconstitute(List&#x3C;DomainEvent> events) {\n        Order order = new Order();\n        events.forEach(order::apply);\n        return order;\n    }\n\n    // Apply event (mutates state, no side effects)\n    private void apply(DomainEvent event) {\n        this.sequenceNumber = event.getSequenceNumber();\n        switch (event) {\n            case OrderPlacedEvent e -> {\n                this.id = e.getOrderId();\n                this.status = OrderStatus.PLACED;\n                this.items = e.getItems();\n            }\n            case OrderShippedEvent e -> this.status = OrderStatus.SHIPPED;\n            case OrderCancelledEvent e -> this.status = OrderStatus.CANCELLED;\n        }\n    }\n}\n</code></pre>\n<h2>Snapshot Strategy</h2>\n<p>For aggregates with long event histories, replaying 10,000 events to load a single aggregate is unacceptable. Snapshots checkpoint the aggregate's state:</p>\n<pre><code class=\"language-sql\">CREATE TABLE aggregate_snapshots (\n    aggregate_id        UUID PRIMARY KEY,\n    aggregate_type      VARCHAR(100) NOT NULL,\n    snapshot_data       JSONB NOT NULL,\n    snapshot_version    INT NOT NULL,            -- Schema version of snapshot\n    sequence_number     BIGINT NOT NULL,         -- Event sequence at snapshot time\n    created_at          TIMESTAMPTZ DEFAULT NOW()\n);\n</code></pre>\n<pre><code class=\"language-java\">public Order loadWithSnapshot(UUID orderId) {\n    // 1. Load most recent snapshot\n    Optional&#x3C;Snapshot> snapshot = snapshotStore.loadLatest(orderId);\n\n    // 2. Load events after snapshot\n    long fromSequence = snapshot.map(Snapshot::getSequenceNumber).orElse(0L);\n    List&#x3C;DomainEvent> events = eventStore.loadEvents(orderId, fromSequence + 1);\n\n    // 3. Reconstitute from snapshot + subsequent events\n    if (snapshot.isPresent()) {\n        Order order = Order.fromSnapshot(snapshot.get());\n        events.forEach(order::apply);\n        return order;\n    } else {\n        return Order.reconstitute(events);\n    }\n}\n\n// Snapshot policy: snapshot after every N events\n@Scheduled(fixedDelay = 60_000)\npublic void snapshotHighVolumeAggregates() {\n    List&#x3C;UUID> candidates = eventStore.findAggregatesWithEventsAbove(\n        SNAPSHOT_THRESHOLD = 100\n    );\n    candidates.forEach(id -> {\n        Order order = load(id);\n        snapshotStore.save(Snapshot.from(order));\n    });\n}\n</code></pre>\n<h2>CQRS: Separate Read and Write Models</h2>\n<p>CQRS (Command Query Responsibility Segregation) pairs naturally with event sourcing: the event store is the write model; projections (denormalized views) are read models built from events.</p>\n<pre><code>CQRS Architecture:\n\nCommand Side:                 Event Side:                Read Side:\n\nUser → OrderController  →   Event Store (append)  →   Projection Worker\n       (Command)             domain_events              ↓\n       ↓                                            Read Model DB\n       Order.apply()                                (PostgreSQL/Elasticsearch/Redis)\n       ↓                                                ↓\n       Emit events →  ──────────────────────────→  API Response\n</code></pre>\n<pre><code class=\"language-java\">// Projection: build a denormalized order summary for quick reads\n@Component\n@Transactional\npublic class OrderSummaryProjection {\n\n    @EventHandler\n    public void on(OrderPlacedEvent event) {\n        orderSummaryRepository.save(new OrderSummary(\n            event.getOrderId(),\n            event.getUserId(),\n            event.getTotalAmount(),\n            \"PLACED\",\n            event.getOccurredAt()\n        ));\n    }\n\n    @EventHandler\n    public void on(OrderShippedEvent event) {\n        orderSummaryRepository.updateStatus(event.getOrderId(), \"SHIPPED\");\n    }\n\n    @EventHandler\n    public void on(OrderItemAddedEvent event) {\n        orderSummaryRepository.addItem(event.getOrderId(), event.getItem());\n    }\n}\n</code></pre>\n<h2>Projection Rebuilding</h2>\n<p>Projections are disposable — they can always be rebuilt from the event store. This is a key advantage. When you add a new projection, or fix a bug in an existing one, you replay all events:</p>\n<pre><code class=\"language-java\">@Component\npublic class ProjectionRebuilder {\n\n    public void rebuild(Class&#x3C;? extends Projection> projectionClass, String aggregateType) {\n        Projection projection = context.getBean(projectionClass);\n\n        // Clear existing projection data\n        projection.reset();\n\n        // Stream events from beginning\n        long position = 0;\n        int batchSize = 1000;\n        List&#x3C;DomainEvent> batch;\n\n        do {\n            batch = eventStore.loadAll(aggregateType, position, batchSize);\n            batch.forEach(event -> {\n                try {\n                    projection.apply(event);\n                } catch (Exception e) {\n                    log.error(\"Failed to apply event {} to projection {}\",\n                        event.getId(), projectionClass.getSimpleName(), e);\n                }\n            });\n            position += batch.size();\n        } while (batch.size() == batchSize);\n\n        log.info(\"Rebuilt projection {} with {} events\", projectionClass.getSimpleName(), position);\n    }\n}\n</code></pre>\n<p>For millions of events, rebuilding in-process is too slow. Use a dedicated rebuild pipeline: stream events from the store to Kafka, run projection workers at full throughput in parallel.</p>\n<h2>Schema Evolution</h2>\n<p>Events are immutable. Once committed, the <code>OrderPlacedEvent</code> from 2022 cannot be changed. But your schema will evolve. Handle this with upcasters — functions that transform old event versions to the current format:</p>\n<pre><code class=\"language-java\">public interface EventUpcaster&#x3C;T extends DomainEvent> {\n    int fromVersion();\n    T upcast(JsonNode rawEvent);\n}\n\n@Component\npublic class OrderPlacedEventV1ToV2Upcaster implements EventUpcaster&#x3C;OrderPlacedEvent> {\n\n    @Override\n    public int fromVersion() { return 1; }\n\n    @Override\n    public OrderPlacedEvent upcast(JsonNode rawEvent) {\n        // V1 had 'customer_id', V2 renamed to 'user_id'\n        ObjectNode upgraded = rawEvent.deepCopy();\n        upgraded.put(\"user_id\", rawEvent.get(\"customer_id\").asText());\n        upgraded.remove(\"customer_id\");\n        return objectMapper.treeToValue(upgraded, OrderPlacedEvent.class);\n    }\n}\n\n// Event serialization layer applies upcasters transparently:\npublic DomainEvent deserialize(StoredEvent stored) {\n    DomainEvent event = objectMapper.readValue(stored.getData(), getDomainEventClass(stored.getEventType()));\n    return upcasterRegistry.upcastChain(event, stored.getEventVersion(), currentVersion());\n}\n</code></pre>\n<h2>The Operational Challenges Nobody Talks About</h2>\n<p><strong>1. Event store grows unboundedly.</strong> Unlike a state-based system where you UPDATE rows, event sourcing only INSERTs. A system with 100 commands/second generates 8.6M events/day. After 1 year: 3.1B events. Plan for:</p>\n<ul>\n<li>Archival: move events older than N years to cold storage (S3 + Parquet)</li>\n<li>Index maintenance: <code>domain_events</code> index grows with the table</li>\n<li>Backup strategies: event stores are large</li>\n</ul>\n<p><strong>2. Eventual consistency between write and read models.</strong> After a command executes, the projection may not be updated for tens of milliseconds. API clients reading immediately after a write may see stale data. Options:</p>\n<ul>\n<li>Return event data directly in command response (avoid the read model for the \"write response\")</li>\n<li>Client polls until the projection catches up (pessimistic, bad UX)</li>\n<li>Optimistic UI updates (update UI before projection confirms)</li>\n</ul>\n<p><strong>3. Process manager complexity.</strong> Long-running business processes (order fulfillment, subscription renewals) require sagas that span multiple aggregates. These are stateful and must handle partial failures. The saga state machine itself needs an event log to be recoverable.</p>\n<p><strong>4. Querying across aggregates.</strong> The event store is aggregate-centric. \"Give me all orders placed in the last 24 hours over $500\" requires a projection. Every new query pattern potentially needs a new projection. Plan your read model database carefully.</p>\n<h2>When Event Sourcing is Worth It</h2>\n<p>Event sourcing is excellent for:</p>\n<ul>\n<li>Domains with audit requirements (finance, healthcare, compliance)</li>\n<li>Complex domain logic where understanding \"how did we get here\" matters</li>\n<li>Systems that benefit from temporal queries (state at any point in time)</li>\n<li>High-throughput write paths (append-only is fast)</li>\n</ul>\n<p>It adds complexity that rarely pays off for:</p>\n<ul>\n<li>Simple CRUD systems</li>\n<li>Systems without audit requirements</li>\n<li>Small teams without DDD expertise</li>\n<li>Services with simple, flat domain models</li>\n</ul>\n<p>The decision should be per-domain, not per-system. Your order management service might benefit from event sourcing; your user profile service almost certainly doesn't.</p>\n","tableOfContents":[{"id":"the-event-store-core-design","text":"The Event Store: Core Design","level":2},{"id":"snapshot-strategy","text":"Snapshot Strategy","level":2},{"id":"cqrs-separate-read-and-write-models","text":"CQRS: Separate Read and Write Models","level":2},{"id":"projection-rebuilding","text":"Projection Rebuilding","level":2},{"id":"schema-evolution","text":"Schema Evolution","level":2},{"id":"the-operational-challenges-nobody-talks-about","text":"The Operational Challenges Nobody Talks About","level":2},{"id":"when-event-sourcing-is-worth-it","text":"When Event Sourcing is Worth It","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"},{"title":"Multi-Tenancy Architecture: Database, Application, and Infrastructure Patterns","description":"Production multi-tenancy: database isolation models (shared schema, shared database, separate database), tenant routing, data partitioning strategies, cross-tenant query prevention, Spring Boot tenant context propagation, and the trade-offs at each isolation level.","date":"2025-05-24","category":"System Design","tags":["multi-tenancy","saas","system design","database","spring boot","architecture","isolation"],"featured":false,"affiliateSection":"system-design-courses","slug":"multi-tenancy-architecture","readingTime":"8 min read","excerpt":"Multi-tenancy is the architecture pattern where a single deployed instance of a software system serves multiple customers (tenants), with each tenant's data logically or physically isolated from others. It's the foundati…"}]},"__N_SSG":true}