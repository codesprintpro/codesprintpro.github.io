{"pageProps":{"post":{"title":"Prompt Engineering: Advanced Techniques for Production LLMs","description":"Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications.","date":"2025-02-26","category":"AI/ML","tags":["ai","llm","prompt engineering","gpt","claude","production"],"featured":false,"affiliateSection":"ai-ml-books","slug":"prompt-engineering-production","readingTime":"11 min read","excerpt":"Most prompt engineering tutorials stop at \"be specific and provide context.\" That's necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-gra…","contentHtml":"<p>Most prompt engineering tutorials stop at \"be specific and provide context.\" That's necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-grade LLM applications: structured output, chain-of-thought, self-consistency, and the ReAct framework for agents.</p>\n<h2>Mental Model: LLMs as Next-Token Predictors</h2>\n<p>Every prompt engineering technique becomes intuitive once you internalize this: <strong>an LLM predicts the most probable next token given the context</strong>. This means:</p>\n<ol>\n<li>The model will continue patterns it sees in the prompt</li>\n<li>Few-shot examples work because they shift the probability distribution</li>\n<li>\"Think step by step\" works because showing the intermediate tokens makes the final answer more probable</li>\n<li>The model doesn't \"understand\" your intent — it finds the most statistically likely completion</li>\n</ol>\n<h2>Technique 1: Zero-Shot vs Few-Shot</h2>\n<p>Zero-shot prompting asks the model to perform a task with instructions alone, while few-shot provides concrete examples first. Think of zero-shot as handing someone a job description and asking them to start immediately, versus few-shot as showing them three completed examples of the work before they begin. For simple tasks zero-shot is sufficient, but for nuanced classifications with subtle category distinctions, examples are far more reliable.</p>\n<h3>Zero-Shot</h3>\n<pre><code>Prompt: \"Classify this email as spam or not spam:\nEmail: 'Congratulations! You've won $1,000,000. Click here to claim.'\nClassification:\"\n\nResponse: \"Spam\"\n</code></pre>\n<h3>Few-Shot (Better for Complex Classifications)</h3>\n<p>Notice how each example below covers a distinct scenario — billing, technical, and account issues. You're not just showing the format; you're showing the model the boundaries between categories by example, which is far more effective than trying to describe those boundaries in words.</p>\n<pre><code>Prompt: \"Classify customer support tickets. Categories: BILLING, TECHNICAL, ACCOUNT, GENERAL.\n\nExample 1:\nTicket: \"My invoice shows a charge I didn't authorize.\"\nCategory: BILLING\n\nExample 2:\nTicket: \"The app crashes when I try to export to PDF.\"\nCategory: TECHNICAL\n\nExample 3:\nTicket: \"I need to transfer my account to a new email address.\"\nCategory: ACCOUNT\n\nNow classify:\nTicket: \"How do I upgrade my subscription plan?\"\nCategory:\"\n\nResponse: \"BILLING\"\n</code></pre>\n<p><strong>Few-shot guidelines:</strong></p>\n<ul>\n<li>Use 3-8 examples (diminishing returns beyond 8 for most tasks)</li>\n<li>Examples should cover edge cases, not just typical cases</li>\n<li>Maintain consistent format between examples and the query</li>\n<li>Order matters: recent examples have more influence (recency bias)</li>\n</ul>\n<h2>Technique 2: Chain-of-Thought (CoT)</h2>\n<p>CoT dramatically improves reasoning on math, logic, and multi-step problems by showing the model that intermediate reasoning is expected.</p>\n<p>The intuition is simple: when you force the model to write out its reasoning step by step, each intermediate conclusion becomes part of the context for the next step. This is why you get correct multi-step answers with CoT that you'd never reliably get from a direct \"just tell me the answer\" prompt — the intermediate steps guide the model toward the right final token.</p>\n<pre><code># Without CoT — often wrong on math\nPrompt: \"A store sells apples for $0.50 each and oranges for $0.75 each.\nIf John bought 3 apples and 5 oranges, how much did he spend?\"\n\nResponse: \"$3.25\"  # Correct, but unreliable for harder problems\n\n# With CoT — much more reliable\nPrompt: \"A store sells apples for $0.50 each and oranges for $0.75 each.\nIf John bought 3 apples and 5 oranges, how much did he spend?\n\nLet's think step by step:\"\n\nResponse:\n\"1. Cost of apples: 3 × $0.50 = $1.50\n2. Cost of oranges: 5 × $0.75 = $3.75\n3. Total: $1.50 + $3.75 = $5.25\n\nJohn spent $5.25.\"\n</code></pre>\n<p><strong>Zero-shot CoT trigger phrases</strong> (any of these work):</p>\n<ul>\n<li>\"Let's think step by step\"</li>\n<li>\"Think through this carefully\"</li>\n<li>\"Reason through each step\"</li>\n<li>\"Let me work through this\"</li>\n</ul>\n<p><strong>Few-shot CoT</strong> — provide examples with explicit reasoning. By showing worked examples, you are training the model's in-context behavior to produce the same style of detailed, step-by-step trace before arriving at an answer.</p>\n<pre><code class=\"language-python\">cot_examples = \"\"\"\nQ: If a train travels 60 mph and needs to cover 150 miles, how long does it take?\nA: Let me work through this step by step:\n   - Distance = 150 miles\n   - Speed = 60 mph\n   - Time = Distance / Speed = 150 / 60 = 2.5 hours\n   The answer is 2.5 hours (2 hours and 30 minutes).\n\nQ: A recipe calls for 2 cups of flour for 12 cookies.\n   How much flour is needed for 30 cookies?\nA: Let me work through this step by step:\n   - Ratio: 2 cups / 12 cookies = 1/6 cup per cookie\n   - For 30 cookies: 30 × (1/6) = 5 cups\n   The answer is 5 cups of flour.\n\"\"\"\n</code></pre>\n<p>With few-shot CoT in hand, you have a powerful building block for reliable reasoning. The next challenge is making structured outputs that your application can actually parse.</p>\n<h2>Technique 3: Structured Output</h2>\n<p>Production systems need parseable output, not prose.</p>\n<p>When you build a real application around an LLM, you almost always need to extract specific fields from the response — a sentiment score, a list of issues, a priority level. The approach below uses Pydantic models to define the exact schema you expect, which both constrains the model's output and gives you Python objects you can work with directly in your code.</p>\n<pre><code class=\"language-python\">from openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import Optional\nimport json\n\nclient = OpenAI()\n\nclass ProductAnalysis(BaseModel):\n    sentiment: str          # \"positive\" | \"negative\" | \"neutral\"\n    score: float            # 0.0 to 1.0\n    key_issues: list[str]\n    recommended_action: str\n    priority: str           # \"low\" | \"medium\" | \"high\"\n\n# Method 1: JSON mode (OpenAI)\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    response_format={\"type\": \"json_object\"},\n    messages=[\n        {\"role\": \"system\", \"content\": \"\"\"You are a customer feedback analyst.\n        Always respond with valid JSON matching this schema:\n        {\n          \"sentiment\": \"positive|negative|neutral\",\n          \"score\": &#x3C;float 0-1>,\n          \"key_issues\": [&#x3C;list of strings>],\n          \"recommended_action\": &#x3C;string>,\n          \"priority\": \"low|medium|high\"\n        }\"\"\"},\n        {\"role\": \"user\", \"content\": f\"Analyze this review: {review_text}\"}\n    ]\n)\nresult = ProductAnalysis(**json.loads(response.choices[0].message.content))\n\n# Method 2: Structured outputs (OpenAI, more reliable)\nresponse = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[...],\n    response_format=ProductAnalysis,\n)\nresult: ProductAnalysis = response.choices[0].message.parsed\n</code></pre>\n<p>Method 2 (structured outputs) is preferred over Method 1 (JSON mode) when available: structured outputs use constrained decoding to guarantee the schema is satisfied at the token level, whereas JSON mode merely requests JSON and can still produce invalid or mismatched structures under edge cases.</p>\n<p><strong>Prompt patterns for structured output:</strong></p>\n<pre><code>System: You must respond ONLY with valid JSON. No explanation, no markdown, no prose.\n        The JSON must match this exact schema: {...}\n\nUser: [Your request]\n\n# Common mistakes:\n❌ \"Respond in JSON format\" — model may wrap in markdown ```json\n❌ No schema — model invents fields\n✓ Provide exact schema + \"ONLY valid JSON\" instruction\n✓ Use JSON mode or structured outputs API\n</code></pre>\n<h2>Technique 4: Self-Consistency</h2>\n<p>For high-stakes questions, generate multiple responses and take the majority vote. This reduces variance from stochastic generation.</p>\n<p>Think of self-consistency like polling multiple experts: any single expert might reason to the wrong conclusion on a hard problem, but if you ask five experts independently and four of them agree, you can be much more confident in that answer. The key is using <code>temperature > 0</code> so each sample takes a genuinely different reasoning path.</p>\n<pre><code class=\"language-python\">def self_consistent_answer(question: str, n_samples: int = 5) -> str:\n    \"\"\"Generate n answers with temperature > 0, take majority vote.\"\"\"\n\n    answers = []\n    for _ in range(n_samples):\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            temperature=0.7,  # Non-zero for diversity\n            messages=[\n                {\"role\": \"system\", \"content\": \"Solve this step by step. End with 'Answer: &#x3C;value>'\"},\n                {\"role\": \"user\", \"content\": question}\n            ]\n        )\n        text = response.choices[0].message.content\n        # Extract final answer\n        if \"Answer:\" in text:\n            answer = text.split(\"Answer:\")[-1].strip()\n            answers.append(answer)\n\n    # Majority vote\n    from collections import Counter\n    return Counter(answers).most_common(1)[0][0]\n\n# Best for: math, logic, classification (discrete answers)\n# Not useful for: creative writing, open-ended questions\n</code></pre>\n<p><strong>Accuracy improvement (GSM8K math benchmark):</strong></p>\n<ul>\n<li>GPT-4 zero-shot: 87%</li>\n<li>GPT-4 CoT: 92%</li>\n<li>GPT-4 CoT + self-consistency (n=40): 97%</li>\n</ul>\n<p>The 5-point jump from CoT to self-consistency (92% → 97%) comes entirely from running the same prompt multiple times and voting — no additional model capability required. This is a powerful lever when accuracy matters more than cost.</p>\n<h2>Technique 5: ReAct — Reasoning + Acting for Agents</h2>\n<p>ReAct interleaves <strong>Re</strong>asoning and <strong>Act</strong>ing — the model thinks, takes an action (tool call), observes the result, and continues reasoning. This is the backbone of most LLM agents.</p>\n<p>The key insight of ReAct is that you don't need to give the model all information upfront. Instead, you give it tools and let it pull in exactly the information it needs, when it needs it. The structured <code>Thought → Action → Observation</code> format keeps the reasoning transparent and makes it easy to debug where the agent went wrong.</p>\n<pre><code>System: You are an assistant that can use tools to answer questions.\nAvailable tools:\n- search(query): Search the web\n- calculate(expression): Evaluate a math expression\n- get_stock_price(ticker): Get current stock price\n\nThink in this format:\nThought: [Your reasoning]\nAction: tool_name(arguments)\nObservation: [Tool result]\n... (repeat as needed)\nFinal Answer: [Your answer]\n\nUser: What is 15% of the current price of AAPL?\n\nResponse:\nThought: I need to get the current price of AAPL first, then calculate 15% of it.\nAction: get_stock_price(AAPL)\nObservation: AAPL current price: $195.42\n\nThought: Now I'll calculate 15% of $195.42.\nAction: calculate(195.42 * 0.15)\nObservation: 29.313\n\nFinal Answer: 15% of the current AAPL price ($195.42) is approximately $29.31.\n</code></pre>\n<p>The implementation below translates this pattern into code. The <code>stop=[\"Observation:\"]</code> parameter is the key mechanism: it makes the model halt before writing the observation, so your code can inject the actual tool result rather than letting the model hallucinate one.</p>\n<pre><code class=\"language-python\"># ReAct loop implementation\ndef react_agent(user_query: str, tools: dict, max_iterations: int = 10) -> str:\n    messages = [\n        {\"role\": \"system\", \"content\": REACT_SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": user_query}\n    ]\n\n    for iteration in range(max_iterations):\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages,\n            stop=[\"Observation:\"]  # Stop before observation to inject tool result\n        )\n\n        thought_action = response.choices[0].message.content\n        messages.append({\"role\": \"assistant\", \"content\": thought_action})\n\n        if \"Final Answer:\" in thought_action:\n            return thought_action.split(\"Final Answer:\")[-1].strip()\n\n        # Parse and execute action\n        if \"Action:\" in thought_action:\n            action_line = [l for l in thought_action.split(\"\\n\") if l.startswith(\"Action:\")][0]\n            tool_name, args = parse_action(action_line)\n\n            result = tools[tool_name](args)\n            observation = f\"Observation: {result}\\n\"\n            messages.append({\"role\": \"user\", \"content\": observation})\n\n    return \"Max iterations reached\"\n</code></pre>\n<h2>Technique 6: Prompt Caching</h2>\n<p>For production systems with expensive prompts, use prompt caching to reduce latency and cost.</p>\n<p>When your system prompt contains a large document — a legal contract, a product catalog, a policy manual — you pay embedding cost for that document on every single API call. Prompt caching solves this by storing the KV cache of the processed prompt server-side, so repeated calls reuse the expensive computation instead of redoing it.</p>\n<pre><code class=\"language-python\"># Anthropic Claude: prefix caching\n# Mark expensive context (documents, instructions) for caching\nresponse = anthropic.messages.create(\n    model=\"claude-opus-4-6\",\n    max_tokens=1024,\n    system=[\n        {\n            \"type\": \"text\",\n            \"text\": \"You are a legal contract analyzer...\",\n        },\n        {\n            \"type\": \"text\",\n            \"text\": full_contract_text,  # 50,000 tokens — expensive\n            \"cache_control\": {\"type\": \"ephemeral\"}  # Cache this for 5 minutes\n        }\n    ],\n    messages=[{\"role\": \"user\", \"content\": \"What are the termination clauses?\"}]\n)\n# First call: full cost. Subsequent calls within 5 min: 90% cost reduction\n# Cached reads: $0.30/MTok vs write $3.75/MTok (Claude Sonnet)\n</code></pre>\n<p>The 90% cost reduction on cache hits makes prompt caching one of the highest-ROI optimizations for production systems that serve many users against a shared, large context. It also reduces latency since the model skips re-processing the cached prefix.</p>\n<h2>Evaluation: Measuring Prompt Quality</h2>\n<p>Building an eval suite is the step most teams skip — and then they wonder why their prompts feel unpredictable. The approach below treats prompt iteration the same way you'd treat code iteration: define expected outputs, measure actual outputs, and only ship changes that improve the metric.</p>\n<pre><code class=\"language-python\"># Build an eval dataset — sample 50-200 real examples\neval_dataset = [\n    {\n        \"input\": \"This product broke after 2 days\",\n        \"expected\": {\"sentiment\": \"negative\", \"priority\": \"high\"},\n    },\n    # ...\n]\n\ndef evaluate_prompt(prompt_template: str, dataset: list) -> dict:\n    results = []\n    for example in dataset:\n        response = run_with_prompt(prompt_template, example[\"input\"])\n        results.append({\n            \"correct\": response == example[\"expected\"],\n            \"latency_ms\": response.latency,\n            \"tokens\": response.usage.total_tokens,\n        })\n\n    return {\n        \"accuracy\": sum(r[\"correct\"] for r in results) / len(results),\n        \"avg_latency_ms\": sum(r[\"latency_ms\"] for r in results) / len(results),\n        \"avg_tokens\": sum(r[\"tokens\"] for r in results) / len(results),\n        \"cost_per_1k\": sum(r[\"tokens\"] for r in results) / 1000 * TOKEN_PRICE,\n    }\n\n# A/B test prompts before shipping to production\nbaseline = evaluate_prompt(OLD_PROMPT, eval_dataset)\ncandidate = evaluate_prompt(NEW_PROMPT, eval_dataset)\nprint(f\"Accuracy: {baseline['accuracy']:.1%} → {candidate['accuracy']:.1%}\")\n</code></pre>\n<p>The highest-leverage prompt engineering investment is building an evaluation suite. Without measurement, you cannot know if a prompt change improves quality or regresses it. Treat prompt changes like code changes — test them before deploying.</p>\n","tableOfContents":[{"id":"mental-model-llms-as-next-token-predictors","text":"Mental Model: LLMs as Next-Token Predictors","level":2},{"id":"technique-1-zero-shot-vs-few-shot","text":"Technique 1: Zero-Shot vs Few-Shot","level":2},{"id":"zero-shot","text":"Zero-Shot","level":3},{"id":"few-shot-better-for-complex-classifications","text":"Few-Shot (Better for Complex Classifications)","level":3},{"id":"technique-2-chain-of-thought-cot","text":"Technique 2: Chain-of-Thought (CoT)","level":2},{"id":"technique-3-structured-output","text":"Technique 3: Structured Output","level":2},{"id":"technique-4-self-consistency","text":"Technique 4: Self-Consistency","level":2},{"id":"technique-5-react-reasoning-acting-for-agents","text":"Technique 5: ReAct — Reasoning + Acting for Agents","level":2},{"id":"technique-6-prompt-caching","text":"Technique 6: Prompt Caching","level":2},{"id":"evaluation-measuring-prompt-quality","text":"Evaluation: Measuring Prompt Quality","level":2}]},"relatedPosts":[{"title":"Fine-Tuning LLMs: When to Fine-Tune, When to Prompt","description":"Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.","date":"2025-03-27","category":"AI/ML","tags":["ai","llm","fine-tuning","lora","hugging face","openai","machine learning"],"featured":false,"affiliateSection":"ai-ml-books","slug":"fine-tuning-llms","readingTime":"10 min read","excerpt":"Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning — fo…"},{"title":"Building AI Agents with Tool Use: From Chatbot to Autonomous Agent","description":"Build production AI agents using Claude's tool use API. Learn the agentic loop, error handling, multi-step reasoning, human-in-the-loop patterns, and how to build reliable autonomous systems.","date":"2025-03-23","category":"AI/ML","tags":["ai","agents","claude","tool use","llm","autonomous systems","python"],"featured":false,"affiliateSection":"ai-ml-books","slug":"llm-agents-tool-use","readingTime":"10 min read","excerpt":"A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio…"},{"title":"Vector Embeddings: The Foundation of Modern AI Applications","description":"Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.","date":"2025-03-11","category":"AI/ML","tags":["ai","embeddings","vector database","semantic search","rag","pgvector","pinecone"],"featured":false,"affiliateSection":"ai-ml-books","slug":"vector-embeddings-deep-dive","readingTime":"11 min read","excerpt":"Every modern AI application — semantic search, RAG, recommendations, duplicate detection — is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantic…"}]},"__N_SSG":true}