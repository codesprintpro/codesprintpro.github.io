{"pageProps":{"post":{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…","contentHtml":"<p>Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance what questions you'd ask. The three pillars are not a framework you layer on after the fact; they shape how you instrument and operate your services from the start.</p>\n<h2>The Three Pillars and How They Relate</h2>\n<pre><code>Request arrives at Service A\n\nMetrics:\n  http_requests_total{service=\"A\", status=\"200\"} += 1\n  http_request_duration_seconds{service=\"A\", p99} = 0.284s\n  → Tell you WHAT is happening (rates, latency, error rates)\n\nTraces:\n  TraceID: abc123\n  SpanID:  span001 → ServiceA.handleRequest [150ms]\n    SpanID: span002 → ServiceB.getUser [80ms]\n      SpanID: span003 → PostgreSQL.query [70ms]\n  → Tell you WHERE time is spent in a specific request\n\nLogs:\n  {\"level\":\"INFO\",\"traceId\":\"abc123\",\"spanId\":\"span002\",\n   \"message\":\"getUser called\",\"userId\":\"12345\",\"latency\":80}\n  → Tell you WHAT happened inside a specific component\n\nThree pillars used together:\nMetrics alert (P99 > 2s) → trace the slow request (traceId from logs) → find the bottleneck span\n</code></pre>\n<h2>OpenTelemetry Java SDK Setup</h2>\n<p>OpenTelemetry is the vendor-neutral standard for instrumentation. Use the Java agent for automatic instrumentation of Spring Boot:</p>\n<pre><code class=\"language-bash\"># Add Java agent to JVM startup:\n-javaagent:/opt/opentelemetry-javaagent.jar\n-Dotel.service.name=order-service\n-Dotel.service.version=2.1.0\n-Dotel.exporter.otlp.endpoint=http://otel-collector:4317\n-Dotel.traces.exporter=otlp\n-Dotel.metrics.exporter=otlp\n-Dotel.logs.exporter=otlp\n-Dotel.resource.attributes=deployment.environment=production,team=platform\n</code></pre>\n<p>The Java agent automatically instruments:</p>\n<ul>\n<li>Spring MVC (incoming HTTP spans)</li>\n<li>Spring WebFlux</li>\n<li>Hibernate/JDBC (database query spans)</li>\n<li>RestTemplate/WebClient (outgoing HTTP spans)</li>\n<li>Kafka producers/consumers</li>\n</ul>\n<p><strong>Manual instrumentation for business logic:</strong></p>\n<pre><code class=\"language-java\">@Service\npublic class OrderService {\n\n    private final Tracer tracer = GlobalOpenTelemetry.getTracer(\"order-service\");\n    private final Meter meter = GlobalOpenTelemetry.getMeter(\"order-service\");\n    private final LongCounter ordersCreated;\n\n    public OrderService() {\n        this.ordersCreated = meter.counterBuilder(\"orders.created\")\n            .setDescription(\"Total orders created\")\n            .setUnit(\"orders\")\n            .build();\n    }\n\n    public Order createOrder(OrderRequest request) {\n        Span span = tracer.spanBuilder(\"createOrder\")\n            .setAttribute(\"order.user_id\", request.getUserId())\n            .setAttribute(\"order.item_count\", request.getItems().size())\n            .startSpan();\n\n        try (Scope scope = span.makeCurrent()) {\n            validateInventory(request);    // Child span auto-created\n            chargePayment(request);        // Child span auto-created\n            Order order = orderRepository.save(Order.from(request));\n\n            span.setAttribute(\"order.id\", order.getId().toString());\n            ordersCreated.add(1,\n                Attributes.of(\n                    AttributeKey.stringKey(\"channel\"), request.getChannel(),\n                    AttributeKey.stringKey(\"payment_method\"), request.getPaymentMethod()\n                )\n            );\n            return order;\n        } catch (Exception e) {\n            span.recordException(e);\n            span.setStatus(StatusCode.ERROR, e.getMessage());\n            throw e;\n        } finally {\n            span.end();\n        }\n    }\n}\n</code></pre>\n<h2>Prometheus Metrics Design</h2>\n<p>Good metrics answer operational questions. Design metrics around user-visible behavior:</p>\n<pre><code class=\"language-java\">@Component\npublic class MetricsConfig {\n\n    @Bean\n    public MeterRegistryCustomizer&#x3C;MeterRegistry> commonTags() {\n        return registry -> registry.config()\n            .commonTags(\n                \"service\", \"order-service\",\n                \"version\", \"${spring.application.version}\",\n                \"env\", \"${spring.profiles.active}\"\n            )\n            .meterFilter(MeterFilter.deny(id ->\n                id.getName().startsWith(\"jvm.threads\") &#x26;&#x26;\n                !id.getTag(\"state\").equals(\"runnable\") // Only track runnable threads\n            ));\n    }\n}\n\n// Custom business metric: track payment failure reasons\n@Autowired\nprivate MeterRegistry registry;\n\npublic void recordPaymentResult(String outcome, String reason) {\n    registry.counter(\"payments.processed\",\n        \"outcome\", outcome,        // success, failed, declined\n        \"reason\", reason,          // insufficient_funds, expired_card, fraud_hold\n        \"gateway\", \"stripe\"\n    ).increment();\n}\n</code></pre>\n<p><strong>The RED method for services:</strong></p>\n<pre><code class=\"language-yaml\"># Prometheus recording rules (pre-compute expensive queries):\ngroups:\n  - name: service_red_metrics\n    rules:\n      - record: job:http_requests:rate5m\n        expr: rate(http_server_requests_seconds_count[5m])\n\n      - record: job:http_request_errors:rate5m\n        expr: rate(http_server_requests_seconds_count{status=~\"5..\"}[5m])\n\n      - record: job:http_error_ratio:rate5m\n        expr: job:http_request_errors:rate5m / job:http_requests:rate5m\n\n      - record: job:http_request_p99:rate5m\n        expr: histogram_quantile(0.99, rate(http_server_requests_seconds_bucket[5m]))\n</code></pre>\n<h2>Structured Logging with Loki</h2>\n<p>Loki stores logs as compressed log streams (label-indexed). Unlike Elasticsearch, Loki doesn't index log content — it indexes labels. This makes it fast and cheap.</p>\n<pre><code class=\"language-java\">// Spring Boot structured logging with Logback:\n// logback-spring.xml:\n&#x3C;configuration>\n  &#x3C;appender name=\"JSON\" class=\"ch.qos.logback.core.ConsoleAppender\">\n    &#x3C;encoder class=\"net.logstash.logback.encoder.LogstashEncoder\">\n      &#x3C;provider class=\"net.logstash.logback.composite.loggingevent.LoggingEventPatternJsonProvider\">\n        &#x3C;pattern>{\"timestamp\":\"%d{ISO8601}\",\"level\":\"%level\",\"logger\":\"%logger\",\"message\":\"%msg\"}&#x3C;/pattern>\n      &#x3C;/provider>\n      &#x3C;!-- Include MDC fields (traceId, spanId injected by OTel agent): -->\n      &#x3C;includeMdcKeyName>traceId&#x3C;/includeMdcKeyName>\n      &#x3C;includeMdcKeyName>spanId&#x3C;/includeMdcKeyName>\n      &#x3C;includeMdcKeyName>userId&#x3C;/includeMdcKeyName>\n    &#x3C;/encoder>\n  &#x3C;/appender>\n  &#x3C;root level=\"INFO\">\n    &#x3C;appender-ref ref=\"JSON\"/>\n  &#x3C;/root>\n&#x3C;/configuration>\n</code></pre>\n<p><strong>Loki query examples (LogQL):</strong></p>\n<pre><code class=\"language-logql\"># Error rate for order service:\nrate({service=\"order-service\", level=\"ERROR\"}[5m])\n\n# Slow requests (> 1s) in last 15 minutes:\n{service=\"order-service\"} |= \"latency\" | json | latencyMs > 1000\n\n# Correlate trace with logs:\n{service=\"order-service\"} |= \"abc123\"  # Find all logs for traceId abc123\n\n# Log volume by endpoint:\nsum by (path) (rate({service=\"order-service\"}[5m]))\n</code></pre>\n<h2>Grafana Dashboard: The Incident Response Dashboard</h2>\n<p>Every service should have one dashboard that tells you in 30 seconds whether the service is healthy:</p>\n<pre><code>┌─────────────────────────────────────────────────────────────────┐\n│  ORDER SERVICE — Production Dashboard                           │\n├──────────────┬──────────────┬──────────────┬────────────────────┤\n│  RPS         │  Error Rate  │  P99 Latency │  DB Connections    │\n│  1,247/s     │  0.12%       │  284ms       │  18/50             │\n│  ▼ -3%       │  ▲ normal    │  ▼ healthy   │  ▲ OK              │\n├──────────────┴──────────────┴──────────────┴────────────────────┤\n│  Request Rate (5m)     │  Error Rate (5m)                       │\n│  [graph]               │  [graph]                               │\n├────────────────────────┴───────────────────────────────────────┤\n│  Latency Distribution (P50/P95/P99)    │  Top Slow Endpoints    │\n│  [graph]                               │  [table]               │\n├────────────────────────────────────────┴───────────────────────┤\n│  Recent Errors (Loki logs, last 50)                             │\n│  [log panel — live tail during incidents]                       │\n└─────────────────────────────────────────────────────────────────┘\n</code></pre>\n<pre><code class=\"language-json\">// Grafana panel JSON for error rate alert indicator:\n{\n  \"type\": \"stat\",\n  \"title\": \"Error Rate\",\n  \"targets\": [{\n    \"expr\": \"sum(rate(http_server_requests_seconds_count{service=\\\"order-service\\\",status=~\\\"5..\\\"}[5m])) / sum(rate(http_server_requests_seconds_count{service=\\\"order-service\\\"}[5m]))\",\n    \"legendFormat\": \"Error %\"\n  }],\n  \"options\": {\n    \"colorMode\": \"background\",\n    \"thresholds\": {\n      \"steps\": [\n        {\"value\": 0, \"color\": \"green\"},\n        {\"value\": 0.01, \"color\": \"yellow\"},\n        {\"value\": 0.05, \"color\": \"red\"}\n      ]\n    }\n  }\n}\n</code></pre>\n<h2>Alerting That Doesn't Create Alert Fatigue</h2>\n<p>Alert on symptoms, not causes:</p>\n<pre><code class=\"language-yaml\"># Prometheus alerting rules:\ngroups:\n  - name: service_slos\n    rules:\n      # Alert on user-visible symptoms:\n      - alert: HighErrorRate\n        expr: job:http_error_ratio:rate5m{service=\"order-service\"} > 0.05\n        for: 2m\n        labels:\n          severity: critical\n          team: backend\n        annotations:\n          summary: \"Order service error rate {{ $value | humanizePercentage }}\"\n          runbook: \"https://wiki/runbooks/order-service-errors\"\n\n      - alert: HighP99Latency\n        expr: job:http_request_p99:rate5m{service=\"order-service\"} > 2\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Order service P99 latency is {{ $value }}s\"\n          dashboard: \"https://grafana/d/order-service\"\n\n      # Infrastructure alerts (lower priority):\n      - alert: HighDatabaseConnections\n        expr: hikaricp_connections_active / hikaricp_connections_max > 0.9\n        for: 3m\n        labels:\n          severity: warning\n</code></pre>\n<p><strong>What NOT to alert on:</strong></p>\n<ul>\n<li>CPU utilization (symptom: latency or errors, not CPU itself)</li>\n<li>Memory usage below limit (only alert near OOM)</li>\n<li>Individual host metrics (alert on aggregate service behavior)</li>\n</ul>\n<h2>Distributed Trace Analysis During Incidents</h2>\n<p>When P99 latency is high, use Jaeger/Tempo to find the slow spans:</p>\n<pre><code>Incident investigation workflow:\n\n1. Grafana alerts: P99 > 2s on order-service\n2. Open Tempo trace search:\n   service=order-service, duration>2000ms, last 15 minutes\n3. Sort by duration, examine top 5 slow traces\n4. Identify common slow span:\n   → PostgreSQL.query on orders table: 1.8s\n   → SELECT * FROM orders WHERE user_id=? ORDER BY created_at DESC\n5. Open query in pg_stat_statements:\n   → Missing index on (user_id, created_at)\n6. Create index CONCURRENTLY (online, no table lock)\n7. P99 drops to 180ms within 2 minutes\n\nTotal time from alert to fix: 15 minutes.\nWithout traces: hours of grep-based log analysis.\n</code></pre>\n<h2>Sampling Strategy</h2>\n<p>100% trace sampling at high throughput is expensive. Use tail-based sampling:</p>\n<pre><code class=\"language-yaml\"># OpenTelemetry Collector config with tail-based sampling:\nprocessors:\n  tail_sampling:\n    decision_wait: 10s\n    num_traces: 100000\n    expected_new_traces_per_sec: 10000\n    policies:\n      - name: errors-policy\n        type: status_code\n        status_code: {status_codes: [ERROR]}\n        # Always sample errors\n      - name: slow-traces-policy\n        type: latency\n        latency: {threshold_ms: 1000}\n        # Sample traces > 1 second\n      - name: probabilistic-policy\n        type: probabilistic\n        probabilistic: {sampling_percentage: 1}\n        # Sample 1% of everything else\n</code></pre>\n<p>This captures 100% of errors and slow requests (the ones you care about) while sampling only 1% of healthy fast requests (the ones you don't need to analyze).</p>\n<p>Observability is an investment with compounding returns. Every hour spent on instrumentation now saves 10 hours of incident investigation later. The teams that build it in from day one navigate incidents in minutes. The teams that add it after a major outage spend the outage blind.</p>\n","tableOfContents":[{"id":"the-three-pillars-and-how-they-relate","text":"The Three Pillars and How They Relate","level":2},{"id":"opentelemetry-java-sdk-setup","text":"OpenTelemetry Java SDK Setup","level":2},{"id":"prometheus-metrics-design","text":"Prometheus Metrics Design","level":2},{"id":"structured-logging-with-loki","text":"Structured Logging with Loki","level":2},{"id":"grafana-dashboard-the-incident-response-dashboard","text":"Grafana Dashboard: The Incident Response Dashboard","level":2},{"id":"alerting-that-doesnt-create-alert-fatigue","text":"Alerting That Doesn't Create Alert Fatigue","level":2},{"id":"distributed-trace-analysis-during-incidents","text":"Distributed Trace Analysis During Incidents","level":2},{"id":"sampling-strategy","text":"Sampling Strategy","level":2}]},"relatedPosts":[{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"},{"title":"Multi-Tenancy Architecture: Database, Application, and Infrastructure Patterns","description":"Production multi-tenancy: database isolation models (shared schema, shared database, separate database), tenant routing, data partitioning strategies, cross-tenant query prevention, Spring Boot tenant context propagation, and the trade-offs at each isolation level.","date":"2025-05-24","category":"System Design","tags":["multi-tenancy","saas","system design","database","spring boot","architecture","isolation"],"featured":false,"affiliateSection":"system-design-courses","slug":"multi-tenancy-architecture","readingTime":"8 min read","excerpt":"Multi-tenancy is the architecture pattern where a single deployed instance of a software system serves multiple customers (tenants), with each tenant's data logically or physically isolated from others. It's the foundati…"}]},"__N_SSG":true}