{"pageProps":{"post":{"title":"System Design: Real-Time Chat Application at Scale","description":"Design a real-time chat system like WhatsApp or Slack handling 1 billion messages per day. Covers WebSocket connection management, message delivery guarantees, presence detection, and storage.","date":"2025-03-17","category":"System Design","tags":["system design","websocket","real-time","kafka","redis","cassandra","distributed systems"],"featured":false,"affiliateSection":"system-design-courses","slug":"system-design-real-time-chat","readingTime":"11 min read","excerpt":"Real-time chat systems are among the most architecturally interesting distributed systems. They require persistent connections at massive scale, exactly-once message delivery guarantees, presence detection across million…","contentHtml":"<p>Real-time chat systems are among the most architecturally interesting distributed systems. They require persistent connections at massive scale, exactly-once message delivery guarantees, presence detection across millions of users, and message history queries that span years. This article designs a system comparable to WhatsApp or Slack.</p>\n<h2>Requirements</h2>\n<p>Before designing anything, you need to quantify the problem precisely. Requirements drive every architectural decision that follows — the choice of database, the number of Kafka partitions, the size of the connection pool. Notice that the numbers below aren't arbitrary; each one is derived from real-world usage patterns and drives specific design choices.</p>\n<p><strong>Functional:</strong></p>\n<ul>\n<li>1:1 messaging and group chats (up to 1000 members)</li>\n<li>Real-time delivery (&#x3C; 100ms end-to-end for online users)</li>\n<li>Message delivery receipts (sent → delivered → read)</li>\n<li>Message history (searchable, up to 5 years)</li>\n<li>User presence (online/offline, last seen)</li>\n<li>Message types: text, images, files, reactions</li>\n</ul>\n<p><strong>Non-Functional:</strong></p>\n<ul>\n<li>500M daily active users, 1B messages/day (~11,600 msg/sec average, 3x peak = 35,000/sec)</li>\n<li>Message delivery guarantee: at-least-once (deduplication client-side)</li>\n<li>Message ordering: within a conversation, strict order</li>\n<li>Storage: ~100 bytes/message × 1B/day × 365 days × 5 years = 180TB</li>\n</ul>\n<h2>High-Level Architecture</h2>\n<p>The architecture is driven by one fundamental constraint: with 500M daily active users and real-time delivery requirements, you cannot use traditional request-response HTTP. Each online user needs a persistent, low-latency connection. The diagram below shows how the system is organized around this constraint.</p>\n<pre><code>Mobile/Web Client\n    │\n    │ WebSocket (persistent connection)\n    ▼\n┌──────────────────────────────────────────────────────────┐\n│                   WebSocket Gateway Cluster              │\n│   (stateful: each server holds N open WebSocket conns)  │\n│   ws-1: [user_A, user_B, user_C, ...]                    │\n│   ws-2: [user_D, user_E, user_F, ...]                    │\n└────────────────────┬─────────────────────────────────────┘\n                     │ Publish messages to Kafka\n                     ▼\n┌──────────────────────────────────────────────────────────┐\n│          Kafka (message bus, ordered per partition)      │\n│   Topic: chat-messages, partitioned by conversation_id   │\n└────────────────────┬─────────────────────────────────────┘\n                     │\n         ┌───────────┼───────────────┐\n         ▼           ▼               ▼\n   Message Service  Storage Service  Notification Service\n   (delivery,       (Cassandra)      (APNs, FCM for\n    routing)                          offline users)\n\nRedis: User → WebSocket server mapping (presence registry)\n</code></pre>\n<p>The WebSocket gateways are stateful — each server maintains thousands of open connections in memory. This statefulness creates the core routing challenge: when User A sends to User B, the system must find which server holds User B's connection. Redis serves as the global directory that maps user IDs to server IDs.</p>\n<h2>WebSocket Connection Management</h2>\n<p>The key challenge: when User A sends to User B, which WebSocket server holds User B's connection?</p>\n<p>Your WebSocket gateway is like an airport's gate assignment system. Each gate (server) handles a set of flights (connections), and the central directory (Redis) tells everyone which gate a given flight is at. When a new connection lands, you register it. When it leaves, you deregister it. The code below handles the full connection lifecycle.</p>\n<pre><code class=\"language-java\">// WebSocket Gateway — each server handles 50,000-100,000 persistent connections\n@Component\npublic class ChatWebSocketHandler extends TextWebSocketHandler {\n\n    @Autowired\n    private UserConnectionRegistry connectionRegistry;\n\n    @Autowired\n    private KafkaTemplate&#x3C;String, ChatMessage> kafkaTemplate;\n\n    // In-process connection map: userId → WebSocket session\n    private final ConcurrentHashMap&#x3C;String, WebSocketSession> localConnections =\n        new ConcurrentHashMap&#x3C;>();\n\n    @Override\n    public void afterConnectionEstablished(WebSocketSession session) {\n        String userId = extractUserId(session);  // From JWT in handshake header\n\n        localConnections.put(userId, session);\n\n        // Register in Redis: userId → this server's ID\n        connectionRegistry.register(userId, getServerId());\n\n        // Send buffered messages (messages received while user was offline)\n        sendOfflineMessages(userId, session);\n    }\n\n    @Override\n    protected void handleTextMessage(WebSocketSession session, TextMessage message) {\n        String userId = extractUserId(session);\n        ChatMessage chatMessage = parseMessage(message, userId);\n\n        // Publish to Kafka (partitioned by conversationId for ordering)\n        kafkaTemplate.send(\n            \"chat-messages\",\n            chatMessage.getConversationId(),  // partition key — ensures ordering\n        chatMessage\n        );\n\n        // Ack to sender immediately\n        sendAck(session, chatMessage.getClientMessageId());\n    }\n\n    @Override\n    public void afterConnectionClosed(WebSocketSession session, CloseStatus status) {\n        String userId = extractUserId(session);\n        localConnections.remove(userId);\n        connectionRegistry.unregister(userId);\n        updateLastSeen(userId);\n    }\n\n    // Deliver message to a local user (called by Message Delivery Service)\n    public boolean deliverLocally(String userId, ChatMessage message) {\n        WebSocketSession session = localConnections.get(userId);\n        if (session == null || !session.isOpen()) return false;\n\n        try {\n            session.sendMessage(new TextMessage(serialize(message)));\n            return true;\n        } catch (IOException e) {\n            return false;\n        }\n    }\n}\n</code></pre>\n<p>Notice that when a message arrives from a client, it's immediately published to Kafka rather than routed directly to the recipient. This decoupling is intentional — Kafka provides durability (the message is persisted even if the delivery service crashes), ordering (messages in the same conversation stay in order), and fan-out (multiple consumers like storage and notification services process the same event).</p>\n<h2>Message Delivery Service</h2>\n<p>With messages flowing through Kafka, the delivery service consumes them and routes each one to the right WebSocket server. This service handles the three cases that arise in any real deployment: the recipient is online on this server, on a different server, or offline entirely.</p>\n<pre><code class=\"language-java\">// Kafka consumer: route messages to the right WebSocket server\n@Service\npublic class MessageDeliveryService {\n\n    @Autowired\n    private UserConnectionRegistry connectionRegistry;  // Redis\n\n    @Autowired\n    private Map&#x3C;String, ChatWebSocketHandler> wsHandlers;  // Local + remote stubs\n\n    @Autowired\n    private OfflineMessageStore offlineStore;\n\n    @KafkaListener(\n        topics = \"chat-messages\",\n        concurrency = \"12\"  // One thread per Kafka partition\n    )\n    public void deliver(ChatMessage message) {\n        List&#x3C;String> recipients = getRecipients(message);  // From conversation members\n\n        for (String recipientId : recipients) {\n            String serverIdForRecipient = connectionRegistry.getServer(recipientId);\n\n            if (serverIdForRecipient == null) {\n                // User is offline: store for later delivery + send push notification\n                offlineStore.store(recipientId, message);\n                pushNotificationService.send(recipientId, message);\n            } else if (serverIdForRecipient.equals(getServerId())) {\n                // User is on THIS server: deliver directly\n                boolean delivered = localWsHandler.deliverLocally(recipientId, message);\n                if (!delivered) {\n                    // Session closed between check and delivery\n                    offlineStore.store(recipientId, message);\n                }\n            } else {\n                // User is on a DIFFERENT server: route via internal HTTP/gRPC\n                deliverToServer(serverIdForRecipient, recipientId, message);\n            }\n        }\n\n        // Store in Cassandra (message history)\n        messageStore.save(message);\n\n        // Update delivery status\n        updateDeliveryStatus(message.getId(), recipients, DeliveryStatus.DELIVERED);\n    }\n}\n</code></pre>\n<p>The <code>concurrency = \"12\"</code> setting means 12 threads consume from 12 Kafka partitions in parallel — matching consumer threads to partitions is the correct way to maximize Kafka throughput. The fallback to offline storage when a session has closed between the Redis check and the delivery attempt is an important edge case — without it, messages would silently drop during the brief window of a connection closing.</p>\n<h2>User Presence with Redis</h2>\n<p>Presence detection is the feature users take for granted but which requires careful engineering. The approach below uses Redis TTL as the mechanism — a heartbeat keeps the key alive, and natural expiry signals the user has gone offline. This is simpler and more scalable than maintaining a connection registry with explicit disconnect events, which can be missed during network failures.</p>\n<pre><code class=\"language-java\">@Service\npublic class PresenceService {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    private static final String ONLINE_KEY_PREFIX = \"presence:online:\";\n    private static final Duration ONLINE_TTL = Duration.ofSeconds(30);\n\n    // Called by WebSocket heartbeat every 20 seconds\n    public void heartbeat(String userId) {\n        redis.opsForValue().set(\n            ONLINE_KEY_PREFIX + userId,\n            Instant.now().toString(),\n            ONLINE_TTL\n        );\n    }\n\n    public boolean isOnline(String userId) {\n        return redis.hasKey(ONLINE_KEY_PREFIX + userId);\n    }\n\n    // Batch check: are these 100 users online?\n    public Map&#x3C;String, Boolean> getBulkPresence(List&#x3C;String> userIds) {\n        List&#x3C;String> keys = userIds.stream()\n            .map(id -> ONLINE_KEY_PREFIX + id).toList();\n\n        List&#x3C;String> values = redis.opsForValue().multiGet(keys);\n\n        Map&#x3C;String, Boolean> result = new HashMap&#x3C;>();\n        for (int i = 0; i &#x3C; userIds.size(); i++) {\n            result.put(userIds.get(i), values.get(i) != null);\n        }\n        return result;\n    }\n\n    // Last seen: stored in Redis when user goes offline\n    public void markOffline(String userId) {\n        redis.delete(ONLINE_KEY_PREFIX + userId);\n        redis.opsForValue().set(\n            \"presence:lastseen:\" + userId,\n            Instant.now().toString()\n        );\n    }\n}\n</code></pre>\n<p>The <code>getBulkPresence</code> method using <code>multiGet</code> is a critical optimization for group chats — instead of making 100 Redis round trips to check 100 members' presence, you make one. In a 1000-member group chat, the difference between single and batch presence checks is the difference between acceptable and unusable latency.</p>\n<h2>Message Storage: Cassandra</h2>\n<p>Messages require high write throughput and time-ordered reads per conversation.</p>\n<p>Relational databases aren't a good fit for chat message storage at this scale. The access pattern is extremely predictable — you always fetch the latest N messages for a specific conversation — and you need write throughput far beyond what a single PostgreSQL instance can handle. Cassandra's data model is designed exactly for this: partition by a natural key (conversation ID + time bucket) and cluster by time within each partition.</p>\n<pre><code class=\"language-sql\">-- Cassandra schema (optimized for \"get last 50 messages in conversation X\")\nCREATE TABLE messages (\n    conversation_id  UUID,\n    bucket           INT,          -- Time bucket (year-month): limits partition size\n    message_id       TIMEUUID,     -- TimeUUID: unique + ordered by time\n    sender_id        UUID,\n    content          TEXT,\n    message_type     TEXT,         -- text, image, file, reaction\n    metadata         MAP&#x3C;TEXT, TEXT>,\n    deleted_at       TIMESTAMP,    -- Soft delete\n\n    PRIMARY KEY ((conversation_id, bucket), message_id)\n) WITH CLUSTERING ORDER BY (message_id DESC)\n  AND compaction = {'class': 'TimeWindowCompactionStrategy',\n                    'compaction_window_unit': 'DAYS',\n                    'compaction_window_size': 7};\n\n-- Query: latest 50 messages in conversation\nSELECT * FROM messages\nWHERE conversation_id = ? AND bucket = ?\nORDER BY message_id DESC\nLIMIT 50;\n\n-- Bucket calculation: ensures no partition grows unbounded\n-- bucket = year * 100 + month (e.g., 202502 for Feb 2025)\n-- Active conversations: query current bucket + previous if needed\n</code></pre>\n<p>The <code>bucket</code> column is the key design insight here — without it, a very active conversation could accumulate millions of rows in a single Cassandra partition, which degrades read and compaction performance. By bucketing per month, you cap each partition at roughly one month's worth of messages per conversation. <code>TimeWindowCompactionStrategy</code> then efficiently compacts data within time windows, which matches the write pattern perfectly.</p>\n<h2>Message Deduplication</h2>\n<p>At-least-once delivery means duplicates are possible. Handle client-side:</p>\n<p>Because the system guarantees at-least-once delivery (not exactly-once), the same message can arrive at a client more than once — for example, during a network reconnect where the client re-requests messages from the last received position. The deduplication service below prevents duplicates from appearing in the UI by tracking which client-generated IDs have already been processed.</p>\n<pre><code class=\"language-java\">// Each message has a client-generated idempotency key\n// Client generates: clientMessageId = UUID.randomUUID()\n// Server stores: (conversationId, clientMessageId) → messageId\n\n@Service\npublic class MessageDeduplicationService {\n\n    @Autowired\n    private RedisTemplate&#x3C;String, String> redis;\n\n    private static final Duration DEDUP_TTL = Duration.ofHours(24);\n\n    public Optional&#x3C;String> isDuplicate(String conversationId, String clientMessageId) {\n        String key = \"dedup:\" + conversationId + \":\" + clientMessageId;\n        String existingMessageId = redis.opsForValue().get(key);\n        return Optional.ofNullable(existingMessageId);\n    }\n\n    public void markProcessed(String conversationId, String clientMessageId, String messageId) {\n        String key = \"dedup:\" + conversationId + \":\" + clientMessageId;\n        redis.opsForValue().set(key, messageId, DEDUP_TTL);\n    }\n}\n</code></pre>\n<p>The 24-hour TTL is a deliberate trade-off — it covers the window in which a duplicate is likely to arrive (network retries, reconnects), while preventing unlimited Redis growth. Messages older than 24 hours will simply be re-stored if re-delivered, which is acceptable because the client can deduplicate by <code>message_id</code> in its local database.</p>\n<h2>Scalability Analysis</h2>\n<p>With the components understood, here's how each one scales to meet the requirements. These numbers give you a concrete target for capacity planning.</p>\n<pre><code>Component         Scale           Technology\n─────────────────────────────────────────────────────────\nWebSocket GW      50K conn/server  Netty/Spring WebFlux\n                  → 10K servers for 500M users\n                  → Use consistent hashing to route users\n\nKafka             35K msg/sec peak  30 partitions (1K/partition)\n                  Retention: 24h for real-time delivery\n\nCassandra         35K writes/sec    10 nodes, RF=3\n                  100TB/year        Use TTL for 5-year retention\n\nRedis (presence)  500M keys         Redis Cluster, 6 shards\n                  30s TTL → natural expiry\n\nMessage routing   gRPC between GW   P2P mesh for intra-cluster\n                  servers           pub/sub for cross-cluster\n\nCDN               Images/files      S3 + CloudFront\n                  Pre-signed URLs   Direct upload from client\n</code></pre>\n<h2>Guarantees and Trade-offs</h2>\n<p>Every distributed system design involves deliberate trade-offs. The choices below reflect the reality that chat applications prioritize availability — users should always be able to send messages, even under network partitions — over strict consistency guarantees that would require coordination overhead.</p>\n<pre><code>Message ordering:\n  Within a conversation: guaranteed (Kafka partitioned by conversationId,\n                                     Cassandra TIMEUUID ordering)\n  Across conversations: best-effort\n\nDelivery guarantee:\n  Online users: at-least-once (deduplicated client-side)\n  Offline users: at-least-once (stored + retried)\n  No guarantee of exactly-once end-to-end (intentional trade-off for performance)\n\nConsistency:\n  Message storage: eventual (Cassandra RF=3, quorum reads)\n  Delivery status: eventual (Redis, replicated)\n  Group membership: strong (PostgreSQL for group metadata)\n\nCAP theorem position: AP (availability + partition tolerance)\n  During network partition: accept writes (store in Kafka),\n  deliver when partition heals → may deliver out of order\n  Trade-off: always available vs always consistent\n</code></pre>\n<p>The hardest part of chat system design isn't the message passing — it's the edge cases. What happens when a user is on 3 devices? (deliver to all) What if a conversation has 1000 members and all are online? (fan-out at scale requires message fan-out service) What if a server crashes mid-delivery? (Kafka offset management + at-least-once). Design the happy path first, then systematically find failure modes.</p>\n","tableOfContents":[{"id":"requirements","text":"Requirements","level":2},{"id":"high-level-architecture","text":"High-Level Architecture","level":2},{"id":"websocket-connection-management","text":"WebSocket Connection Management","level":2},{"id":"message-delivery-service","text":"Message Delivery Service","level":2},{"id":"user-presence-with-redis","text":"User Presence with Redis","level":2},{"id":"message-storage-cassandra","text":"Message Storage: Cassandra","level":2},{"id":"message-deduplication","text":"Message Deduplication","level":2},{"id":"scalability-analysis","text":"Scalability Analysis","level":2},{"id":"guarantees-and-trade-offs","text":"Guarantees and Trade-offs","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}