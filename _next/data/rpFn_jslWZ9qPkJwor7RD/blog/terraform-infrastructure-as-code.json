{"pageProps":{"post":{"title":"Terraform Infrastructure as Code: Production Patterns and Pitfalls","description":"Production Terraform: module design, state management with S3 and DynamoDB locking, workspace strategies for multi-environment deployments, sensitive variable handling, drift detection, and the Terraform anti-patterns that cause outages.","date":"2025-05-14","category":"AWS","tags":["terraform","infrastructure as code","aws","devops","s3","modules","ci/cd"],"featured":false,"affiliateSection":"aws-resources","slug":"terraform-infrastructure-as-code","readingTime":"7 min read","excerpt":"Terraform is the industry-standard tool for Infrastructure as Code (IaC) — defining cloud infrastructure as declarative HCL configuration that can be version-controlled, reviewed, and applied reproducibly. The value prop…","contentHtml":"<p>Terraform is the industry-standard tool for Infrastructure as Code (IaC) — defining cloud infrastructure as declarative HCL configuration that can be version-controlled, reviewed, and applied reproducibly. The value proposition is real: no more manual console clicks, no more \"works in staging but not production\" configurations, no more knowledge silos about how infrastructure is set up. The pitfalls are equally real: state corruption, accidental resource deletion, and modules that become maintenance nightmares.</p>\n<h2>Project Structure for Production</h2>\n<pre><code>infrastructure/\n├── modules/              # Reusable modules\n│   ├── vpc/\n│   │   ├── main.tf\n│   │   ├── variables.tf\n│   │   └── outputs.tf\n│   ├── eks-cluster/\n│   ├── rds-postgres/\n│   └── ecs-service/\n├── environments/         # Environment-specific configurations\n│   ├── prod/\n│   │   ├── main.tf       # Uses modules with prod-specific values\n│   │   ├── variables.tf\n│   │   └── backend.tf    # Points to prod state file\n│   ├── staging/\n│   └── dev/\n└── global/               # Cross-environment resources (Route53, IAM)\n    ├── main.tf\n    └── backend.tf\n</code></pre>\n<p>This structure separates reusable infrastructure patterns (modules) from environment-specific configuration (environments). Each environment is an independent Terraform root module with its own state file.</p>\n<h2>Remote State with S3 and DynamoDB Locking</h2>\n<p>Local state (<code>terraform.tfstate</code>) is never acceptable in a team environment. Remote state in S3 with DynamoDB locking is the standard AWS pattern:</p>\n<pre><code class=\"language-hcl\"># bootstrap/main.tf — create state infrastructure first (bootstrapping)\nresource \"aws_s3_bucket\" \"terraform_state\" {\n  bucket = \"company-terraform-state-${data.aws_caller_identity.current.account_id}\"\n  # Prevent accidental deletion:\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\nresource \"aws_s3_bucket_versioning\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  versioning_configuration {\n    status = \"Enabled\"  # Every state change is versioned — rollback possible\n  }\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"aws:kms\"  # Encrypt state at rest\n    }\n  }\n}\n\nresource \"aws_dynamodb_table\" \"terraform_locks\" {\n  name         = \"terraform-state-locks\"\n  billing_mode = \"PAY_PER_REQUEST\"\n  hash_key     = \"LockID\"\n\n  attribute {\n    name = \"LockID\"\n    type = \"S\"\n  }\n}\n\n# environments/prod/backend.tf:\nterraform {\n  backend \"s3\" {\n    bucket         = \"company-terraform-state-123456789012\"\n    key            = \"prod/terraform.tfstate\"   # Unique key per environment\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-state-locks\"   # DynamoDB for locking\n  }\n}\n</code></pre>\n<p>DynamoDB locking prevents two engineers from running <code>terraform apply</code> simultaneously — concurrent applies on the same state cause corruption. One apply blocks while the other holds the lock.</p>\n<h2>Module Design Patterns</h2>\n<pre><code class=\"language-hcl\"># modules/rds-postgres/main.tf — reusable RDS module:\nvariable \"identifier\" {\n  description = \"Database instance identifier\"\n  type        = string\n}\n\nvariable \"instance_class\" {\n  description = \"RDS instance type\"\n  type        = string\n  default     = \"db.t3.medium\"\n}\n\nvariable \"allocated_storage_gb\" {\n  type    = number\n  default = 20\n}\n\nvariable \"environment\" {\n  description = \"Environment name (used for tagging)\"\n  type        = string\n  validation {\n    condition     = contains([\"prod\", \"staging\", \"dev\"], var.environment)\n    error_message = \"environment must be prod, staging, or dev\"\n  }\n}\n\nvariable \"database_password\" {\n  description = \"Master database password\"\n  type        = string\n  sensitive   = true  # Never printed in terraform output\n}\n\nresource \"aws_db_instance\" \"this\" {\n  identifier             = var.identifier\n  engine                 = \"postgres\"\n  engine_version         = \"15.4\"\n  instance_class         = var.instance_class\n  allocated_storage      = var.allocated_storage_gb\n  storage_type           = \"gp3\"\n  storage_encrypted      = true\n\n  db_name  = \"appdb\"\n  username = \"admin\"\n  password = var.database_password\n\n  # Prod: Multi-AZ for high availability; dev: single-AZ for cost\n  multi_az = var.environment == \"prod\"\n\n  # Prod: 7-day backups; dev: 1 day\n  backup_retention_period = var.environment == \"prod\" ? 7 : 1\n\n  # Prevent accidental deletion in production:\n  deletion_protection = var.environment == \"prod\"\n  skip_final_snapshot = var.environment != \"prod\"\n  final_snapshot_identifier = var.environment == \"prod\" ? \"${var.identifier}-final\" : null\n\n  vpc_security_group_ids = [aws_security_group.rds.id]\n  db_subnet_group_name   = aws_db_subnet_group.this.name\n\n  tags = {\n    Environment = var.environment\n    ManagedBy   = \"terraform\"\n    Module      = \"rds-postgres\"\n  }\n}\n\noutput \"endpoint\" {\n  description = \"RDS connection endpoint\"\n  value       = aws_db_instance.this.endpoint\n}\n\noutput \"port\" {\n  value = aws_db_instance.this.port\n}\n\n# environments/prod/main.tf — using the module:\nmodule \"orders_db\" {\n  source = \"../../modules/rds-postgres\"\n\n  identifier           = \"orders-prod\"\n  instance_class       = \"db.r6g.large\"\n  allocated_storage_gb = 100\n  environment          = \"prod\"\n  database_password    = var.db_password  # From secrets manager or env var\n}\n\n# Reference module output:\noutput \"orders_db_endpoint\" {\n  value     = module.orders_db.endpoint\n  sensitive = false\n}\n</code></pre>\n<h2>Sensitive Variables: Never Hardcode Secrets</h2>\n<pre><code class=\"language-hcl\"># WRONG: Secret in plaintext\nvariable \"db_password\" {\n  default = \"supersecretpassword123\"  # Committed to git — security incident\n}\n\n# WRONG: Sensitive value in terraform.tfvars committed to git\ndb_password = \"supersecretpassword123\"\n\n# RIGHT: Read from AWS Secrets Manager:\ndata \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = \"prod/orders-db/password\"\n}\n\nlocals {\n  db_password = jsondecode(data.aws_secretsmanager_secret_version.db_password.secret_string)[\"password\"]\n}\n\nmodule \"orders_db\" {\n  source            = \"../../modules/rds-postgres\"\n  database_password = local.db_password  # Read from Secrets Manager, never hardcoded\n}\n\n# RIGHT: Pass via environment variable (CI/CD):\n# TF_VAR_db_password=... terraform apply\n# GitHub Actions secret → TF_VAR_db_password environment variable\n</code></pre>\n<p><strong>Mark sensitive outputs:</strong></p>\n<pre><code class=\"language-hcl\">output \"db_endpoint\" {\n  value     = module.orders_db.endpoint\n  sensitive = false  # Endpoint is not sensitive (you know it from Route53)\n}\n\noutput \"db_password\" {\n  value     = module.orders_db.password\n  sensitive = true   # Never printed in plan/apply output\n}\n</code></pre>\n<h2>Workspaces vs. Separate State Files</h2>\n<p>Terraform workspaces create separate state files within the same backend. They sound like the right tool for multiple environments, but they have a critical limitation: all workspace configs share the same <code>.tf</code> files. This prevents environment-specific configuration like <code>instance_class = \"db.r6g.large\"</code> in prod and <code>instance_class = \"db.t3.small\"</code> in dev (unless you use many conditionals on <code>terraform.workspace</code>).</p>\n<p><strong>Recommendation:</strong> Separate directories (as shown in the project structure above) are cleaner than workspaces for environment isolation. Each environment has its own <code>main.tf</code> with explicit values. Workspaces work well for feature branches or PR preview environments where the configuration is identical except for the state.</p>\n<h2>CI/CD Pipeline Integration</h2>\n<pre><code class=\"language-yaml\"># .github/workflows/terraform.yml:\nname: Terraform\n\non:\n  pull_request:\n    paths: ['infrastructure/**']\n  push:\n    branches: [main]\n    paths: ['infrastructure/**']\n\njobs:\n  plan:\n    name: Terraform Plan\n    runs-on: ubuntu-latest\n    if: github.event_name == 'pull_request'\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: ${{ secrets.AWS_TERRAFORM_ROLE_ARN }}  # OIDC, no keys\n          aws-region: us-east-1\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: \"1.7.0\"\n\n      - name: Terraform Init\n        run: terraform init\n        working-directory: infrastructure/environments/prod\n\n      - name: Terraform Plan\n        id: plan\n        run: terraform plan -no-color -out=tfplan\n        working-directory: infrastructure/environments/prod\n        env:\n          TF_VAR_db_password: ${{ secrets.PROD_DB_PASSWORD }}\n\n      - name: Comment Plan on PR\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const plan = `${{ steps.plan.outputs.stdout }}`\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: '```hcl\\n' + plan + '\\n```'\n            })\n\n  apply:\n    name: Terraform Apply\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main' &#x26;&#x26; github.event_name == 'push'\n    environment: production  # Requires manual approval in GitHub\n    steps:\n      - uses: actions/checkout@v4\n      # ... (same init steps)\n      - name: Terraform Apply\n        run: terraform apply -auto-approve\n        working-directory: infrastructure/environments/prod\n</code></pre>\n<h2>Preventing Accidental Destruction</h2>\n<p>Terraform's <code>destroy</code> capability is as powerful as it is dangerous. Guards against accidental destruction:</p>\n<pre><code class=\"language-hcl\"># Resource-level protection:\nresource \"aws_rds_cluster\" \"orders\" {\n  lifecycle {\n    prevent_destroy = true  # terraform destroy will fail with an error\n  }\n}\n\n# Prevent replacement (some changes destroy and recreate resources):\nresource \"aws_elasticsearch_domain\" \"search\" {\n  lifecycle {\n    # Force team to explicitly acknowledge destruction:\n    prevent_destroy = true\n    # Ignore tag changes that would otherwise trigger replacement:\n    ignore_changes = [tags]\n  }\n}\n</code></pre>\n<pre><code class=\"language-bash\"># Plan always before apply — never apply blindly:\nterraform plan -out=tfplan\n# Review: look for \"-\" (destroy) lines carefully\n# Especially: aws_rds_instance will be DESTROYED → check WHY\n\n# Targeted apply for surgical changes:\nterraform apply -target=module.orders_db  # Only apply orders_db module\n\n# Refresh state without changing resources:\nterraform refresh  # Detects drift between state and actual infrastructure\n</code></pre>\n<h2>Drift Detection</h2>\n<p>Real infrastructure drifts from Terraform state when engineers make manual changes in the console, incident responses bypass automation, or resources are modified by external automation.</p>\n<pre><code class=\"language-bash\"># Detect drift:\nterraform plan -refresh-only\n# Shows: \"Objects have changed outside of Terraform\"\n# Lists actual vs. expected state for each drifted resource\n\n# Import resources created outside Terraform into state:\nterraform import aws_s3_bucket.logs my-application-logs-bucket\n# Now Terraform manages this bucket — future changes go through Terraform\n</code></pre>\n<p><strong>Organizational discipline:</strong> The hardest part of IaC is not the tooling — it's the culture. Every infrastructure change must go through Terraform or it creates drift. Enforce this with IAM policies that deny manual resource creation in production, and use AWS Config rules to detect manually created resources.</p>\n<p>Terraform's power comes from predictability — the plan shows exactly what will change before anything does. That predictability only holds when every change goes through the plan/apply cycle, sensitive data stays in secrets management, and state is shared, locked, and versioned. The teams that treat <code>terraform plan</code> output with the same rigor they treat code review catch infrastructure problems before they become production incidents.</p>\n","tableOfContents":[{"id":"project-structure-for-production","text":"Project Structure for Production","level":2},{"id":"remote-state-with-s3-and-dynamodb-locking","text":"Remote State with S3 and DynamoDB Locking","level":2},{"id":"module-design-patterns","text":"Module Design Patterns","level":2},{"id":"sensitive-variables-never-hardcode-secrets","text":"Sensitive Variables: Never Hardcode Secrets","level":2},{"id":"workspaces-vs-separate-state-files","text":"Workspaces vs. Separate State Files","level":2},{"id":"cicd-pipeline-integration","text":"CI/CD Pipeline Integration","level":2},{"id":"preventing-accidental-destruction","text":"Preventing Accidental Destruction","level":2},{"id":"drift-detection","text":"Drift Detection","level":2}]},"relatedPosts":[{"title":"AWS Lambda in Production: Cold Starts, Concurrency, and Cost Optimization","description":"How Lambda execution environments work, cold start mitigation strategies, concurrency limits and throttling, Lambda power tuning, VPC networking costs, and when Lambda is the wrong tool.","date":"2025-06-28","category":"AWS","tags":["aws","lambda","serverless","java","cold start","performance","cost optimization"],"featured":false,"affiliateSection":"aws-resources","slug":"aws-lambda-production-patterns","readingTime":"7 min read","excerpt":"Lambda's value proposition is compelling: run code without managing servers, pay per invocation, scale from zero to 10,000 concurrent executions without configuration. The reality is a set of execution model nuances that…"},{"title":"Kubernetes in Production: Patterns Every Backend Engineer Must Know","description":"Resource requests and limits, liveness vs readiness probes, rolling deployments, HPA configuration, pod disruption budgets, and the mistakes that cause production outages in Kubernetes.","date":"2025-06-08","category":"AWS","tags":["kubernetes","k8s","devops","containers","deployment","aws","eks"],"featured":false,"affiliateSection":"aws-resources","slug":"kubernetes-production-best-practices","readingTime":"6 min read","excerpt":"Running a container in Kubernetes and running a production workload in Kubernetes are different disciplines. The gap between  and a service that survives node failures, deployment rollouts, and traffic spikes without use…"},{"title":"Cloud Cost Optimization: Engineering Practices That Cut AWS Bills by 50%","description":"Systematic AWS cost reduction: right-sizing EC2 and RDS instances, Savings Plans vs Reserved Instances, S3 lifecycle policies, data transfer cost elimination, EKS node optimization, RDS read replicas vs caching, and the observability stack for cost monitoring.","date":"2025-04-29","category":"AWS","tags":["aws","cost optimization","ec2","rds","s3","eks","savings plans","finops"],"featured":false,"affiliateSection":"aws-resources","slug":"cloud-cost-optimization","readingTime":"7 min read","excerpt":"Cloud bills scale with usage — but they also scale with inattention. Most teams that haven't deliberately optimized their AWS spend are 30-50% over what they need to pay for the same workload. The savings come from a pre…"}]},"__N_SSG":true}