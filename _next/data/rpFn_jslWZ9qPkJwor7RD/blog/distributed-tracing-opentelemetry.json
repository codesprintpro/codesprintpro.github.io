{"pageProps":{"post":{"title":"Distributed Tracing with OpenTelemetry: End-to-End Observability","description":"Implement distributed tracing across microservices using OpenTelemetry, Jaeger, and Spring Boot. Learn trace context propagation, span correlation, and production observability patterns.","date":"2025-03-05","category":"System Design","tags":["observability","opentelemetry","distributed tracing","jaeger","spring boot","microservices"],"featured":false,"affiliateSection":"system-design-courses","slug":"distributed-tracing-opentelemetry","readingTime":"11 min read","excerpt":"A request enters your system, touches 8 services, and takes 3 seconds. Which service is slow? Without distributed tracing, you're correlating timestamps across 8 log files. With distributed tracing, you click on the trac…","contentHtml":"<p>A request enters your system, touches 8 services, and takes 3 seconds. Which service is slow? Without distributed tracing, you're correlating timestamps across 8 log files. With distributed tracing, you click on the trace and see the waterfall: Service A took 50ms, Service B took 2800ms. Problem found.</p>\n<h2>How Distributed Tracing Works</h2>\n<p>Before instrumenting anything, it helps to understand the data model. The trace below shows what a real slow checkout request looks like after tracing is in place. Each indented line is a span — a named, timed operation. The tree structure shows causality: which service called which, and how long each call took. Without this structure, you would be staring at timestamps across 8 separate log streams trying to reconstruct the same picture manually.</p>\n<pre><code>Request: POST /checkout\n\nTrace ID: abc-123 (spans entire request, crosses all services)\n\nSpan tree:\n  [abc-123] API Gateway          0ms - 3100ms  ←──── Root span\n    [abc-123] Order Service       5ms - 3090ms\n      [abc-123] Validate Cart     5ms - 50ms\n      [abc-123] Inventory Service 55ms - 300ms  ← External call\n      [abc-123] Payment Service   305ms - 3085ms ← SLOW — 2780ms!\n        [abc-123] Stripe API      310ms - 3080ms ← Stripe timeout\n\nProblem: Payment Service → Stripe call took 2770ms\nFix: Implement timeout + retry for Stripe calls\n</code></pre>\n<p>Each <strong>trace</strong> represents one end-to-end request. Each <strong>span</strong> represents one operation within that trace. Spans have parent-child relationships forming a tree.</p>\n<h2>OpenTelemetry: The Standard</h2>\n<p>OpenTelemetry (OTel) is the industry standard for instrumentation — vendor-neutral, CNCF project. It replaces Zipkin/Jaeger-specific SDKs.</p>\n<p>The architecture diagram below shows the data flow from your application through the OTel Collector to your backend storage systems. The Collector is the critical piece — it acts as a buffer and router between your application's telemetry output and whichever backends you use. If you change from Jaeger to Grafana Tempo next year, you update the Collector configuration and leave your application code untouched. This vendor neutrality is the primary reason to use OTel over instrumenting directly against a backend SDK.</p>\n<pre><code>Your App\n  │ OTel SDK (instrumentation)\n  │\n  ▼\nOTel Collector (receive, process, export)\n  │\n  ├──► Jaeger (traces UI)\n  ├──► Prometheus (metrics)\n  └──► Elasticsearch (logs)\n</code></pre>\n<h2>Spring Boot Auto-Instrumentation</h2>\n<p>The easiest path — Spring Boot 3 has first-class OpenTelemetry support via Spring Actuator + Micrometer Tracing.</p>\n<p>Add the following three dependencies to get automatic instrumentation of your entire Spring Boot application. The <code>micrometer-tracing-bridge-otel</code> dependency bridges Spring's internal tracing abstraction to the OTel SDK, so all Spring components (RestTemplate, JPA, Kafka) emit traces without any code changes.</p>\n<pre><code class=\"language-xml\">&#x3C;!-- pom.xml -->\n&#x3C;dependency>\n    &#x3C;groupId>io.micrometer&#x3C;/groupId>\n    &#x3C;artifactId>micrometer-tracing-bridge-otel&#x3C;/artifactId>\n&#x3C;/dependency>\n&#x3C;dependency>\n    &#x3C;groupId>io.opentelemetry&#x3C;/groupId>\n    &#x3C;artifactId>opentelemetry-exporter-otlp&#x3C;/artifactId>\n&#x3C;/dependency>\n&#x3C;dependency>\n    &#x3C;groupId>io.opentelemetry.instrumentation&#x3C;/groupId>\n    &#x3C;artifactId>opentelemetry-spring-boot-starter&#x3C;/artifactId>\n    &#x3C;version>2.9.0&#x3C;/version>\n&#x3C;/dependency>\n</code></pre>\n<p>The application configuration below is where you define your service identity and connect to your OTel Collector. The <code>sampling.probability: 1.0</code> setting traces 100% of requests — appropriate for development where you want full coverage. In production, you will lower this to 0.01-0.1 to control data volume and cost, or switch to tail-based sampling entirely (covered later).</p>\n<pre><code class=\"language-yaml\"># application.yml\nspring:\n  application:\n    name: order-service\n\nmanagement:\n  tracing:\n    sampling:\n      probability: 1.0      # 100% in dev, 0.01-0.1 in production\n\notel:\n  exporter:\n    otlp:\n      endpoint: http://otel-collector:4317\n  resource:\n    attributes:\n      service.name: order-service\n      service.version: 1.0.0\n      deployment.environment: production\n</code></pre>\n<p>With these dependencies, Spring Boot automatically instruments:</p>\n<ul>\n<li>All HTTP requests/responses (RestTemplate, WebClient, @RestController)</li>\n<li>All database calls (Spring Data, JDBC)</li>\n<li>All Kafka producer/consumer operations</li>\n<li>All @Scheduled and async operations</li>\n</ul>\n<p><strong>Zero code changes required for basic tracing.</strong></p>\n<h2>Manual Instrumentation: Custom Spans</h2>\n<p>For business logic you want to trace explicitly:</p>\n<p>Auto-instrumentation captures framework-level operations but has no awareness of your business logic. When you want to understand how long inventory validation took versus payment processing within a single service, you need custom spans. The checkout example below creates a parent span for the entire operation and child spans for each sub-step — giving you granular timing data for each business decision, plus structured tags that make spans searchable by customer, order size, or failure reason.</p>\n<pre><code class=\"language-java\">@Service\npublic class CheckoutService {\n\n    private final Tracer tracer;\n\n    public CheckoutService(Tracer tracer) {\n        this.tracer = tracer;\n    }\n\n    public CheckoutResult checkout(CheckoutRequest request) {\n        // Create a custom span for the entire checkout flow\n        Span span = tracer.nextSpan()\n            .name(\"checkout.process\")\n            .tag(\"customer.id\", request.getCustomerId())\n            .tag(\"cart.item_count\", String.valueOf(request.getItems().size()))\n            .start();\n\n        try (Tracer.SpanInScope ws = tracer.withSpan(span)) {\n\n            // Child span: inventory validation\n            CheckoutResult inventoryResult = withSpan(\"checkout.validate_inventory\", () -> {\n                return inventoryService.validateAndReserve(request.getItems());\n            });\n\n            if (!inventoryResult.isSuccess()) {\n                span.tag(\"checkout.failure_reason\", \"inventory_unavailable\");\n                span.event(\"inventory_check_failed\");\n                return CheckoutResult.inventoryFailed(inventoryResult.getUnavailableItems());\n            }\n\n            // Child span: payment processing\n            PaymentResult paymentResult = withSpan(\"checkout.process_payment\", () -> {\n                return paymentService.charge(request.getPaymentMethod(), inventoryResult.getTotal());\n            });\n\n            span.tag(\"payment.provider\", paymentResult.getProvider());\n            span.tag(\"checkout.success\", \"true\");\n\n            return CheckoutResult.success(paymentResult.getOrderId());\n\n        } catch (Exception e) {\n            span.tag(\"error\", \"true\");\n            span.tag(\"error.message\", e.getMessage());\n            throw e;\n        } finally {\n            span.end();\n        }\n    }\n\n    private &#x3C;T> T withSpan(String name, Supplier&#x3C;T> operation) {\n        Span childSpan = tracer.nextSpan().name(name).start();\n        try (Tracer.SpanInScope ws = tracer.withSpan(childSpan)) {\n            return operation.get();\n        } catch (Exception e) {\n            childSpan.tag(\"error\", \"true\");\n            throw e;\n        } finally {\n            childSpan.end();\n        }\n    }\n}\n</code></pre>\n<p>The <code>span.end()</code> call in the <code>finally</code> block is essential — an unclosed span is never exported to Jaeger. Always use try/finally or the try-with-resources pattern to guarantee spans are closed, even when exceptions propagate.</p>\n<h2>Trace Context Propagation</h2>\n<p>Spans across services need the trace ID to be passed in HTTP headers. Spring Boot does this automatically, but for custom HTTP clients:</p>\n<p>Trace context propagation is what makes distributed tracing distributed. Without it, each service creates its own isolated trace with no connection to the upstream caller. The W3C <code>traceparent</code> header format is now the standard way to carry trace context across process boundaries — it encodes the trace ID, parent span ID, and sampling flag in a single header that all compliant frameworks recognize automatically.</p>\n<pre><code class=\"language-java\">// W3C Trace Context standard (use this — it's the industry standard)\n// Headers: traceparent: 00-traceId-spanId-flags\n\n@Bean\npublic RestTemplate tracingRestTemplate(RestTemplateBuilder builder) {\n    return builder\n        .additionalInterceptors(new TracingClientHttpRequestInterceptor())\n        .build();\n}\n\n// For Kafka: propagate trace context in message headers\n@Service\npublic class OrderEventPublisher {\n\n    @Autowired\n    private KafkaTemplate&#x3C;String, OrderEvent> kafkaTemplate;\n\n    @Autowired\n    private Tracer tracer;\n\n    public void publish(OrderEvent event) {\n        Span currentSpan = tracer.currentSpan();\n\n        ProducerRecord&#x3C;String, OrderEvent> record = new ProducerRecord&#x3C;>(\"order-events\", event);\n\n        // Inject current trace context into Kafka headers\n        if (currentSpan != null) {\n            TextMapPropagator propagator = GlobalOpenTelemetry.getPropagators().getTextMapPropagator();\n            propagator.inject(\n                Context.current(),\n                record.headers(),\n                (headers, key, value) -> headers.add(key, value.getBytes())\n            );\n        }\n\n        kafkaTemplate.send(record);\n    }\n}\n\n// Consumer: extract trace context from Kafka headers\n@KafkaListener(topics = \"order-events\")\npublic void handleOrderEvent(ConsumerRecord&#x3C;String, OrderEvent> record) {\n    // Extract trace context from headers\n    Context extractedContext = GlobalOpenTelemetry.getPropagators()\n        .getTextMapPropagator()\n        .extract(Context.current(), record.headers(),\n            (headers, key) -> new String(headers.lastHeader(key).value())\n        );\n\n    // Start a new span that's a child of the producer span\n    Span span = tracer.nextSpan(extractedContext)\n        .name(\"kafka.consume.order-events\")\n        .start();\n\n    try (Tracer.SpanInScope ws = tracer.withSpan(span)) {\n        processOrder(record.value());\n    } finally {\n        span.end();\n    }\n}\n</code></pre>\n<p>The Kafka propagation pattern deserves special attention: the producer injects trace context into Kafka message headers, and the consumer extracts it to create a child span. This creates a single trace that spans the publish/consume boundary — even though the consumer may run minutes or hours later on a completely different instance. Without this, your async event processing appears as disconnected, orphaned traces.</p>\n<h2>OpenTelemetry Collector Configuration</h2>\n<p>The Collector is where you shape your telemetry data before it reaches your backends. The configuration below is a production-ready pipeline that batches spans for efficiency, samples 10% in production, enriches all spans with environment metadata, and drops health check spans that add noise without diagnostic value.</p>\n<pre><code class=\"language-yaml\"># otel-collector.yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n      http:\n        endpoint: 0.0.0.0:4318\n\nprocessors:\n  batch:\n    timeout: 1s\n    send_batch_size: 1024\n\n  # Sample 10% in production (saves cost)\n  probabilistic_sampler:\n    sampling_percentage: 10\n\n  # Add environment attribute to all spans\n  resource:\n    attributes:\n      - key: deployment.environment\n        value: production\n        action: insert\n\n  # Drop health check traces (noise)\n  filter:\n    traces:\n      span:\n        - 'attributes[\"http.route\"] == \"/health\"'\n        - 'attributes[\"http.route\"] == \"/actuator/health\"'\n\nexporters:\n  jaeger:\n    endpoint: http://jaeger:14250\n\n  # Also export to Tempo (Grafana) for correlated metrics+traces\n  otlp/tempo:\n    endpoint: http://tempo:4317\n\n  # Export to Elasticsearch for long-term storage\n  elasticsearch:\n    endpoints: [http://elasticsearch:9200]\n    index: traces-{yyyy.MM.dd}\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch, probabilistic_sampler, resource, filter]\n      exporters: [jaeger, otlp/tempo]\n</code></pre>\n<p>The filter processor that drops <code>/health</code> and <code>/actuator/health</code> spans is worth calling out explicitly — at 10,000 RPS with Kubernetes liveness probes running every 10 seconds, health check spans can account for 30-40% of your total trace volume without providing any diagnostic value. Filtering them at the Collector prevents wasted storage and keeps your trace UI clean.</p>\n<h2>Correlating Traces with Logs</h2>\n<p>Traces tell you which service is slow — logs tell you why. The connection between them is the trace ID, which needs to appear in every log line so you can pivot from a slow span in Jaeger directly to the log lines that explain what happened.</p>\n<pre><code class=\"language-java\">// Add trace ID to all log entries — essential for correlation\n// Spring Boot + Logback — automatic with micrometer-tracing\n\n// logback-spring.xml\n&#x3C;configuration>\n  &#x3C;appender name=\"JSON\" class=\"ch.qos.logback.core.ConsoleAppender\">\n    &#x3C;encoder class=\"net.logstash.logback.encoder.LogstashEncoder\">\n      &#x3C;!-- Automatically includes traceId and spanId from MDC -->\n      &#x3C;includeMdcKeyName>traceId&#x3C;/includeMdcKeyName>\n      &#x3C;includeMdcKeyName>spanId&#x3C;/includeMdcKeyName>\n    &#x3C;/encoder>\n  &#x3C;/appender>\n&#x3C;/configuration>\n</code></pre>\n<p>With <code>micrometer-tracing</code> on the classpath, Spring automatically writes the current trace ID and span ID into the logging MDC (Mapped Diagnostic Context). Every log line your application writes will automatically include both fields — no manual log decoration needed. The JSON output below shows what this looks like in practice.</p>\n<pre><code class=\"language-json\">// Log output — every log line has traceId\n{\n  \"timestamp\": \"2025-03-05T10:15:32.045Z\",\n  \"level\": \"INFO\",\n  \"logger\": \"CheckoutService\",\n  \"message\": \"Processing payment for order abc-123\",\n  \"traceId\": \"4bf92f3577b34da6a3ce929d0e0e4736\",\n  \"spanId\": \"00f067aa0ba902b7\",\n  \"service\": \"order-service\"\n}\n</code></pre>\n<p>Now in Jaeger, you click a slow span, copy the traceId, and search Elasticsearch for all logs with that traceId. You go from trace → exact log lines that caused the failure.</p>\n<h2>Production Sampling Strategies</h2>\n<p>Sampling is the most consequential operational decision in your tracing setup. At 10,000 RPS with 100% sampling, you are exporting 10,000 traces per second — roughly 864 million traces per day. The cost and storage implications make 100% sampling infeasible in production, but naive random sampling means you are likely to miss the exact traces you need most (errors and slow requests).</p>\n<pre><code class=\"language-java\">// Don't trace 100% in production — at 10,000 RPS, that's enormous data\n// Strategies:\n\n// 1. Head-based sampling (decide at trace start)\n// Simple: sample 1% of all traces\n// Problem: you miss rare slow/error traces\n\n// 2. Tail-based sampling (decide after trace completes) — better\n// Always sample:\n//   - Errors (status >= 400)\n//   - Slow traces (duration > 2 seconds)\n//   - Low-traffic traces\n// Sample 1% of fast/successful traces\n\n// In OTel Collector (tail-based sampling):\nprocessors:\n  tail_sampling:\n    decision_wait: 10s      # Wait 10s after trace start to decide\n    policies:\n      - name: errors\n        type: status_code\n        status_code: {status_codes: [ERROR]}\n      - name: slow-traces\n        type: latency\n        latency: {threshold_ms: 2000}\n      - name: probabilistic-1-percent\n        type: probabilistic\n        probabilistic: {sampling_percentage: 1}\n    operator: or            # Include if ANY policy matches\n</code></pre>\n<p>The tail-based sampler above is the recommended production configuration: it guarantees you always capture error traces and slow traces (the ones you actually need for debugging), while sampling only 1% of the fast/successful traces (which are useful for baseline statistics but not individual analysis). The <code>decision_wait: 10s</code> delay means the Collector buffers span data for 10 seconds before deciding — long enough to see whether the full trace completed with errors.</p>\n<h2>Observability Dashboard: The Three Pillars</h2>\n<p>With traces, logs, and metrics all in place, the workflow for debugging a production incident becomes deterministic. The three-step investigation pattern below shows how each telemetry type builds on the last to identify root cause without guesswork.</p>\n<pre><code>Metrics (Prometheus/Grafana): WHAT is broken\n  - p99 latency: 3.2s ← abnormal\n  - Error rate: 12% ← abnormal\n  - Throughput: 800 RPS ← normal\n\nTraces (Jaeger): WHERE it's broken\n  - Click slow trace → waterfall\n  - Payment Service: 2800ms of 3200ms total\n  - Span: \"stripe.charge\" — status=ERROR\n\nLogs (Elasticsearch): WHY it's broken\n  - Filter by traceId\n  - \"Connection timeout after 2000ms: https://api.stripe.com\"\n  - Log 12 seconds earlier: \"Stripe circuit breaker opened\"\n</code></pre>\n<p>Distributed tracing without logs and metrics is incomplete. The full observability picture requires all three: metrics tell you something is wrong, traces tell you where, logs tell you why. OpenTelemetry gives you a unified way to instrument all three from the same codebase.</p>\n<h2>Quick Start: Local Development</h2>\n<p>The fastest way to validate your instrumentation is working is to run Jaeger and the OTel Collector locally. The docker-compose configuration below gives you a complete observability stack in two containers — no cloud accounts, no billing, no configuration beyond a single YAML file.</p>\n<pre><code class=\"language-yaml\"># docker-compose.yml\nversion: '3.8'\nservices:\n  jaeger:\n    image: jaegertracing/all-in-one:latest\n    ports:\n      - \"16686:16686\"   # Jaeger UI\n      - \"14250:14250\"   # gRPC\n    environment:\n      - COLLECTOR_OTLP_ENABLED=true\n\n  otel-collector:\n    image: otel/opentelemetry-collector-contrib:latest\n    volumes:\n      - ./otel-collector.yaml:/etc/otel-collector.yaml\n    command: [\"--config=/etc/otel-collector.yaml\"]\n    ports:\n      - \"4317:4317\"   # OTLP gRPC\n      - \"4318:4318\"   # OTLP HTTP\n    depends_on:\n      - jaeger\n</code></pre>\n<p>Navigate to <code>localhost:16686</code> after running your app — you'll see traces. Click any trace to see the full waterfall. Click any span to see attributes, logs, and events.</p>\n<p>The shift from \"check logs on 8 servers\" to \"click on the trace\" is one of the largest productivity improvements in microservices operations. Instrument once, debug forever.</p>\n","tableOfContents":[{"id":"how-distributed-tracing-works","text":"How Distributed Tracing Works","level":2},{"id":"opentelemetry-the-standard","text":"OpenTelemetry: The Standard","level":2},{"id":"spring-boot-auto-instrumentation","text":"Spring Boot Auto-Instrumentation","level":2},{"id":"manual-instrumentation-custom-spans","text":"Manual Instrumentation: Custom Spans","level":2},{"id":"trace-context-propagation","text":"Trace Context Propagation","level":2},{"id":"opentelemetry-collector-configuration","text":"OpenTelemetry Collector Configuration","level":2},{"id":"correlating-traces-with-logs","text":"Correlating Traces with Logs","level":2},{"id":"production-sampling-strategies","text":"Production Sampling Strategies","level":2},{"id":"observability-dashboard-the-three-pillars","text":"Observability Dashboard: The Three Pillars","level":2},{"id":"quick-start-local-development","text":"Quick Start: Local Development","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}