{"pageProps":{"post":{"title":"Database Indexing Deep Dive: B-Trees, Hash Indexes, and Query Planning","description":"Master database indexing internals. Understand B-tree structure, hash indexes, composite indexes, covering indexes, and how query planners use them. Includes PostgreSQL EXPLAIN analysis.","date":"2025-03-09","category":"Databases","tags":["postgresql","indexing","b-tree","query optimization","databases","performance"],"featured":false,"affiliateSection":"database-resources","slug":"database-indexing-deep-dive","readingTime":"12 min read","excerpt":"Indexes are the single most impactful optimization in database performance. A 10-second query becomes 20ms with the right index. A wrong index slows writes and misleads the query planner. Understanding the internals — no…","contentHtml":"<p>Indexes are the single most impactful optimization in database performance. A 10-second query becomes 20ms with the right index. A wrong index slows writes and misleads the query planner. Understanding the internals — not just \"add an index on the WHERE column\" — is what separates engineers who tune databases from those who keep adding hardware.</p>\n<h2>How B-Tree Indexes Work</h2>\n<p>PostgreSQL's default index type is B-Tree (Balanced Tree). Every index lookup starts here.</p>\n<p>Think of a B-Tree like a filing cabinet with a hierarchical sorting system. If you want to find a customer with ID <code>cust-123</code> in a 10-million-row table without an index, you have to flip through every single record. With a B-Tree, you start at the root, follow a branch left or right at each node based on the key value, and arrive at the exact record in roughly 24 steps — no matter how large the table grows. This is the difference between O(n) and O(log n).</p>\n<pre><code>Table: orders (10 million rows)\nColumn: customer_id (VARCHAR, not indexed)\n\nFull table scan:\n  SELECT * FROM orders WHERE customer_id = 'cust-123'\n  → Read all 10M rows, discard 9,999,990\n  → Cost: O(n) — terrible\n\nWith B-Tree index on customer_id:\n\nB-Tree structure:\n                    [cust-500]\n                   /          \\\n          [cust-200]            [cust-800]\n         /         \\            /        \\\n  [cust-100]  [cust-300] [cust-600] [cust-900]\n   /      \\    /      \\    ...\n[cust-123] ...\n\nLookup: cust-123\n  1. Root: cust-123 &#x3C; cust-500 → go left\n  2. Node: cust-123 &#x3C; cust-200 → go left\n  3. Node: cust-123 > cust-100 → go right\n  4. Found: cust-123 → pointer to row location\n\nCost: O(log n) = ~24 comparisons for 10M rows\nResult: 0.02ms vs 10 seconds\n</code></pre>\n<p>Each B-Tree leaf node stores:</p>\n<ul>\n<li>Index key value (customer_id)</li>\n<li>Pointer to heap page (the actual table row)</li>\n<li>Pointer to next/previous leaf node (for range scans)</li>\n</ul>\n<p>The leaf-node linking is what makes range queries (<code>WHERE created_at BETWEEN x AND y</code>) efficient on B-Trees. Once you find the starting key, you just follow the linked list of leaf nodes forward — no need to traverse the tree again for each value.</p>\n<h2>Index Scans vs Heap Fetches</h2>\n<p>Creating an index is only half the work. You also need to understand what happens after the index is used — specifically, the extra step of fetching the actual row data from the table heap. The <code>EXPLAIN ANALYZE</code> output below shows what this looks like in practice and hints at the opportunity for covering indexes.</p>\n<pre><code class=\"language-sql\">-- Create table and index\nCREATE TABLE orders (\n    id          BIGSERIAL PRIMARY KEY,\n    customer_id VARCHAR(36),\n    status      VARCHAR(20),\n    total_cents INTEGER,\n    created_at  TIMESTAMPTZ DEFAULT NOW()\n);\n\nCREATE INDEX idx_orders_customer ON orders (customer_id);\n\n-- Query 1: Point lookup — extremely efficient\nEXPLAIN ANALYZE SELECT * FROM orders WHERE customer_id = 'cust-123';\n\n-- Output:\n-- Index Scan using idx_orders_customer on orders\n--   (cost=0.56..8.58 rows=5 width=80) (actual time=0.030..0.045 rows=5 loops=1)\n--   Index Cond: (customer_id = 'cust-123')\n\n-- Query 2: Range scan — also efficient\nEXPLAIN ANALYZE SELECT * FROM orders\nWHERE customer_id = 'cust-123' AND created_at > NOW() - INTERVAL '30 days';\n\n-- The planner uses the index on customer_id, then filters by created_at\n-- Better: composite index (customer_id, created_at)\n</code></pre>\n<h2>Composite Indexes: Order Matters</h2>\n<p>The column order in a composite index determines which queries benefit.</p>\n<p>A composite index is like a phone book sorted by last name, then first name. You can efficiently look up everyone named \"Smith\" (leftmost column), or find \"John Smith\" specifically (both columns). But you cannot efficiently find everyone named \"John\" across all last names — there is no way to skip the first sort key. This is the leftmost prefix rule, and it governs every composite index you create.</p>\n<pre><code class=\"language-sql\">-- Index: (customer_id, created_at)\nCREATE INDEX idx_orders_customer_date ON orders (customer_id, created_at DESC);\n\n-- This index CAN satisfy these queries (leftmost prefix rule):\n-- 1. WHERE customer_id = ?                          ← uses full index\n-- 2. WHERE customer_id = ? AND created_at > ?       ← uses full index\n-- 3. WHERE customer_id = ? ORDER BY created_at DESC ← uses index order (no sort)\n\n-- This index CANNOT satisfy:\n-- 4. WHERE created_at > ?                           ← can't skip first column\n-- 5. WHERE status = ?                               ← not in index\n\n-- For query 4, you need a separate index: CREATE INDEX ON orders (created_at);\n\n-- Rule: put equality conditions first, range conditions last\n-- (a = ?, b = ?, c > ?)  →  INDEX(a, b, c)  ← correct\n-- (c > ?, a = ?, b = ?)  →  INDEX(a, b, c)  ← correct index, wrong column order\n</code></pre>\n<p>Always place equality conditions before range conditions in a composite index. PostgreSQL can use equality conditions to narrow down a specific sub-tree of the B-Tree, then scan forward for the range — but only if the range column comes after the equality columns.</p>\n<h2>Covering Indexes: Eliminate Heap Fetches</h2>\n<p>A covering index contains all columns the query needs — the index itself answers the query without touching the table.</p>\n<p>Every time your query returns a column that is not in the index, PostgreSQL must jump from the index to the actual table heap to fetch that column. For a query returning 50,000 rows, that is 50,000 random disk reads. A covering index eliminates all of them by embedding the needed column values directly in the index leaf nodes using <code>INCLUDE</code>.</p>\n<pre><code class=\"language-sql\">-- Query: get order IDs and totals for a customer (no need to fetch full row)\nSELECT id, total_cents FROM orders WHERE customer_id = 'cust-123';\n\n-- Without covering index:\n--   1. Scan index → find matching row pointers\n--   2. Fetch each row from heap (random I/O — expensive for many rows)\n\n-- Covering index (INCLUDE adds columns to leaf nodes without affecting tree structure):\nCREATE INDEX idx_orders_customer_covering\n  ON orders (customer_id)\n  INCLUDE (id, total_cents);\n\n-- Now query can be answered from index only — \"Index Only Scan\"\nEXPLAIN ANALYZE SELECT id, total_cents FROM orders WHERE customer_id = 'cust-123';\n-- Index Only Scan using idx_orders_customer_covering on orders\n--   Heap Fetches: 0  ← zero table reads!\n</code></pre>\n<p><code>Heap Fetches: 0</code> is what you are aiming for with a covering index. The <code>INCLUDE</code> columns live only in the leaf nodes and are not part of the B-Tree sort key, so they do not increase index maintenance cost as much as adding them as regular index columns would.</p>\n<h2>Partial Indexes: Index Only What You Query</h2>\n<pre><code class=\"language-sql\">-- Problem: 10M orders, but 99% are DELIVERED (rarely queried)\n-- Full index on status wastes space and slows writes\n\n-- Partial index: only index PENDING and PROCESSING orders\nCREATE INDEX idx_orders_active_status\n  ON orders (status, created_at)\n  WHERE status IN ('PENDING', 'PROCESSING');\n\n-- This index is tiny (~50K rows instead of 10M) and fast\n-- Query:\nSELECT * FROM orders WHERE status = 'PENDING' ORDER BY created_at;\n-- Uses partial index — only scans the 50K active rows\n\n-- Useful patterns:\n-- WHERE deleted_at IS NULL    (soft-deleted records)\n-- WHERE processed = false     (queue-like patterns)\n-- WHERE status != 'COMPLETED' (active/pending states)\n</code></pre>\n<p>Partial indexes are one of the most underused PostgreSQL features. If you have a queue-like table where 99% of rows are in a terminal state (COMPLETED, DELIVERED, ARCHIVED) but your application only queries active rows, a full index on the status column is 99% waste. A partial index covering only the active states is smaller, faster to update, and more likely to fit in the OS page cache.</p>\n<h2>Hash Indexes</h2>\n<p>Hash indexes are faster for equality lookups than B-Trees but support only <code>=</code> (no ranges, no ordering).</p>\n<p>Think of a hash index as a lookup dictionary with direct addressing: given a key, compute a hash, jump directly to the bucket. This is O(1) rather than B-Tree's O(log n), making hash lookups faster for pure equality queries. The trade-off is that hash functions produce no ordering — so range queries, sorting, and prefix searches are impossible.</p>\n<pre><code class=\"language-sql\">-- Create hash index\nCREATE INDEX idx_orders_id_hash ON orders USING HASH (id);\n\n-- Hash index uses: O(1) lookup for equality\n-- B-Tree: O(log n)\n-- Hash advantage: 20-40% faster for equality-only lookups\n\n-- BUT: hash indexes don't support:\n-- ORDER BY, BETWEEN, >, &#x3C;, >=, &#x3C;=\n-- LIKE 'prefix%'\n-- Multiple columns\n\n-- Use hash indexes for: lookup tables, user ID lookups, session tokens\n-- Use B-Tree indexes for: everything else\n</code></pre>\n<h2>Index Bloat and Maintenance</h2>\n<p>Over time, indexes accumulate dead weight. Every <code>UPDATE</code> or <code>DELETE</code> marks old index entries as dead rather than immediately removing them — PostgreSQL's MVCC model requires this so that older transactions can still use the stale entries. <code>VACUUM</code> reclaims this space automatically, but on very high-churn tables, bloat can accumulate faster than <code>autovacuum</code> clears it.</p>\n<pre><code class=\"language-sql\">-- Check index size and bloat\nSELECT\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE relname = 'orders'\nORDER BY pg_relation_size(indexrelid) DESC;\n\n-- Find unused indexes (never scanned — wasting write overhead)\nSELECT indexname, idx_scan\nFROM pg_stat_user_indexes\nWHERE relname = 'orders' AND idx_scan = 0;\n-- These indexes are candidates for removal\n\n-- Index bloat: happens after many updates/deletes\n-- PostgreSQL marks old versions dead but doesn't immediately reclaim space\n-- Fix: VACUUM ANALYZE (automatic) or REINDEX CONCURRENTLY (manual, online)\n\n-- Reindex without locking (PostgreSQL 12+):\nREINDEX INDEX CONCURRENTLY idx_orders_customer;\n</code></pre>\n<p>Any index with <code>idx_scan = 0</code> is a write tax with no corresponding read benefit. Drop it. Unused indexes are surprisingly common — they often accumulate from exploratory optimization attempts that were later superseded by a different index.</p>\n<h2>Reading EXPLAIN ANALYZE Output</h2>\n<p><code>EXPLAIN ANALYZE</code> is your most powerful tool for understanding what PostgreSQL actually does when it runs a query. The query below joins orders with customers and filters by status and date — a common pattern that exercises index selection, join strategy, and sort behavior all at once.</p>\n<pre><code class=\"language-sql\">EXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT o.id, o.total_cents, c.name\nFROM orders o\nJOIN customers c ON c.id = o.customer_id\nWHERE o.status = 'PENDING'\n  AND o.created_at > NOW() - INTERVAL '7 days'\nORDER BY o.total_cents DESC\nLIMIT 10;\n</code></pre>\n<pre><code>Hash Join  (cost=1234.56..2345.67 rows=10 width=80)\n           (actual time=45.234..89.123 rows=10 loops=1)\n   Buffers: shared hit=1234 read=567  ← hit=cache, read=disk I/O\n   ->  Limit  (cost=1000.00..1100.00 rows=10 width=60)\n         (actual time=40.123..40.456 rows=10 loops=1)\n       ->  Sort  (cost=1000.00..1050.00 rows=50 width=60)\n                 (actual time=40.100..40.200 rows=10 loops=1)\n             Sort Key: o.total_cents DESC\n             Sort Method: top-N heapsort  Memory: 25kB\n             ->  Index Scan using idx_orders_status_date on orders o\n                           (cost=0.56..900.34 rows=50 width=60)\n                           (actual time=0.100..35.234 rows=2847 loops=1)\n                   Index Cond: (status = 'PENDING' AND created_at > ...)\n   ->  Hash  (cost=100.00..100.00 rows=10000 width=40)\n             (actual time=4.567..4.567 rows=10000 loops=1)\n         ->  Seq Scan on customers c  ← WARNING: full table scan on customers\n               (cost=0.00..100.00 rows=10000 width=40)\n               (actual time=0.100..2.345 rows=10000 loops=1)\n\nPlanning Time: 2.345 ms\nExecution Time: 89.456 ms\n</code></pre>\n<p><strong>Reading the output:</strong></p>\n<ul>\n<li><code>cost=X..Y</code>: Estimated cost (X=first row, Y=all rows)</li>\n<li><code>actual time=X..Y</code>: Real measured time in ms</li>\n<li><code>rows=N</code>: Estimated vs actual rows (large difference = stale statistics)</li>\n<li><code>Seq Scan</code>: Full table scan — usually needs an index</li>\n<li><code>Buffers: read=567</code>: Disk reads — high count = slow query, consider caching</li>\n<li><code>Sort Method: external merge</code>: Sorting spilled to disk — increase work_mem</li>\n</ul>\n<p>In the output above, the <code>Seq Scan on customers</code> is the red flag — every query hitting this join is doing a full scan of the customers table. Adding an index on <code>customers.id</code> would likely eliminate it. Always look for <code>Seq Scan</code> on large tables as your first optimization target.</p>\n<h2>Index Strategy for Common Patterns</h2>\n<p>Now that you understand the tools, here is how they combine for the most common query patterns you will encounter in production applications. Each pattern below pairs a real-world query type with the optimal index structure.</p>\n<pre><code class=\"language-sql\">-- Pattern 1: User's recent orders (most common)\nCREATE INDEX idx_orders_user_recent ON orders (customer_id, created_at DESC)\n  WHERE status != 'CANCELLED';\n\n-- Pattern 2: Admin dashboard — orders by status with pagination\nCREATE INDEX idx_orders_status_created ON orders (status, created_at DESC);\n\n-- Pattern 3: Slow full-text search on description\nCREATE INDEX idx_products_search ON products USING GIN (\n  to_tsvector('english', name || ' ' || description)\n);\n\n-- Query:\nSELECT * FROM products\nWHERE to_tsvector('english', name || ' ' || description) @@ plainto_tsquery('wireless headphones');\n\n-- Pattern 4: JSON column queries\nCREATE INDEX idx_events_metadata ON events USING GIN (metadata jsonb_path_ops);\n-- Query: WHERE metadata @> '{\"type\": \"PAYMENT_FAILED\"}'\n\n-- Pattern 5: UUID primary key — use BRIN for sequential UUIDs (v7)\n-- UUIDv7 is monotonically increasing — use BRIN for 99% smaller index\nCREATE INDEX idx_orders_id_brin ON orders USING BRIN (id)\n  WHERE id::text ~ '^[0-9a-f]{8}-7';  -- Only UUIDv7 style\n</code></pre>\n<h2>The Index Decision Framework</h2>\n<pre><code>Should I add this index?\n\n1. Is this query in a hot path? (runs frequently or is user-facing)\n   NO → probably not worth it\n\n2. Does EXPLAIN show Seq Scan on a large table (>100K rows)?\n   YES → strong signal for an index\n\n3. What's the selectivity?\n   HIGH selectivity (WHERE user_id = ?) → B-Tree index\n   LOW selectivity (WHERE status = 'active', 90% rows) → partial index or no index\n\n4. Is this a write-heavy table?\n   YES → every index adds overhead to INSERT/UPDATE/DELETE\n   Rule: never add an index without measuring write performance impact\n\n5. Can a covering index eliminate heap fetches?\n   YES, if query reads few columns → INCLUDE those columns\n\nThe right number of indexes for most tables: 2-5.\nEvery additional index costs write throughput. Choose carefully.\n</code></pre>\n<p>The discipline of database optimization is 90% indexing and 10% everything else. Before touching application code, schema, or hardware, run EXPLAIN ANALYZE on your slowest queries and check: Is the query planner using the right index? If not, why not? Missing index? Wrong column order? Stale statistics? Answer those questions first.</p>\n","tableOfContents":[{"id":"how-b-tree-indexes-work","text":"How B-Tree Indexes Work","level":2},{"id":"index-scans-vs-heap-fetches","text":"Index Scans vs Heap Fetches","level":2},{"id":"composite-indexes-order-matters","text":"Composite Indexes: Order Matters","level":2},{"id":"covering-indexes-eliminate-heap-fetches","text":"Covering Indexes: Eliminate Heap Fetches","level":2},{"id":"partial-indexes-index-only-what-you-query","text":"Partial Indexes: Index Only What You Query","level":2},{"id":"hash-indexes","text":"Hash Indexes","level":2},{"id":"index-bloat-and-maintenance","text":"Index Bloat and Maintenance","level":2},{"id":"reading-explain-analyze-output","text":"Reading EXPLAIN ANALYZE Output","level":2},{"id":"index-strategy-for-common-patterns","text":"Index Strategy for Common Patterns","level":2},{"id":"the-index-decision-framework","text":"The Index Decision Framework","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring — every node is equal, there's no primary, and data placement is determined by partit…"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you…"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible — rollback a bad deploy and your code is back to the previous version. Database schema changes are st…"}]},"__N_SSG":true}