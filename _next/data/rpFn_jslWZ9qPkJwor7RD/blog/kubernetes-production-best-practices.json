{"pageProps":{"post":{"title":"Kubernetes in Production: Patterns Every Backend Engineer Must Know","description":"Resource requests and limits, liveness vs readiness probes, rolling deployments, HPA configuration, pod disruption budgets, and the mistakes that cause production outages in Kubernetes.","date":"2025-06-08","category":"AWS","tags":["kubernetes","k8s","devops","containers","deployment","aws","eks"],"featured":false,"affiliateSection":"aws-resources","slug":"kubernetes-production-best-practices","readingTime":"6 min read","excerpt":"Running a container in Kubernetes and running a production workload in Kubernetes are different disciplines. The gap between  and a service that survives node failures, deployment rollouts, and traffic spikes without use…","contentHtml":"<p>Running a container in Kubernetes and running a production workload in Kubernetes are different disciplines. The gap between <code>kubectl apply -f deployment.yaml</code> and a service that survives node failures, deployment rollouts, and traffic spikes without user-visible downtime is filled with configuration that doesn't exist in most tutorials.</p>\n<h2>Resource Requests and Limits: The Foundation</h2>\n<p>Every production pod must have resource requests and limits. Without them, Kubernetes cannot make scheduling decisions and nodes become dangerously overloaded.</p>\n<pre><code class=\"language-yaml\">resources:\n  requests:\n    memory: \"512Mi\"    # Scheduler uses this for placement decisions\n    cpu: \"250m\"        # 250 millicores = 25% of one CPU core\n  limits:\n    memory: \"1Gi\"      # Container is OOMKilled if it exceeds this\n    cpu: \"1000m\"       # Container is CPU-throttled (not killed) if it exceeds this\n</code></pre>\n<p><strong>CPU throttling vs OOM Kill:</strong> CPU limits throttle — the container is slowed but kept running. Memory limits kill — the container is OOMKilled and restarted. This distinction matters: a CPU limit that's too low causes latency spikes; a memory limit that's too low causes crashes.</p>\n<p><strong>Requests vs Limits ratio:</strong> Kubernetes allows \"overcommitting\" — requesting 500m but limiting at 2000m. This is valid for bursty workloads but creates a risk: if all pods burst simultaneously, the node runs out of resources. For critical services, set requests = limits (Guaranteed QoS class) to prevent eviction.</p>\n<p><strong>Setting the right values:</strong></p>\n<pre><code class=\"language-bash\"># Check actual usage in production:\nkubectl top pod -l app=api-service --containers\n# Use P95 of observed memory as request, P99 + 20% headroom as limit\n\n# For CPU: set request = P50 usage, limit = 2-4× request\n</code></pre>\n<h2>Liveness vs Readiness vs Startup Probes</h2>\n<p>These three probes are distinct and frequently misconfigured:</p>\n<pre><code class=\"language-yaml\">livenessProbe:\n  # Is the application alive? If not, restart the container.\n  # Use this ONLY for deadlock detection — processes that are running but stuck.\n  httpGet:\n    path: /actuator/health/liveness\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  failureThreshold: 3         # Restart after 3 failures\n  timeoutSeconds: 5\n\nreadinessProbe:\n  # Can the application serve traffic? If not, remove from Service endpoints.\n  # Use this to signal when the app is ready and when it's temporarily busy.\n  httpGet:\n    path: /actuator/health/readiness\n    port: 8080\n  initialDelaySeconds: 10\n  periodSeconds: 5\n  failureThreshold: 3\n  successThreshold: 1\n\nstartupProbe:\n  # Overrides liveness during startup — prevents premature restarts for slow-starting apps.\n  # Only needed when app takes > 30s to start.\n  httpGet:\n    path: /actuator/health/liveness\n    port: 8080\n  failureThreshold: 30        # Allow up to 30 × 10s = 300s to start\n  periodSeconds: 10\n</code></pre>\n<p><strong>Spring Boot actuator separation:</strong></p>\n<pre><code class=\"language-java\">// application.properties\nmanagement.endpoint.health.group.liveness.include=livenessState\nmanagement.endpoint.health.group.readiness.include=readinessState,db,redis\n</code></pre>\n<p>Readiness probe fails → pod removed from load balancer (no new traffic) → existing connections drain. This is correct behavior during DB connection issues — the pod stays alive but stops receiving traffic.</p>\n<p>Liveness probe fails → pod restarted. <strong>Do not include DB/external checks in liveness probes.</strong> If your DB is down and liveness probes fail, Kubernetes restarts all pods. Now you have all pods simultaneously in restart loops. The DB comes back but pods are thrashing. Always keep liveness probes lightweight.</p>\n<h2>Rolling Deployments Without Downtime</h2>\n<p>Default rolling update configuration is too aggressive:</p>\n<pre><code class=\"language-yaml\">strategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: 1          # Default: 25% — create at most 1 extra pod\n    maxUnavailable: 0    # Never have fewer than replicas running\n                         # This ensures zero-downtime: new pod must be Ready before old is terminated\n</code></pre>\n<p>For a service with 10 replicas:</p>\n<ul>\n<li><code>maxUnavailable: 0, maxSurge: 1</code> → 1 new pod created, 1 old pod terminated when new is Ready. Linear, predictable.</li>\n<li><code>maxUnavailable: 25%, maxSurge: 25%</code> → up to 2 old pods removed before new pods are Ready → brief 80% capacity.</li>\n</ul>\n<p><strong>Graceful shutdown:</strong> When Kubernetes terminates a pod, it sends <code>SIGTERM</code>, waits <code>terminationGracePeriodSeconds</code>, then sends <code>SIGKILL</code>. Your application must handle <code>SIGTERM</code> gracefully — stop accepting new connections, finish in-flight requests, then exit.</p>\n<pre><code class=\"language-java\">// Spring Boot graceful shutdown:\n// application.properties:\nserver.shutdown=graceful\nspring.lifecycle.timeout-per-shutdown-phase=30s\n</code></pre>\n<pre><code class=\"language-yaml\"># Pod spec:\nterminationGracePeriodSeconds: 60  # Must be > your slowest request timeout\nlifecycle:\n  preStop:\n    exec:\n      command: [\"sh\", \"-c\", \"sleep 5\"]\n      # 5-second sleep before SIGTERM gives the load balancer time to\n      # deregister the pod before it stops accepting connections\n</code></pre>\n<h2>Horizontal Pod Autoscaler Configuration</h2>\n<pre><code class=\"language-yaml\">apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-service\n  minReplicas: 3          # Never go below 3 — one per AZ for HA\n  maxReplicas: 50\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 60    # Scale at 60%, not 80% — headroom for spikes\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 70\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 0      # Scale up immediately\n      policies:\n      - type: Percent\n        value: 100                        # Can double pod count per 15s\n        periodSeconds: 15\n    scaleDown:\n      stabilizationWindowSeconds: 300    # Wait 5 minutes before scaling down\n</code></pre>\n<p><strong>The HPA + JVM problem:</strong> JVM heap is counted against memory limits. During startup, JVM allocates max heap upfront. If <code>maxHeap > memory.request</code>, every new pod immediately looks memory-heavy. HPA sees average memory at 90% and scales up before the JVM has warmed up. Fix: set <code>Xmx</code> to <code>memory.limit × 0.75</code>, and set <code>memory.request = memory.limit</code> (Guaranteed QoS).</p>\n<h2>Pod Disruption Budgets</h2>\n<p>PDBs prevent Kubernetes from simultaneously evicting too many pods during node drains or cluster upgrades:</p>\n<pre><code class=\"language-yaml\">apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: api-service-pdb\nspec:\n  minAvailable: 2     # Always keep at least 2 pods running\n  # OR:\n  maxUnavailable: 1   # Never disrupt more than 1 pod at a time\n  selector:\n    matchLabels:\n      app: api-service\n</code></pre>\n<p>Without a PDB, <code>kubectl drain node-1</code> removes all pods on that node simultaneously. With <code>minAvailable: 2</code> on a 3-replica deployment, the drain can only proceed one pod at a time — safe.</p>\n<h2>ConfigMaps and Secrets: Common Mistakes</h2>\n<pre><code class=\"language-yaml\"># DO: Use envFrom for cleaner pod specs\nenvFrom:\n- configMapRef:\n    name: api-config\n- secretRef:\n    name: api-secrets\n\n# DON'T: Mount secrets as env vars for sensitive data that rotates —\n# env vars require pod restart to pick up new values.\n# DO: Mount as files for secrets that rotate:\nvolumeMounts:\n- name: db-credentials\n  mountPath: /etc/credentials\n  readOnly: true\nvolumes:\n- name: db-credentials\n  secret:\n    secretName: db-credentials\n    # Updates to the secret propagate to the file within ~1 minute\n    # No pod restart needed\n</code></pre>\n<h2>Resource Quotas Per Namespace</h2>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: production-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"50\"          # Total CPU requests across all pods\n    requests.memory: 100Gi\n    limits.cpu: \"100\"\n    limits.memory: 200Gi\n    pods: \"200\"\n    services: \"20\"\n    persistentvolumeclaims: \"50\"\n</code></pre>\n<p>Quotas prevent a single team's misconfigured deployment from consuming all cluster resources.</p>\n<h2>Production Checklist</h2>\n<p>Before any service goes to production on Kubernetes:</p>\n<pre><code>□ Resource requests AND limits set on all containers\n□ Liveness probe (lightweight, no external deps)\n□ Readiness probe (includes DB/cache connectivity)\n□ Graceful shutdown configured (SIGTERM handler + preStop sleep)\n□ terminationGracePeriodSeconds > max request duration\n□ PodDisruptionBudget configured (minAvailable ≥ 2 for critical services)\n□ HPA configured with appropriate min/max replicas\n□ Anti-affinity rules for HA (pods spread across AZs)\n□ Network policies limiting ingress/egress\n□ Image tag pinned (never use :latest in production)\n□ Resource quotas on namespace\n</code></pre>\n<p>Anti-affinity for AZ spread:</p>\n<pre><code class=\"language-yaml\">affinity:\n  podAntiAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n    - labelSelector:\n        matchLabels:\n          app: api-service\n      topologyKey: topology.kubernetes.io/zone\n      # Required: pods MUST be in different AZs\n      # If only 1 AZ available, pod stays Pending (fail safe)\n</code></pre>\n","tableOfContents":[{"id":"resource-requests-and-limits-the-foundation","text":"Resource Requests and Limits: The Foundation","level":2},{"id":"liveness-vs-readiness-vs-startup-probes","text":"Liveness vs Readiness vs Startup Probes","level":2},{"id":"rolling-deployments-without-downtime","text":"Rolling Deployments Without Downtime","level":2},{"id":"horizontal-pod-autoscaler-configuration","text":"Horizontal Pod Autoscaler Configuration","level":2},{"id":"pod-disruption-budgets","text":"Pod Disruption Budgets","level":2},{"id":"configmaps-and-secrets-common-mistakes","text":"ConfigMaps and Secrets: Common Mistakes","level":2},{"id":"resource-quotas-per-namespace","text":"Resource Quotas Per Namespace","level":2},{"id":"production-checklist","text":"Production Checklist","level":2}]},"relatedPosts":[{"title":"AWS Lambda in Production: Cold Starts, Concurrency, and Cost Optimization","description":"How Lambda execution environments work, cold start mitigation strategies, concurrency limits and throttling, Lambda power tuning, VPC networking costs, and when Lambda is the wrong tool.","date":"2025-06-28","category":"AWS","tags":["aws","lambda","serverless","java","cold start","performance","cost optimization"],"featured":false,"affiliateSection":"aws-resources","slug":"aws-lambda-production-patterns","readingTime":"7 min read","excerpt":"Lambda's value proposition is compelling: run code without managing servers, pay per invocation, scale from zero to 10,000 concurrent executions without configuration. The reality is a set of execution model nuances that…"},{"title":"Terraform Infrastructure as Code: Production Patterns and Pitfalls","description":"Production Terraform: module design, state management with S3 and DynamoDB locking, workspace strategies for multi-environment deployments, sensitive variable handling, drift detection, and the Terraform anti-patterns that cause outages.","date":"2025-05-14","category":"AWS","tags":["terraform","infrastructure as code","aws","devops","s3","modules","ci/cd"],"featured":false,"affiliateSection":"aws-resources","slug":"terraform-infrastructure-as-code","readingTime":"7 min read","excerpt":"Terraform is the industry-standard tool for Infrastructure as Code (IaC) — defining cloud infrastructure as declarative HCL configuration that can be version-controlled, reviewed, and applied reproducibly. The value prop…"},{"title":"Cloud Cost Optimization: Engineering Practices That Cut AWS Bills by 50%","description":"Systematic AWS cost reduction: right-sizing EC2 and RDS instances, Savings Plans vs Reserved Instances, S3 lifecycle policies, data transfer cost elimination, EKS node optimization, RDS read replicas vs caching, and the observability stack for cost monitoring.","date":"2025-04-29","category":"AWS","tags":["aws","cost optimization","ec2","rds","s3","eks","savings plans","finops"],"featured":false,"affiliateSection":"aws-resources","slug":"cloud-cost-optimization","readingTime":"7 min read","excerpt":"Cloud bills scale with usage — but they also scale with inattention. Most teams that haven't deliberately optimized their AWS spend are 30-50% over what they need to pay for the same workload. The savings come from a pre…"}]},"__N_SSG":true}