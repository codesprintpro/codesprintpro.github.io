{"pageProps":{"post":{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you…","contentHtml":"<p>DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you build your schema, because the schema directly encodes those patterns. The engineers who learn DynamoDB from relational habits write inefficient schemas. The engineers who internalize its data model write schemas that scale to billions of rows with consistent latency.</p>\n<h2>DynamoDB's Data Model</h2>\n<p>Every DynamoDB table has a primary key — either a partition key (simple) or a partition key + sort key (composite). Unlike Cassandra, there's no concept of clustering order across the whole table — ordering is local to a partition key.</p>\n<pre><code>Table: Orders\n┌─────────────┬──────────────────┬────────────────────┬───────────────┐\n│  PK (pk)    │  SK (sk)         │  Attributes         │  Notes        │\n├─────────────┼──────────────────┼────────────────────┼───────────────┤\n│ ORDER#1001  │ ORDER#1001       │ status, total, date │ Order entity  │\n│ ORDER#1001  │ ITEM#prod-001    │ qty, price, name    │ Order item    │\n│ ORDER#1001  │ ITEM#prod-002    │ qty, price, name    │ Order item    │\n│ ORDER#1001  │ STATUS#shipped   │ carrier, tracking   │ Status event  │\n│ CUSTOMER#42 │ CUSTOMER#42      │ name, email, tier   │ Customer      │\n│ CUSTOMER#42 │ ORDER#1001       │ date, status, total │ Cust → Order  │\n│ CUSTOMER#42 │ ORDER#1005       │ date, status, total │ Cust → Order  │\n└─────────────┴──────────────────┴────────────────────┴───────────────┘\n</code></pre>\n<p>This is single-table design — multiple entity types co-exist in one table, distinguished by the format of their PK/SK values.</p>\n<h2>Single-Table Design: The Core Principle</h2>\n<p>In relational databases, one entity = one table. In DynamoDB, one access pattern group = one table. Single-table design collapses your entities into one table, enabling:</p>\n<ol>\n<li><strong>Transactional writes</strong> across related entities (DynamoDB transactions require items to be in the same table)</li>\n<li><strong>Query efficiency</strong> — fetching an order with all its items in one request using <code>Query</code></li>\n<li><strong>Cost efficiency</strong> — fewer requests = fewer RCUs consumed</li>\n</ol>\n<p><strong>Access pattern mapping (do this before writing a line of code):</strong></p>\n<table>\n<thead>\n<tr>\n<th>Access Pattern</th>\n<th>Operation</th>\n<th>Key Condition</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Get order by ID</td>\n<td>GetItem</td>\n<td>PK=ORDER#id, SK=ORDER#id</td>\n</tr>\n<tr>\n<td>Get all items in order</td>\n<td>Query</td>\n<td>PK=ORDER#id, SK begins_with ITEM#</td>\n</tr>\n<tr>\n<td>Get all orders for customer</td>\n<td>Query</td>\n<td>PK=CUSTOMER#id, SK begins_with ORDER#</td>\n</tr>\n<tr>\n<td>Get customer profile</td>\n<td>GetItem</td>\n<td>PK=CUSTOMER#id, SK=CUSTOMER#id</td>\n</tr>\n<tr>\n<td>Get order status history</td>\n<td>Query</td>\n<td>PK=ORDER#id, SK begins_with STATUS#</td>\n</tr>\n</tbody>\n</table>\n<p>Each row in this table drives a direct DynamoDB operation. If you can't express a query as a <code>GetItem</code> or <code>Query</code> on your primary key structure, you need a GSI.</p>\n<h2>Global Secondary Indexes: Overloading and Sparse Indexes</h2>\n<p><strong>GSI basics:</strong> A GSI is a separate index with a different partition key (and optional sort key). DynamoDB automatically maintains it. You can query on GSI keys the same way you query the main table.</p>\n<p><strong>GSI overloading</strong> — one GSI serves multiple access patterns:</p>\n<pre><code>GSI: GSI1 (gsi1pk, gsi1sk)\n\nItem type    | gsi1pk                 | gsi1sk          | Access pattern\n-------------|------------------------|-----------------|------------------------\nOrder        | STATUS#pending         | ORDER#2025-01-15| All pending orders by date\nCustomer     | TIER#gold              | CUSTOMER#name   | All gold tier customers\nProduct      | CATEGORY#electronics  | PRODUCT#name    | All electronics products\n</code></pre>\n<p>One GSI answers three different queries — \"all pending orders\", \"gold tier customers\", \"electronics products\" — by using a generic <code>gsi1pk</code>/<code>gsi1sk</code> attribute name that different entity types populate with different values.</p>\n<pre><code class=\"language-javascript\">// DynamoDB SDK v3 (JavaScript/TypeScript):\n\n// All pending orders (most recent first):\nconst pendingOrders = await client.send(new QueryCommand({\n  TableName: 'Orders',\n  IndexName: 'GSI1',\n  KeyConditionExpression: 'gsi1pk = :status',\n  ExpressionAttributeValues: {\n    ':status': 'STATUS#pending'\n  },\n  ScanIndexForward: false  // DESC order\n}));\n\n// Gold tier customers:\nconst goldCustomers = await client.send(new QueryCommand({\n  TableName: 'Orders',\n  IndexName: 'GSI1',\n  KeyConditionExpression: 'gsi1pk = :tier',\n  ExpressionAttributeValues: {\n    ':tier': 'TIER#gold'\n  }\n}));\n</code></pre>\n<p><strong>Sparse indexes</strong> — GSIs only include items that have the GSI key attribute. This creates naturally filtered indexes:</p>\n<pre><code class=\"language-javascript\">// Only active sessions have a TTL and gsi1pk='SESSION#active'\n// Expired sessions have no gsi1pk → not in GSI\n// Result: GSI contains only active sessions — no filter needed\n\nconst activeSessions = await client.send(new QueryCommand({\n  TableName: 'Table',\n  IndexName: 'GSI1',\n  KeyConditionExpression: 'gsi1pk = :active',\n  ExpressionAttributeValues: { ':active': 'SESSION#active' }\n}));\n</code></pre>\n<h2>Adjacency List Pattern: Graph Relationships</h2>\n<p>For many-to-many relationships (e.g., users follow other users, products in multiple categories):</p>\n<pre><code>Following relationship (Twitter-like):\nPK              | SK              | type     | gsi1pk          | gsi1sk\n----------------|-----------------|----------|-----------------|----------------\nUSER#alice      | USER#alice      | USER     | —               | —\nUSER#alice      | FOLLOWS#bob     | FOLLOW   | FOLLOWED_BY#bob | USER#alice\nUSER#alice      | FOLLOWS#carol   | FOLLOW   | FOLLOWED_BY#carol| USER#alice\nUSER#bob        | USER#bob        | USER     | —               | —\nUSER#bob        | FOLLOWS#alice   | FOLLOW   | FOLLOWED_BY#alice| USER#bob\n\nAccess patterns:\n1. \"Who does Alice follow?\"     → Query PK=USER#alice, SK begins_with FOLLOWS#\n2. \"Who follows Bob?\"          → Query GSI1 gsi1pk=FOLLOWED_BY#bob\n3. \"Does Alice follow Bob?\"    → GetItem PK=USER#alice, SK=FOLLOWS#bob\n</code></pre>\n<p>The adjacency list stores the relationship in both directions — direct on the main table (PK of follower), inverted via GSI (queried by followee). No join table needed.</p>\n<h2>DynamoDB Streams and Event-Driven Patterns</h2>\n<p>DynamoDB Streams capture every write (INSERT, MODIFY, REMOVE) as an ordered record per partition. Combine with Lambda for event-driven architectures:</p>\n<pre><code>DynamoDB Streams Architecture:\n\nWrite → DynamoDB Table → Stream (ordered per partition)\n                              ↓\n                         Lambda trigger\n                         (128 shards max)\n                              ↓\n                    Event processing:\n                    - Fan-out notifications\n                    - Update search index (Elasticsearch)\n                    - Replicate to analytics (Kinesis → S3)\n                    - Invalidate cache (ElastiCache)\n                    - Cross-region replication\n</code></pre>\n<pre><code class=\"language-javascript\">// Lambda handler for DynamoDB Stream:\nexport const handler = async (event: DynamoDBStreamEvent) => {\n  for (const record of event.Records) {\n    if (record.eventName === 'INSERT') {\n      const newItem = unmarshall(record.dynamodb!.NewImage!);\n\n      if (newItem.pk?.startsWith('ORDER#')) {\n        // New order created — send confirmation email\n        await emailService.sendOrderConfirmation(newItem);\n        // Update search index\n        await searchService.indexOrder(newItem);\n      }\n    }\n\n    if (record.eventName === 'MODIFY') {\n      const oldItem = unmarshall(record.dynamodb!.OldImage!);\n      const newItem = unmarshall(record.dynamodb!.NewImage!);\n\n      if (oldItem.status !== newItem.status) {\n        // Status changed — trigger downstream workflow\n        await workflowService.onStatusChange(newItem.pk, oldItem.status, newItem.status);\n      }\n    }\n  }\n};\n</code></pre>\n<p><strong>Stream delivery guarantees:</strong></p>\n<ul>\n<li>Each record delivered at least once (exactly-once semantics not guaranteed)</li>\n<li>Records delivered in order per partition key</li>\n<li>Lambda processes shards concurrently — different partition keys may process in parallel</li>\n</ul>\n<p>Design your handlers to be idempotent (safe to call multiple times with the same record).</p>\n<h2>Transactions: ACID Across Multiple Items</h2>\n<p>DynamoDB supports ACID transactions for up to 100 items per transaction:</p>\n<pre><code class=\"language-javascript\">// Transfer funds between accounts (atomic debit + credit):\nconst transferFunds = async (fromAccountId: string, toAccountId: string, amount: number) => {\n  await client.send(new TransactWriteCommand({\n    TransactItems: [\n      {\n        // Debit source account (with condition: sufficient funds)\n        Update: {\n          TableName: 'Accounts',\n          Key: { pk: `ACCOUNT#${fromAccountId}`, sk: `ACCOUNT#${fromAccountId}` },\n          UpdateExpression: 'SET balance = balance - :amount',\n          ConditionExpression: 'balance >= :amount',\n          ExpressionAttributeValues: { ':amount': amount }\n        }\n      },\n      {\n        // Credit destination account\n        Update: {\n          TableName: 'Accounts',\n          Key: { pk: `ACCOUNT#${toAccountId}`, sk: `ACCOUNT#${toAccountId}` },\n          UpdateExpression: 'SET balance = balance + :amount',\n          ExpressionAttributeValues: { ':amount': amount }\n        }\n      },\n      {\n        // Write transaction record\n        Put: {\n          TableName: 'Accounts',\n          Item: {\n            pk: `TRANSFER#${uuid()}`,\n            sk: `TRANSFER#${uuid()}`,\n            fromAccount: fromAccountId,\n            toAccount: toAccountId,\n            amount,\n            createdAt: new Date().toISOString()\n          }\n        }\n      }\n    ]\n  }));\n};\n</code></pre>\n<p>Transactions use OCC (optimistic concurrency control). If any condition fails, the entire transaction is rolled back. Cost: 2× read/write capacity units compared to non-transactional operations.</p>\n<h2>Capacity Planning: Avoiding Bill Shock</h2>\n<p>DynamoDB pricing is based on Read Capacity Units (RCUs) and Write Capacity Units (WCUs):</p>\n<pre><code>1 RCU = 1 strongly consistent read of up to 4KB\n      = 2 eventually consistent reads of up to 4KB\n      = 0.5 transactional reads of up to 4KB\n\n1 WCU = 1 write of up to 1KB\n      = 0.5 transactional writes of up to 1KB\n\nOn-demand pricing (us-east-1):\n$0.25 per million read request units\n$1.25 per million write request units\n\nProvisioned capacity pricing:\n$0.00013 per RCU-hour ($0.0065 per RCU-day)\n$0.00065 per WCU-hour ($0.0325 per WCU-day)\n</code></pre>\n<p><strong>Example calculation:</strong></p>\n<p>E-commerce site: 1,000 orders/hour, each order writes 5 items (order + 3 items + customer update) of ~500 bytes each.</p>\n<pre><code>Writes:\n1,000 orders/hour × 5 items × 1 WCU (500 bytes &#x3C; 1KB)\n= 5,000 WCUs/hour for orders\n\nIf using transactions (2× cost):\n= 10,000 WCUs/hour\n\nMonthly cost (on-demand):\n10,000 WCUs/hour × 720 hours × $1.25/million\n= 7.2M WCUs × $1.25/million = $9/month for writes\n\nReads (query order + items = 1 RCU per request):\nAssume 5× read:write ratio = 25,000 RCUs/hour\nMonthly cost: 18M RCUs × $0.25/million = $4.50/month\n\nTotal: ~$13.50/month for 720,000 orders\n</code></pre>\n<p>At this scale, on-demand is fine. At 10M orders/month, provisioned capacity with auto-scaling is 70-80% cheaper.</p>\n<p><strong>Hot partition detection:</strong></p>\n<pre><code>CloudWatch metric to monitor:\n- ConsumedWriteCapacityUnits (per-partition) — not exposed directly\n- ThrottledRequests > 0 — always indicates a problem\n- SystemErrors — usually indicates hot partitions causing timeouts\n\nIn AWS Console: DynamoDB → Metrics → ConsumedWriteCapacityUnits\nLook for spikes indicating one partition getting disproportionate traffic\n</code></pre>\n<p>Signs of a hot partition:</p>\n<ul>\n<li>Throttling on specific items even though provisioned capacity isn't exhausted globally</li>\n<li>One product/user getting orders of magnitude more traffic than others</li>\n</ul>\n<p>Fix: Add jitter to partition keys (e.g., append a random 0-9 suffix, query all 10 suffixes and merge client-side).</p>\n<h2>TTL: Automatic Data Expiration</h2>\n<p>DynamoDB TTL automatically deletes items past their expiration timestamp — at no cost:</p>\n<pre><code class=\"language-javascript\">// Setting TTL:\nconst expirationTime = Math.floor(Date.now() / 1000) + (30 * 24 * 60 * 60); // 30 days\n\nawait client.send(new PutItemCommand({\n  TableName: 'Sessions',\n  Item: {\n    pk: { S: `SESSION#${sessionId}` },\n    sk: { S: `SESSION#${sessionId}` },\n    userId: { S: userId },\n    ttl: { N: expirationTime.toString() }  // Unix epoch seconds\n  }\n}));\n\n// DynamoDB automatically deletes items when ttl &#x3C; current epoch\n// Deletion happens within 48 hours of expiration\n// Deleted items appear in DynamoDB Streams as REMOVE events (useful for audit)\n</code></pre>\n<p>TTL is the correct pattern for session data, temporary caches, and compliance-driven data retention. Never run a Lambda to purge old data — TTL does it free.</p>\n<h2>When DynamoDB Is Wrong</h2>\n<p>DynamoDB excels at:</p>\n<ul>\n<li>Known, stable access patterns (OLTP workloads)</li>\n<li>Key-value lookups at massive scale</li>\n<li>Event sourcing and time-series data</li>\n<li>Serverless architectures (pay-per-request, zero management)</li>\n</ul>\n<p>DynamoDB is the wrong choice for:</p>\n<ul>\n<li>Ad-hoc analytics (use Athena on S3 exports or Redshift)</li>\n<li>Complex queries with multiple filter conditions on non-indexed attributes</li>\n<li>Flexible schema requirements that change frequently (schema changes in single-table design are painful)</li>\n<li>Teams without DynamoDB expertise (the learning curve is steep and mistakes are expensive)</li>\n</ul>\n<p>The difference between DynamoDB experts and beginners: experts define all access patterns on a whiteboard before writing any code. Every GSI, every overloaded attribute, every entity prefix is a deliberate decision made against a list of queries the application must support. The schema follows the queries — not the other way around.</p>\n","tableOfContents":[{"id":"dynamodbs-data-model","text":"DynamoDB's Data Model","level":2},{"id":"single-table-design-the-core-principle","text":"Single-Table Design: The Core Principle","level":2},{"id":"global-secondary-indexes-overloading-and-sparse-indexes","text":"Global Secondary Indexes: Overloading and Sparse Indexes","level":2},{"id":"adjacency-list-pattern-graph-relationships","text":"Adjacency List Pattern: Graph Relationships","level":2},{"id":"dynamodb-streams-and-event-driven-patterns","text":"DynamoDB Streams and Event-Driven Patterns","level":2},{"id":"transactions-acid-across-multiple-items","text":"Transactions: ACID Across Multiple Items","level":2},{"id":"capacity-planning-avoiding-bill-shock","text":"Capacity Planning: Avoiding Bill Shock","level":2},{"id":"ttl-automatic-data-expiration","text":"TTL: Automatic Data Expiration","level":2},{"id":"when-dynamodb-is-wrong","text":"When DynamoDB Is Wrong","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring — every node is equal, there's no primary, and data placement is determined by partit…"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible — rollback a bad deploy and your code is back to the previous version. Database schema changes are st…"},{"title":"PostgreSQL Performance Tuning: From Slow Queries to Sub-Millisecond Reads","description":"A production guide to PostgreSQL query optimization: EXPLAIN ANALYZE, index design, VACUUM tuning, connection pooling with PgBouncer, partitioning, and the configuration changes that actually move the needle.","date":"2025-06-03","category":"Databases","tags":["postgresql","databases","performance","sql","indexing","query optimization"],"featured":false,"affiliateSection":"database-resources","slug":"postgresql-performance-tuning","readingTime":"9 min read","excerpt":"PostgreSQL ships with defaults tuned for a 512MB machine from 2005. Every production deployment needs to be re-tuned. Beyond that, most slow queries are not a PostgreSQL problem — they're a query design problem that Post…"}]},"__N_SSG":true}