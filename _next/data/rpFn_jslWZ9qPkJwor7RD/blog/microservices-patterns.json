{"pageProps":{"post":{"title":"Microservices Patterns: Circuit Breaker, Retry, Bulkhead, and Saga","description":"Master the resilience patterns that keep microservices systems running when individual services fail. Covers circuit breaker, retry with backoff, bulkhead isolation, and distributed transactions with Saga.","date":"2025-02-28","category":"System Design","tags":["microservices","resilience","circuit breaker","saga","distributed systems","spring boot"],"featured":false,"affiliateSection":"system-design-courses","slug":"microservices-patterns","readingTime":"14 min read","excerpt":"A monolith fails as a unit — one process, one crash, everything stops. A microservices system fails differently: some services go down, some slow to a crawl, some remain perfectly healthy. This partial-failure behaviour …","contentHtml":"<p>A monolith fails as a unit — one process, one crash, everything stops. A microservices system fails differently: some services go down, some slow to a crawl, some remain perfectly healthy. This partial-failure behaviour is actually harder to deal with than total failure, because the system is still <em>partially</em> up and clients keep sending requests into the degraded parts.</p>\n<p>Without resilience patterns, one slow service can bring down your entire system in minutes through a mechanism called cascading failure. With the right patterns applied correctly, the system degrades gracefully — the slow payment service becomes temporarily unavailable to users, while inventory browsing, cart management, and search continue working normally. This article covers the four patterns that make that possible, with Spring Boot + Resilience4j implementations.</p>\n<h2>Why Microservices Fail Differently</h2>\n<p>The most dangerous failure mode in distributed systems is not an immediate crash — it's a slow response. A service that returns an error immediately frees the calling thread right away. A service that hangs for 10 seconds holds a thread for 10 seconds, and at high load those threads accumulate until the thread pool is exhausted.</p>\n<pre><code>Monolith failure: Everything fails at once (simple, but total)\n\nMicroservice failure cascade:\n  Order Service calls Inventory Service calls Warehouse Service\n\n  Warehouse Service goes slow (200ms → 10s response time)\n  Inventory Service threads pile up waiting for Warehouse\n  Order Service threads pile up waiting for Inventory\n  All three services become unresponsive\n  → Entire system down, from one slow service\n\nThis is \"cascading failure\" — the #1 microservices operational problem.\n</code></pre>\n<p>The patterns below address cascading failure at different levels. The circuit breaker stops calls to a failing service. Retry handles transient blips before they reach the circuit breaker. The bulkhead contains failures to one partition of the system. Saga coordinates the cleanup when a multi-step operation fails partway through.</p>\n<h2>Pattern 1: Circuit Breaker</h2>\n<p>The circuit breaker is named after the electrical component that trips when a circuit overloads, preventing damage. The software version does the same: when calls to a downstream service start failing at a high rate, the circuit breaker <strong>opens</strong> and immediately returns an error to callers — without actually calling the downstream service. This gives the failing service breathing room to recover while protecting the calling service's threads.</p>\n<p>The three states are the heart of the pattern:</p>\n<pre><code>Circuit states:\n  CLOSED: Normal operation — calls pass through\n  OPEN: Failure threshold exceeded — calls fail immediately (fast fail)\n  HALF-OPEN: Trial period — limited calls allowed to test recovery\n\nState transitions:\n  CLOSED → OPEN: When failure rate > threshold (e.g., 50% of last 10 calls failed)\n  OPEN → HALF-OPEN: After wait duration (e.g., 30 seconds)\n  HALF-OPEN → CLOSED: If trial calls succeed\n  HALF-OPEN → OPEN: If trial calls fail\n\nTimeline:\n  0s:  Normal. All calls succeed.\n  30s: Downstream service starts failing.\n  35s: Failure rate hits 50% → Circuit OPENS.\n  35s-65s: All calls fail immediately (fast fail), downstream gets no load.\n  65s: Circuit HALF-OPENS, 3 trial calls allowed.\n  66s: Trial calls succeed → Circuit CLOSES.\n  66s+: Normal operation resumes.\n</code></pre>\n<p>The HALF-OPEN state is subtle but important. Without it, a circuit that opens would never close — you'd need manual intervention to restore service. HALF-OPEN is the automatic recovery probe: after the wait duration, the circuit allows a small number of test calls through. If they succeed, normal operation resumes. If they fail, the circuit opens again and waits longer.</p>\n<p>The <code>@CircuitBreaker</code> annotation in Resilience4j wires all of this up automatically. The <code>fallbackMethod</code> is the method called when the circuit is open or when all retries are exhausted — it's your graceful degradation path.</p>\n<pre><code class=\"language-java\">// build.gradle\n// implementation 'io.github.resilience4j:resilience4j-spring-boot3:2.2.0'\n\n@Service\npublic class InventoryService {\n\n    private final CircuitBreakerRegistry circuitBreakerRegistry;\n    private final InventoryClient inventoryClient;\n\n    public InventoryService(CircuitBreakerRegistry registry, InventoryClient client) {\n        this.circuitBreakerRegistry = registry;\n        this.inventoryClient = client;\n    }\n\n    @CircuitBreaker(name = \"inventory\", fallbackMethod = \"getInventoryFallback\")\n    public InventoryResponse checkInventory(String productId) {\n        return inventoryClient.check(productId);\n    }\n\n    // The fallback runs when: circuit is OPEN, or a call fails (and no retry is configured)\n    // Its signature must match the original method plus an Exception parameter\n    public InventoryResponse getInventoryFallback(String productId, Exception e) {\n        log.warn(\"Circuit breaker activated for inventory service: {}\", e.getMessage());\n        // Good fallback options: return stale cache, return \"UNKNOWN\" status,\n        // or show a UI message like \"Availability not currently shown\"\n        return inventoryCache.getLastKnown(productId)\n            .orElse(new InventoryResponse(productId, AvailabilityStatus.UNKNOWN, 0));\n    }\n}\n</code></pre>\n<p>The YAML configuration gives you fine-grained control over when the circuit trips. The <code>slow-call-rate-threshold</code> is particularly useful — a service that responds in 8 seconds is just as harmful as one that throws exceptions, and the circuit can open for slowness, not just errors.</p>\n<pre><code class=\"language-yaml\"># application.yml\nresilience4j:\n  circuitbreaker:\n    instances:\n      inventory:\n        sliding-window-type: COUNT_BASED\n        sliding-window-size: 10              # Evaluate the last 10 calls\n        failure-rate-threshold: 50           # Open when 5 of last 10 calls fail\n        slow-call-rate-threshold: 80         # Also open when 8 of 10 calls are slow\n        slow-call-duration-threshold: 3s     # \"Slow\" means > 3 seconds\n        permitted-number-of-calls-in-half-open-state: 3\n        wait-duration-in-open-state: 30s\n        register-health-indicator: true      # Exposes circuit state in /actuator/health\n</code></pre>\n<p>With <code>register-health-indicator: true</code>, your Spring Boot <code>/actuator/health</code> endpoint will show the current state of each circuit breaker. This is invaluable during incidents — you can see immediately whether a circuit is open and which downstream service is causing it.</p>\n<h2>Pattern 2: Retry with Exponential Backoff</h2>\n<p>Not all failures are meaningful. A network packet gets dropped, a connection pool momentarily exhausts, a cloud provider briefly throttles a request — these happen in production every day and resolve themselves within milliseconds or seconds. Retry with backoff handles this class of failure automatically.</p>\n<p>The key word is <strong>exponential</strong> backoff. A flat retry interval (retry every 500ms) keeps hammering the service at the same rate. Exponential backoff doubles the wait time between each retry attempt, giving the downstream service progressively more time to recover: 500ms, then 1 second, then 2 seconds. This self-limiting behaviour is why exponential backoff is the industry standard.</p>\n<p>The code below shows combining <code>@Retry</code> with <code>@CircuitBreaker</code> — a common production pattern. The retry fires first (for transient failures), and the circuit breaker wraps the entire thing (for persistent failures):</p>\n<pre><code class=\"language-java\">@Service\npublic class PaymentService {\n\n    // Retry is applied \"inside\" the circuit breaker:\n    // 1. If a call fails, retry up to 3 times (Retry)\n    // 2. If failure rate across all attempts exceeds threshold, open circuit (CircuitBreaker)\n    @Retry(name = \"payment\", fallbackMethod = \"paymentFallback\")\n    @CircuitBreaker(name = \"payment\")\n    public PaymentResult charge(PaymentRequest request) {\n        return paymentClient.charge(request);\n    }\n\n    // Called only after all retry attempts are exhausted\n    // Returning \"pending\" is better than failing outright — the payment can be retried async\n    public PaymentResult paymentFallback(PaymentRequest request, Exception e) {\n        asyncRetryQueue.enqueue(request);\n        return PaymentResult.pending(request.getOrderId(), \"Payment queued for retry\");\n    }\n}\n</code></pre>\n<p>The YAML configuration for retry is where the subtlety lives. Two decisions matter enormously here: which exceptions to retry (network errors, not business errors), and whether to use jitter.</p>\n<pre><code class=\"language-yaml\">resilience4j:\n  retry:\n    instances:\n      payment:\n        max-attempts: 3\n        wait-duration: 500ms\n        enable-exponential-backoff: true\n        exponential-backoff-multiplier: 2      # Retry at 500ms, 1s, 2s\n        exponential-max-wait-duration: 10s\n        retry-exceptions:\n          - java.net.ConnectException          # Network-level failures — safe to retry\n          - java.net.SocketTimeoutException\n          - feign.RetryableException\n        ignore-exceptions:\n          - com.example.PaymentDeclinedException  # Business failure — retrying won't help\n          - com.example.DuplicatePaymentException # Idempotency error — don't retry!\n        randomized-wait-factor: 0.5            # Adds jitter: ±50% of the wait time\n</code></pre>\n<p><strong>Why <code>ignore-exceptions</code> matters</strong>: A <code>PaymentDeclinedException</code> means the card was declined — retrying three times won't change that outcome, and charging the card three times creates a terrible user experience. Always separate retryable infrastructure errors from non-retryable business errors.</p>\n<p><strong>Jitter is critical</strong>: Without jitter, all retrying clients retry at the same time, creating a \"thundering herd\" that overwhelms the recovering service.</p>\n<pre><code>Without jitter (thundering herd):\n  T=500ms: All 1000 clients retry simultaneously → service gets 1000 requests at once\n\nWith jitter (spread load):\n  T=250-750ms: Clients retry randomly in this window → ~4 requests per millisecond\n</code></pre>\n<p>With <code>randomized-wait-factor: 0.5</code>, the 500ms wait becomes anywhere from 250ms to 750ms — a small change that dramatically reduces retry storm load on the recovering service.</p>\n<h2>Pattern 3: Bulkhead — Isolation</h2>\n<p>The ship's bulkhead divides the hull into watertight compartments. When one compartment floods, the others remain dry and the ship stays afloat. The software pattern does exactly this for thread pools.</p>\n<p>Without bulkheads, all downstream calls compete for the same shared thread pool. When the payment service goes slow and its threads don't return, they're borrowed from the pool indefinitely. Eventually the pool is exhausted and every service call fails — even inventory checks that have nothing to do with payment.</p>\n<pre><code>Without bulkhead:\n  Thread pool: 200 threads total\n  Slow payment service consumes all 200 threads\n  → No threads left for inventory, user, order services\n  → Entire system unresponsive\n\nWith bulkhead:\n  Payment thread pool: 20 threads (separate pool)\n  Other services share remaining 180 threads\n  → Payment slowness isolated; other services unaffected\n</code></pre>\n<p>The implementation below uses <code>Bulkhead.Type.THREADPOOL</code>, which gives each downstream service its own dedicated thread pool. Even if payment service threads are all blocked waiting, inventory threads are in a completely separate pool and continue working.</p>\n<pre><code class=\"language-java\">@Service\npublic class OrderOrchestrator {\n\n    // THREADPOOL bulkhead: each downstream service gets its own isolated thread pool\n    // If payment service blocks all 20 of its threads, inventory is unaffected\n    @Bulkhead(name = \"payment\", type = Bulkhead.Type.THREADPOOL)\n    @CircuitBreaker(name = \"payment\")\n    public CompletableFuture&#x3C;PaymentResult> processPayment(PaymentRequest request) {\n        return CompletableFuture.supplyAsync(() -> paymentClient.charge(request));\n    }\n\n    @Bulkhead(name = \"inventory\", type = Bulkhead.Type.THREADPOOL)\n    @CircuitBreaker(name = \"inventory\")\n    public CompletableFuture&#x3C;InventoryResult> checkInventory(String productId) {\n        return CompletableFuture.supplyAsync(() -> inventoryClient.check(productId));\n    }\n\n    public OrderResult createOrder(OrderRequest request) throws Exception {\n        // Fan-out: both calls start simultaneously, each in their own bulkhead pool\n        // Total time = max(payment_time, inventory_time), not payment_time + inventory_time\n        CompletableFuture&#x3C;PaymentResult> paymentFuture = processPayment(request.getPayment());\n        CompletableFuture&#x3C;InventoryResult> inventoryFuture = checkInventory(request.getProductId());\n\n        // Wait for both to complete before proceeding\n        CompletableFuture.allOf(paymentFuture, inventoryFuture).join();\n\n        return buildOrder(paymentFuture.get(), inventoryFuture.get());\n    }\n}\n</code></pre>\n<p>The <code>queue-capacity</code> setting in the YAML below is your overflow valve. When all threads in the pool are busy, new requests queue up to this limit before being rejected. Size it based on how long callers can reasonably wait and how many concurrent requests you expect.</p>\n<pre><code class=\"language-yaml\">resilience4j:\n  thread-pool-bulkhead:\n    instances:\n      payment:\n        max-thread-pool-size: 20           # Maximum concurrent payment calls\n        core-thread-pool-size: 5           # Always-warm thread count\n        queue-capacity: 50                 # Queue up to 50 requests before rejecting\n        keep-alive-duration: 20ms\n      inventory:\n        max-thread-pool-size: 30           # Inventory is called more frequently\n        core-thread-pool-size: 10\n        queue-capacity: 100\n</code></pre>\n<p>Size your bulkhead pools based on observed concurrency, not guesswork. Run a load test, check how many threads are typically in use, and set the maximum pool to 20-30% above peak. This leaves headroom for spikes while still containing failures.</p>\n<h2>Pattern 4: Saga — Distributed Transactions</h2>\n<p>The trickiest failure scenario in microservices: a multi-step business operation that spans several services. Consider order creation — you need to reserve inventory, charge the customer, and confirm the order. In a monolith, you'd wrap all of this in a database transaction and get atomicity for free. In microservices, each service has its own database — there's no shared transaction to roll back.</p>\n<p>The Saga pattern solves this by breaking the operation into a sequence of <strong>local transactions</strong>, each followed by a <strong>compensating transaction</strong> that reverses the step if a later step fails.</p>\n<pre><code>Order creation saga (Choreography-based):\n\nStep 1: Order Service creates order (PENDING)\n    → publishes \"OrderCreated\" event\n\nStep 2: Inventory Service reserves stock\n    → publishes \"StockReserved\" event\n    (if fails → publishes \"StockReservationFailed\")\n\nStep 3: Payment Service charges customer\n    → publishes \"PaymentProcessed\" event\n    (if fails → publishes \"PaymentFailed\")\n\nStep 4: Order Service updates order to CONFIRMED\n    → publishes \"OrderConfirmed\"\n\nFailure handling (compensating transactions):\n  PaymentFailed →\n    Inventory Service: release reserved stock (compensation)\n    Order Service: mark order as CANCELLED\n\n  StockReservationFailed →\n    Order Service: mark order as CANCELLED (no payment taken yet)\n</code></pre>\n<p>There are two ways to implement Sagas: <strong>choreography</strong> and <strong>orchestration</strong>. Understanding the difference is essential for choosing the right approach.</p>\n<p><strong>Choreography</strong> has no central coordinator — each service reacts to events and publishes its own. Services are loosely coupled and can evolve independently. The downside is that the overall business flow is difficult to visualise — it's spread across multiple services, and tracing a failure requires correlating events from all of them.</p>\n<p><strong>Orchestration</strong> has a central saga orchestrator that explicitly tells each service what to do next. The entire business flow lives in one place, making it much easier to understand and debug. The trade-off is a coordination point that must be kept resilient.</p>\n<pre><code class=\"language-java\">// === CHOREOGRAPHY: Each service reacts to events independently ===\n\n@Service\npublic class InventoryService {\n\n    // Listen for orders, reserve stock, emit result\n    @KafkaListener(topics = \"order-events\", filter = \"OrderCreated\")\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        try {\n            reserveStock(event.getProductId(), event.getQuantity());\n            // Success: tell the next participant (Payment Service) to proceed\n            kafkaTemplate.send(\"inventory-events\",\n                new StockReservedEvent(event.getOrderId(), event.getProductId()));\n        } catch (InsufficientStockException e) {\n            // Failure: publish a failure event so the saga can compensate\n            kafkaTemplate.send(\"inventory-events\",\n                new StockReservationFailedEvent(event.getOrderId(), e.getMessage()));\n        }\n    }\n\n    // Compensating transaction: runs if payment fails AFTER stock was reserved\n    // This is what \"undoes\" the stock reservation to keep data consistent\n    @KafkaListener(topics = \"payment-events\", filter = \"PaymentFailed\")\n    public void handlePaymentFailed(PaymentFailedEvent event) {\n        releaseStock(event.getOrderId());\n        log.info(\"Released stock for failed order {}\", event.getOrderId());\n    }\n}\n\n// === ORCHESTRATION: A central orchestrator coordinates the flow ===\n\n@Service\npublic class OrderSagaOrchestrator {\n\n    @SagaOrchestrationStart\n    public void createOrder(OrderRequest request) {\n        // The orchestrator explicitly tracks saga state in the database\n        // This gives you a single place to see the status of any in-flight order\n        OrderSaga saga = OrderSaga.builder()\n            .orderId(UUID.randomUUID().toString())\n            .request(request)\n            .state(SagaState.STARTED)\n            .build();\n\n        sagaRepository.save(saga);\n\n        // The orchestrator calls each service in sequence and handles outcomes\n        inventoryService.reserve(saga.getOrderId(), request.getProductId(), request.getQuantity())\n            .onSuccess(result -> {\n                saga.setState(SagaState.INVENTORY_RESERVED);\n                sagaRepository.save(saga);  // Persist progress — survives restarts\n                paymentService.charge(saga.getOrderId(), request.getPayment())\n                    .onSuccess(payResult -> confirmOrder(saga))\n                    .onFailure(e -> compensateInventory(saga, e));  // Run compensation explicitly\n            })\n            .onFailure(e -> cancelOrder(saga, \"Insufficient stock: \" + e.getMessage()));\n    }\n\n    private void compensateInventory(OrderSaga saga, Exception e) {\n        // Explicitly undo the previous step\n        inventoryService.release(saga.getOrderId());\n        cancelOrder(saga, \"Payment failed: \" + e.getMessage());\n    }\n}\n</code></pre>\n<p>The orchestration approach saves saga state to the database after each step. This means if the orchestrator process crashes mid-saga, a new instance can pick up from the last saved state and continue from where it left off — rather than starting over and potentially double-charging customers.</p>\n<p><strong>Which to choose?</strong> For a small number of services with clear ownership, choreography's loose coupling is appealing. For complex flows involving many services, or whenever you need visibility into \"where did this order get stuck?\", orchestration is worth the added coordination. Most production systems with 4+ saga participants use orchestration.</p>\n<h2>Combining Patterns: The Full Resilience Stack</h2>\n<p>In production, you combine all four patterns for critical service calls. The annotation order matters because Resilience4j applies them inside-out:</p>\n<pre><code class=\"language-java\">// Read the annotations from innermost to outermost:\n// Bulkhead → TimeLimiter → CircuitBreaker → Retry\n// 1. Bulkhead: assigns this call to the payment thread pool\n// 2. TimeLimiter: cuts the call off if it runs longer than N seconds\n// 3. CircuitBreaker: tracks failures and opens the circuit if threshold is exceeded\n// 4. Retry: if the call fails, tries again (up to max-attempts) before the circuit records it\n@CircuitBreaker(name = \"payment\", fallbackMethod = \"paymentFallback\")\n@Retry(name = \"payment\")\n@Bulkhead(name = \"payment\", type = Bulkhead.Type.THREADPOOL)\n@TimeLimiter(name = \"payment\")\npublic CompletableFuture&#x3C;PaymentResult> chargeCustomer(PaymentRequest request) {\n    return CompletableFuture.supplyAsync(() -> paymentClient.charge(request));\n}\n</code></pre>\n<p>Why this order? Retry wraps around CircuitBreaker means retries can occur even when the circuit is open — which defeats the purpose. The correct order ensures: the Bulkhead limits concurrent threads first, TimeLimiter enforces a hard deadline, CircuitBreaker accumulates failure statistics, and Retry attempts recovery before declaring a failure to the circuit breaker.</p>\n<p><strong>Resilience pattern decision guide:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Pattern</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Transient (network blip)</td>\n<td>Retry with backoff</td>\n</tr>\n<tr>\n<td>Persistent downstream failure</td>\n<td>Circuit Breaker</td>\n</tr>\n<tr>\n<td>Resource exhaustion</td>\n<td>Bulkhead</td>\n</tr>\n<tr>\n<td>Long response times</td>\n<td>Timeout + Circuit Breaker</td>\n</tr>\n<tr>\n<td>Multi-step distributed operation</td>\n<td>Saga</td>\n</tr>\n<tr>\n<td>All of the above</td>\n<td>All of the above</td>\n</tr>\n</tbody>\n</table>\n<p>The key insight: <strong>resilience is not about preventing failures, it's about controlling how failures propagate</strong>. Every distributed system will experience failures — the question is whether a payment service outage takes down your entire platform or just the checkout flow. These four patterns answer that question.</p>\n<p>Start with the circuit breaker and retry — they cover the majority of failure scenarios with minimal complexity. Add bulkheads when you identify that one slow service is stealing resources from others. Add sagas when you have multi-step operations that need compensating logic. Apply them incrementally, measure the impact, and only add the next layer when the data shows you need it.</p>\n","tableOfContents":[{"id":"why-microservices-fail-differently","text":"Why Microservices Fail Differently","level":2},{"id":"pattern-1-circuit-breaker","text":"Pattern 1: Circuit Breaker","level":2},{"id":"pattern-2-retry-with-exponential-backoff","text":"Pattern 2: Retry with Exponential Backoff","level":2},{"id":"pattern-3-bulkhead-isolation","text":"Pattern 3: Bulkhead — Isolation","level":2},{"id":"pattern-4-saga-distributed-transactions","text":"Pattern 4: Saga — Distributed Transactions","level":2},{"id":"combining-patterns-the-full-resilience-stack","text":"Combining Patterns: The Full Resilience Stack","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why — by exploring system state through metrics, traces, and logs without needing to know in advance…"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory — store events instead of state, derive state by replaying events — is sou…"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t…"}]},"__N_SSG":true}