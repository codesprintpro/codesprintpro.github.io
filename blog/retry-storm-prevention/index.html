<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Designing a Retry System Without Causing a Retry Storm<!-- --> | CodeSprintPro</title><meta name="description" content="Exponential backoff with jitter, circuit breakers, bulkhead isolation, Kafka retry topics, and the retry amplification problem â€” with Java implementations and a real outage postmortem." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/retry-storm-prevention/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Designing a Retry System Without Causing a Retry Storm" data-next-head=""/><meta property="og:description" content="Exponential backoff with jitter, circuit breakers, bulkhead isolation, Kafka retry topics, and the retry amplification problem â€” with Java implementations and a real outage postmortem." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/retry-storm-prevention/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-05-22" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="System Design" data-next-head=""/><meta property="article:tag" content="retry" data-next-head=""/><meta property="article:tag" content="circuit breaker" data-next-head=""/><meta property="article:tag" content="resilience" data-next-head=""/><meta property="article:tag" content="spring boot" data-next-head=""/><meta property="article:tag" content="kafka" data-next-head=""/><meta property="article:tag" content="distributed systems" data-next-head=""/><meta property="article:tag" content="java" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Designing a Retry System Without Causing a Retry Storm" data-next-head=""/><meta name="twitter:description" content="Exponential backoff with jitter, circuit breakers, bulkhead isolation, Kafka retry topics, and the retry amplification problem â€” with Java implementations and a real outage postmortem." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Designing a Retry System Without Causing a Retry Storm","description":"Exponential backoff with jitter, circuit breakers, bulkhead isolation, Kafka retry topics, and the retry amplification problem â€” with Java implementations and a real outage postmortem.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-05-22","dateModified":"2025-05-22","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/retry-storm-prevention/"},"keywords":"retry, circuit breaker, resilience, spring boot, kafka, distributed systems, java","articleSection":"System Design"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Designing a Retry System Without Causing a Retry Storm</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">System Design</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Designing a Retry System Without Causing a Retry Storm</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Exponential backoff with jitter, circuit breakers, bulkhead isolation, Kafka retry topics, and the retry amplification problem â€” with Java implementations and a real outage postmortem.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>Â·</span><span>May 22, 2025</span><span>Â·</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->retry</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->circuit breaker</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->resilience</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->spring boot</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->kafka</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->distributed systems</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->java</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Retry logic is the second most dangerous code in a distributed system, after "delete all records." The intent is to improve reliability by recovering from transient failures. The actual effect, when implemented naively, is to turn a brief service degradation into a cascading system-wide outage.</p>
<p>The pattern is predictable: a downstream service slows down, client retries pile up, the downstream service is now handling 3Ã— the original load while already struggling, it slows down more, more retries, complete failure. A retry storm.</p>
<p>This article covers every layer of the retry stack â€” from jitter algorithms to Kafka DLQ topology â€” and the production outage that reshaped how our team thinks about retry.</p>
<h2>Exponential Backoff: Why Linear Retry Is Wrong</h2>
<p>Linear retry: wait 1 second, then try again. If the service is down for 60 seconds and you have 1,000 clients, each retrying every second, that's 1,000 Ã— 60 = 60,000 retry requests during the outage. When the service recovers, it receives all 1,000 pending retries simultaneously.</p>
<p>Exponential backoff: each retry waits 2Ã— longer than the previous:</p>
<pre><code>Attempt 1: wait 1s
Attempt 2: wait 2s
Attempt 3: wait 4s
Attempt 4: wait 8s
Attempt 5: wait 16s â†’ give up
</code></pre>
<p>Total client load during a 60-second outage with exponential backoff: 5 retries per client Ã— 1,000 clients = 5,000 requests â€” 12Ã— fewer than linear retry.</p>
<p>But exponential backoff alone still causes a thundering herd on recovery: all 1,000 clients synchronized on the same backoff schedule will all retry at t=1s, t=2s, t=4s simultaneously.</p>
<h2>Jitter: The Fix for Synchronized Retries</h2>
<p>Jitter adds randomness to the wait time, spreading retries across a time window:</p>
<pre><code class="language-java">public class ExponentialBackoffWithJitter {

    private final int maxAttempts;
    private final long baseDelayMs;
    private final long maxDelayMs;
    private final double jitterFactor;

    // Full jitter: random(0, min(maxDelay, baseDelay * 2^attempt))
    public long computeDelay(int attempt) {
        long exponentialDelay = (long) (baseDelayMs * Math.pow(2, attempt));
        long cappedDelay = Math.min(maxDelayMs, exponentialDelay);
        return ThreadLocalRandom.current().nextLong(0, cappedDelay);
    }

    // Equal jitter: split between base and random portion
    // Guarantees minimum wait while still spreading load
    public long computeEqualJitterDelay(int attempt) {
        long exponentialDelay = (long) (baseDelayMs * Math.pow(2, attempt));
        long capped = Math.min(maxDelayMs, exponentialDelay);
        return (capped / 2) + ThreadLocalRandom.current().nextLong(0, capped / 2);
    }

    // Decorrelated jitter (AWS recommendation): harder to reason about but best distribution
    public long computeDecorrelatedDelay(int attempt, long previousDelay) {
        return Math.min(maxDelayMs,
            ThreadLocalRandom.current().nextLong(baseDelayMs, previousDelay * 3));
    }
}
</code></pre>
<p><strong>Which jitter to use:</strong></p>
<ul>
<li>Full jitter: best load distribution, minimum guaranteed wait is 0 (acceptable for most cases)</li>
<li>Equal jitter: ensures some minimum delay, good for avoiding hammering on immediate retry</li>
<li>Decorrelated jitter: AWS's recommended approach, best statistical distribution</li>
</ul>
<h2>Circuit Breaker</h2>
<p>The circuit breaker prevents retrying a service that's known to be down. Instead of each client independently discovering the service is down, the circuit breaker shares this knowledge:</p>
<pre><code>Circuit Breaker States:

CLOSED (normal):
  Requests flow through
  Failure rate monitored
  If failure rate > threshold â†’ open circuit
         â”‚
         â–¼
OPEN (service is down):
  All requests rejected immediately (fail fast)
  No requests sent to downstream
  After timeout â†’ move to half-open
         â”‚
         â–¼
HALF-OPEN (testing recovery):
  Limited requests allowed through
  If they succeed â†’ close circuit
  If they fail â†’ re-open circuit
</code></pre>
<pre><code class="language-java">@Bean
public CircuitBreaker paymentCircuitBreaker(CircuitBreakerRegistry registry) {
    CircuitBreakerConfig config = CircuitBreakerConfig.custom()
        .slidingWindowType(SlidingWindowType.TIME_BASED)
        .slidingWindowSize(30)                           // 30 seconds window
        .minimumNumberOfCalls(10)                        // Min calls before evaluating
        .failureRateThreshold(50)                        // Open at 50% failure rate
        .slowCallRateThreshold(80)                       // Also open at 80% slow calls
        .slowCallDurationThreshold(Duration.ofSeconds(2))
        .waitDurationInOpenState(Duration.ofSeconds(30)) // Stay open 30s
        .permittedNumberOfCallsInHalfOpenState(5)
        .recordExceptions(IOException.class, TimeoutException.class,
                         ServiceUnavailableException.class)
        .ignoreExceptions(ValidationException.class,    // Don't count business logic failures
                         AuthenticationException.class)
        .build();

    return registry.circuitBreaker("payment-service", config);
}

// Usage:
public PaymentResult charge(PaymentRequest request) {
    return circuitBreaker.executeSupplier(() -> {
        return paymentClient.charge(request);
    });
}
</code></pre>
<p>The <code>slowCallDurationThreshold</code> is critical and often missed. A service that responds in 5 seconds is not "failing" â€” the exception-based circuit breaker stays closed. But 5-second calls are exhausting your thread pool. Trip the circuit breaker on slow calls, not just errors.</p>
<h2>Bulkhead Pattern</h2>
<p>Bulkheads isolate failure domains. Without bulkheads, a slow downstream service exhausts all your thread pool capacity, taking down unrelated functionality.</p>
<pre><code>Without bulkhead:
All requests share 200 Tomcat threads
Payment service slow â†’ 200 threads busy waiting for payments
All other endpoints (search, profile, cart) â†’ timeout

With bulkhead:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tomcat (200 total threads)                  â”‚
â”‚                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚ â”‚ Payment pool â”‚  â”‚ Search pool  â”‚          â”‚
â”‚ â”‚ 30 threads   â”‚  â”‚ 50 threads   â”‚          â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚ â”‚ Profile pool â”‚  â”‚ Cart pool    â”‚          â”‚
â”‚ â”‚ 20 threads   â”‚  â”‚ 20 threads   â”‚          â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<pre><code class="language-java">// Resilience4j ThreadPoolBulkhead:
@Bean
public ThreadPoolBulkhead paymentBulkhead(ThreadPoolBulkheadRegistry registry) {
    ThreadPoolBulkheadConfig config = ThreadPoolBulkheadConfig.custom()
        .maxThreadPoolSize(30)
        .coreThreadPoolSize(15)
        .queueCapacity(20)          // Queue depth before rejecting
        .keepAliveDuration(Duration.ofSeconds(20))
        .build();

    return registry.bulkhead("payment", config);
}

public CompletableFuture&#x3C;PaymentResult> chargeAsync(PaymentRequest request) {
    return paymentBulkhead.executeSupplier(() ->
        CompletableFuture.supplyAsync(() -> paymentClient.charge(request))
    );
}
</code></pre>
<p>When the payment service is slow, only the 30 payment threads are affected. Search, profile, and cart keep running with their own thread pools.</p>
<h2>Dead Letter Queues</h2>
<p>Messages that consistently fail need to go somewhere that isn't "try again forever." DLQ design:</p>
<pre><code>DLQ requirements:
1. Not lost (durable storage)
2. Observable (alert on DLQ message arrival)
3. Reprocessable (ability to replay after fix)
4. Auditable (track when message failed and why)

DLQ record schema:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ original_payload: &#x3C;message bytes>      â”‚
â”‚ original_topic: "payments"             â”‚
â”‚ original_partition: 7                  â”‚
â”‚ original_offset: 10034567              â”‚
â”‚ failure_count: 4                       â”‚
â”‚ last_failure_time: 2025-04-15T14:32:00 â”‚
â”‚ last_error: "GatewayTimeoutException"  â”‚
â”‚ last_error_trace: "..."               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2>Kafka Retry Topics</h2>
<p>Kafka's at-least-once delivery means consumers must handle retries. The naive approach â€” retry in the consumer loop â€” holds the partition and blocks other messages. Use a retry topic pattern instead:</p>
<pre><code>Kafka Retry Topology:

payments (main)
    â”‚
    â–¼
Consumer (payments-group)
    â”‚
    â”œâ”€â”€ Success â†’ ack, continue
    â”‚
    â”œâ”€â”€ Retryable failure (1st time)
    â”‚       â””â”€â”€ Publish to payments-retry-30s
    â”‚
    â””â”€â”€ Non-retryable â†’ payments-dlq (immediately)

payments-retry-30s
    â”‚  Consumer pauses 30s before consuming
    â–¼
Consumer (retry-group-30s)
    â”‚
    â”œâ”€â”€ Success â†’ ack
    â”œâ”€â”€ Retryable â†’ payments-retry-5m
    â””â”€â”€ Max retries â†’ payments-dlq

payments-retry-5m â†’ payments-retry-30m â†’ payments-dlq
</code></pre>
<pre><code class="language-java">@RetryableTopic(
    attempts = "4",
    backoff = @Backoff(
        delay = 30_000,           // 30 seconds initial
        multiplier = 6,           // Ã— 6 each attempt: 30s, 3m, 18m
        maxDelay = 1_800_000      // Cap at 30 minutes
    ),
    dltStrategy = DltStrategy.FAIL_ON_ERROR,
    autoCreateTopics = "false",   // Create topics via IaC, not code
    include = {
        RetryablePaymentException.class,
        GatewayTimeoutException.class
    },
    exclude = {
        NonRetryablePaymentException.class,
        InvalidRequestException.class
    }
)
@KafkaListener(topics = "payments", groupId = "payment-processor")
public void process(ConsumerRecord&#x3C;String, PaymentEvent> record) {
    paymentService.process(record.value());
}

@DltHandler
public void handleDeadLetter(
        PaymentEvent event,
        @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
        @Header(KafkaHeaders.EXCEPTION_FQCN) String exceptionFqcn) {
    deadLetterService.record(event, topic, exceptionFqcn);
    alertingService.notifyDltArrival(event);
}
</code></pre>
<h2>Idempotency Considerations</h2>
<p>Retries without idempotent consumers cause duplicate processing. Every message that might be retried must be safe to process twice:</p>
<pre><code class="language-java">@Transactional
public void processPayment(PaymentEvent event) {
    // Idempotency check upfront
    if (processedPayments.existsByEventId(event.getEventId())) {
        log.info("Duplicate event skipped: {}", event.getEventId());
        return;
    }

    // Mark as processing (atomic insert, fails on duplicate)
    processedPayments.markProcessing(event.getEventId());

    try {
        PaymentResult result = gateway.charge(event);
        processedPayments.markComplete(event.getEventId(), result);
    } catch (Exception e) {
        processedPayments.markFailed(event.getEventId(), e.getMessage());
        throw e; // Re-throw for retry
    }
}
</code></pre>
<h2>Retry Amplification Problem</h2>
<p>Each service in a call chain that retries independently amplifies the total request count:</p>
<pre><code>Service A calls B calls C calls D
Each service retries 3 times on failure

D fails:
C retries D 3 times â†’ 3 calls to D
B retries C 3 times â†’ 3 Ã— 3 = 9 calls to D
A retries B 3 times â†’ 3 Ã— 3 Ã— 3 = 27 calls to D

27 calls to D for 1 user request
At 1,000 concurrent users: 27,000 calls to D
</code></pre>
<p>The fix: <strong>retry at the edge, not in the interior of a call chain</strong>. Services B and C should not retry â€” they should propagate errors back to A. A (the edge service, closest to the user) retries the full request.</p>
<p>Alternatively, use idempotency keys in the retry headers so interior services can deduplicate:</p>
<pre><code class="language-java">@GetMapping("/order")
public ResponseEntity&#x3C;OrderResponse> createOrder(@RequestBody OrderRequest request) {
    String idempotencyKey = UUID.randomUUID().toString();

    return retryTemplate.execute(context -> {
        // Same idempotency key on all retries
        return orderService.createOrder(request, idempotencyKey);
    });
}
</code></pre>
<h2>Real Outage Scenario</h2>
<p><strong>System:</strong> Payment processing service, Spring Boot, calls external payment gateway.</p>
<p><strong>Timeline:</strong></p>
<ul>
<li>09:00: Payment gateway experiences degraded performance (30% of requests timing out at 10 seconds)</li>
<li>09:01: Payment service's <code>RestTemplate</code> has <code>connectTimeout=10s, readTimeout=10s</code></li>
<li>09:01: Retry logic: 3 retries with 1-second delay (linear, no jitter)</li>
<li>09:02: 1,000 concurrent payment requests Ã— 3 retries Ã— 10s timeout = 30,000 seconds of thread holding. 200 Tomcat threads exhausted within 2 minutes.</li>
<li>09:03: Payment service appears down. API gateway returns 503. Alert fires.</li>
<li>09:03: On-call restarts payment service. Gateway still degraded. Service exhausts threads again in 90 seconds.</li>
<li>09:04: Retry on restart causes 3,000 requests to hit the still-struggling gateway simultaneously (retry storm on service startup)</li>
<li>09:15: Gateway recovers. Payment service recovers.</li>
<li><strong>Total downtime:</strong> 15 minutes. Payment service fully available for only 10 minutes of that window.</li>
</ul>
<p><strong>Root causes:</strong></p>
<ol>
<li>10-second timeout was too long â€” threads held too long during degradation</li>
<li>Linear retry with no jitter created synchronized load spikes</li>
<li>No circuit breaker â€” service kept sending requests to a known-degraded gateway</li>
<li>No bulkhead â€” payment calls exhausted the shared thread pool</li>
</ol>
<p><strong>Fixes applied:</strong></p>
<pre><code class="language-java">// Before:
restTemplate.setConnectTimeout(10_000);
restTemplate.setReadTimeout(10_000);
// 3 retries, 1-second delay

// After:
restTemplate.setConnectTimeout(2_000);   // 2 seconds - fail fast
restTemplate.setReadTimeout(5_000);      // 5 seconds max read

CircuitBreakerConfig config = CircuitBreakerConfig.custom()
    .slowCallDurationThreshold(Duration.ofSeconds(3))
    .slowCallRateThreshold(50)
    .failureRateThreshold(30)
    .waitDurationInOpenState(Duration.ofSeconds(20))
    .build();

RetryConfig retryConfig = RetryConfig.custom()
    .maxAttempts(3)
    .intervalFunction(IntervalFunction.ofExponentialRandomBackoff(
        Duration.ofMillis(500),  // base delay
        2.0,                     // multiplier
        Duration.ofSeconds(10)   // max delay
    ))
    .build();
</code></pre>
<p>Result: During the next gateway degradation event (4 weeks later), the circuit breaker opened after 30 seconds of degraded performance, fast-failing requests instead of holding threads, payment service remained available (returning "payment gateway temporarily unavailable" to users), gateway recovered, circuit breaker closed, normal operations resumed. Total user-visible downtime: 30 seconds.</p>
<h2>Monitoring Retry Rates</h2>
<pre><code class="language-java">@Component
public class RetryMetrics {

    @EventListener
    public void onRetry(RetryOnRetryEvent event) {
        meterRegistry.counter("retry.attempts",
            "service", event.getName(),
            "attempt", String.valueOf(event.getNumberOfRetryAttempts())
        ).increment();
    }

    @EventListener
    public void onError(RetryOnErrorEvent event) {
        meterRegistry.counter("retry.failures",
            "service", event.getName(),
            "exception", event.getLastThrowable().getClass().getSimpleName()
        ).increment();
    }
}
</code></pre>
<p>Grafana alert: <code>rate(retry.attempts[5m]) > 100</code> â€” indicates upstream degradation in progress before it becomes an outage.</p>
<h2>Architecture Diagram</h2>
<pre><code>Resilient Retry Architecture:

User Request
     â”‚
     â–¼
API Gateway
(rate limiting, auth)
     â”‚
     â–¼
Edge Service
     â”‚
     â”œâ”€â”€ Bulkhead: Payment pool (30 threads)
     â”‚       â”‚
     â”‚       â”œâ”€â”€ Circuit Breaker (open/closed/half-open)
     â”‚       â”‚       â”‚
     â”‚       â”‚       â–¼
     â”‚       â”‚   Retry (3 attempts, exponential + jitter)
     â”‚       â”‚       â”‚
     â”‚       â”‚       â–¼
     â”‚       â”‚   Payment Gateway (external)
     â”‚       â”‚
     â”‚       â””â”€â”€ Circuit open â†’ Return cached/degraded response
     â”‚
     â”œâ”€â”€ Bulkhead: Inventory pool (20 threads)
     â”‚       â””â”€â”€ [same pattern]
     â”‚
     â””â”€â”€ Bulkhead: User profile pool (15 threads)
             â””â”€â”€ [same pattern]

Async Kafka path:
Event â†’ payments topic â†’ Consumer
                              â”‚
                              â”œâ”€â”€ Success â†’ payments-processed
                              â”œâ”€â”€ Retry   â†’ payments-retry-* topics
                              â””â”€â”€ Max retry â†’ payments-dlq â†’ alert
</code></pre>
<p>Retry logic is not a feature â€” it's infrastructure. It needs the same rigor as your deployment pipeline. An untested retry strategy will fail exactly when you need it most: during an outage, when the retry storm amplifies the problem it was meant to solve.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">ğŸ“š</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Designing Data-Intensive Applications</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Best Seller</span></div><p class="text-xs text-gray-600">The definitive guide to building scalable, reliable distributed systems by Martin Kleppmann.</p></div><a href="https://amzn.to/3RyKzOA" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> â†’</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Kafka: The Definitive Guide</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Editor&#x27;s Pick</span></div><p class="text-xs text-gray-600">Real-time data and stream processing by Confluent engineers.</p></div><a href="https://amzn.to/3TpGKsI" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> â†’</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Apache Kafka Series on Udemy</span></div><p class="text-xs text-gray-600">Hands-on Kafka course covering producers, consumers, Kafka Streams, and Connect.</p></div><a href="https://www.udemy.com/course/apache-kafka/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> â†’</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Designing%20a%20Retry%20System%20Without%20Causing%20a%20Retry%20Storm&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fretry-storm-prevention%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fretry-storm-prevention%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#exponential-backoff-why-linear-retry-is-wrong" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Exponential Backoff: Why Linear Retry Is Wrong</a></li><li class=""><a href="#jitter-the-fix-for-synchronized-retries" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Jitter: The Fix for Synchronized Retries</a></li><li class=""><a href="#circuit-breaker" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Circuit Breaker</a></li><li class=""><a href="#bulkhead-pattern" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Bulkhead Pattern</a></li><li class=""><a href="#dead-letter-queues" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Dead Letter Queues</a></li><li class=""><a href="#kafka-retry-topics" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Kafka Retry Topics</a></li><li class=""><a href="#idempotency-considerations" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Idempotency Considerations</a></li><li class=""><a href="#retry-amplification-problem" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Retry Amplification Problem</a></li><li class=""><a href="#real-outage-scenario" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Real Outage Scenario</a></li><li class=""><a href="#monitoring-retry-rates" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Monitoring Retry Rates</a></li><li class=""><a href="#architecture-diagram" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Architecture Diagram</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/observability-opentelemetry-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Building Production Observability with OpenTelemetry and Grafana Stack</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why â€” by exploring system state through metrics, traces, and logs without needing to know in advanceâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jul 3, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>6 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->observability</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->opentelemetry</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->prometheus</span></div></article></a><a href="/blog/event-sourcing-cqrs-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Event Sourcing and CQRS in Production: Beyond the Theory</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory â€” store events instead of state, derive state by replaying events â€” is souâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 23, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->event sourcing</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->cqrs</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->system design</span></div></article></a><a href="/blog/grpc-vs-rest-vs-graphql/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">gRPC vs REST vs GraphQL: Choosing the Right API Protocol</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to tâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 18, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->grpc</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->rest</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->graphql</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">â† Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS â€” by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">Â© <!-- -->2026<!-- --> CodeSprintPro Â· Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js Â· TailwindCSS Â· Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Designing a Retry System Without Causing a Retry Storm","description":"Exponential backoff with jitter, circuit breakers, bulkhead isolation, Kafka retry topics, and the retry amplification problem â€” with Java implementations and a real outage postmortem.","date":"2025-05-22","category":"System Design","tags":["retry","circuit breaker","resilience","spring boot","kafka","distributed systems","java"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"retry-storm-prevention","readingTime":"10 min read","excerpt":"Retry logic is the second most dangerous code in a distributed system, after \"delete all records.\" The intent is to improve reliability by recovering from transient failures. The actual effect, when implemented naively, â€¦","contentHtml":"\u003cp\u003eRetry logic is the second most dangerous code in a distributed system, after \"delete all records.\" The intent is to improve reliability by recovering from transient failures. The actual effect, when implemented naively, is to turn a brief service degradation into a cascading system-wide outage.\u003c/p\u003e\n\u003cp\u003eThe pattern is predictable: a downstream service slows down, client retries pile up, the downstream service is now handling 3Ã— the original load while already struggling, it slows down more, more retries, complete failure. A retry storm.\u003c/p\u003e\n\u003cp\u003eThis article covers every layer of the retry stack â€” from jitter algorithms to Kafka DLQ topology â€” and the production outage that reshaped how our team thinks about retry.\u003c/p\u003e\n\u003ch2\u003eExponential Backoff: Why Linear Retry Is Wrong\u003c/h2\u003e\n\u003cp\u003eLinear retry: wait 1 second, then try again. If the service is down for 60 seconds and you have 1,000 clients, each retrying every second, that's 1,000 Ã— 60 = 60,000 retry requests during the outage. When the service recovers, it receives all 1,000 pending retries simultaneously.\u003c/p\u003e\n\u003cp\u003eExponential backoff: each retry waits 2Ã— longer than the previous:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eAttempt 1: wait 1s\nAttempt 2: wait 2s\nAttempt 3: wait 4s\nAttempt 4: wait 8s\nAttempt 5: wait 16s â†’ give up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTotal client load during a 60-second outage with exponential backoff: 5 retries per client Ã— 1,000 clients = 5,000 requests â€” 12Ã— fewer than linear retry.\u003c/p\u003e\n\u003cp\u003eBut exponential backoff alone still causes a thundering herd on recovery: all 1,000 clients synchronized on the same backoff schedule will all retry at t=1s, t=2s, t=4s simultaneously.\u003c/p\u003e\n\u003ch2\u003eJitter: The Fix for Synchronized Retries\u003c/h2\u003e\n\u003cp\u003eJitter adds randomness to the wait time, spreading retries across a time window:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic class ExponentialBackoffWithJitter {\n\n    private final int maxAttempts;\n    private final long baseDelayMs;\n    private final long maxDelayMs;\n    private final double jitterFactor;\n\n    // Full jitter: random(0, min(maxDelay, baseDelay * 2^attempt))\n    public long computeDelay(int attempt) {\n        long exponentialDelay = (long) (baseDelayMs * Math.pow(2, attempt));\n        long cappedDelay = Math.min(maxDelayMs, exponentialDelay);\n        return ThreadLocalRandom.current().nextLong(0, cappedDelay);\n    }\n\n    // Equal jitter: split between base and random portion\n    // Guarantees minimum wait while still spreading load\n    public long computeEqualJitterDelay(int attempt) {\n        long exponentialDelay = (long) (baseDelayMs * Math.pow(2, attempt));\n        long capped = Math.min(maxDelayMs, exponentialDelay);\n        return (capped / 2) + ThreadLocalRandom.current().nextLong(0, capped / 2);\n    }\n\n    // Decorrelated jitter (AWS recommendation): harder to reason about but best distribution\n    public long computeDecorrelatedDelay(int attempt, long previousDelay) {\n        return Math.min(maxDelayMs,\n            ThreadLocalRandom.current().nextLong(baseDelayMs, previousDelay * 3));\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eWhich jitter to use:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFull jitter: best load distribution, minimum guaranteed wait is 0 (acceptable for most cases)\u003c/li\u003e\n\u003cli\u003eEqual jitter: ensures some minimum delay, good for avoiding hammering on immediate retry\u003c/li\u003e\n\u003cli\u003eDecorrelated jitter: AWS's recommended approach, best statistical distribution\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eCircuit Breaker\u003c/h2\u003e\n\u003cp\u003eThe circuit breaker prevents retrying a service that's known to be down. Instead of each client independently discovering the service is down, the circuit breaker shares this knowledge:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCircuit Breaker States:\n\nCLOSED (normal):\n  Requests flow through\n  Failure rate monitored\n  If failure rate \u003e threshold â†’ open circuit\n         â”‚\n         â–¼\nOPEN (service is down):\n  All requests rejected immediately (fail fast)\n  No requests sent to downstream\n  After timeout â†’ move to half-open\n         â”‚\n         â–¼\nHALF-OPEN (testing recovery):\n  Limited requests allowed through\n  If they succeed â†’ close circuit\n  If they fail â†’ re-open circuit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Bean\npublic CircuitBreaker paymentCircuitBreaker(CircuitBreakerRegistry registry) {\n    CircuitBreakerConfig config = CircuitBreakerConfig.custom()\n        .slidingWindowType(SlidingWindowType.TIME_BASED)\n        .slidingWindowSize(30)                           // 30 seconds window\n        .minimumNumberOfCalls(10)                        // Min calls before evaluating\n        .failureRateThreshold(50)                        // Open at 50% failure rate\n        .slowCallRateThreshold(80)                       // Also open at 80% slow calls\n        .slowCallDurationThreshold(Duration.ofSeconds(2))\n        .waitDurationInOpenState(Duration.ofSeconds(30)) // Stay open 30s\n        .permittedNumberOfCallsInHalfOpenState(5)\n        .recordExceptions(IOException.class, TimeoutException.class,\n                         ServiceUnavailableException.class)\n        .ignoreExceptions(ValidationException.class,    // Don't count business logic failures\n                         AuthenticationException.class)\n        .build();\n\n    return registry.circuitBreaker(\"payment-service\", config);\n}\n\n// Usage:\npublic PaymentResult charge(PaymentRequest request) {\n    return circuitBreaker.executeSupplier(() -\u003e {\n        return paymentClient.charge(request);\n    });\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eslowCallDurationThreshold\u003c/code\u003e is critical and often missed. A service that responds in 5 seconds is not \"failing\" â€” the exception-based circuit breaker stays closed. But 5-second calls are exhausting your thread pool. Trip the circuit breaker on slow calls, not just errors.\u003c/p\u003e\n\u003ch2\u003eBulkhead Pattern\u003c/h2\u003e\n\u003cp\u003eBulkheads isolate failure domains. Without bulkheads, a slow downstream service exhausts all your thread pool capacity, taking down unrelated functionality.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eWithout bulkhead:\nAll requests share 200 Tomcat threads\nPayment service slow â†’ 200 threads busy waiting for payments\nAll other endpoints (search, profile, cart) â†’ timeout\n\nWith bulkhead:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Tomcat (200 total threads)                  â”‚\nâ”‚                                             â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\nâ”‚ â”‚ Payment pool â”‚  â”‚ Search pool  â”‚          â”‚\nâ”‚ â”‚ 30 threads   â”‚  â”‚ 50 threads   â”‚          â”‚\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\nâ”‚ â”‚ Profile pool â”‚  â”‚ Cart pool    â”‚          â”‚\nâ”‚ â”‚ 20 threads   â”‚  â”‚ 20 threads   â”‚          â”‚\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Resilience4j ThreadPoolBulkhead:\n@Bean\npublic ThreadPoolBulkhead paymentBulkhead(ThreadPoolBulkheadRegistry registry) {\n    ThreadPoolBulkheadConfig config = ThreadPoolBulkheadConfig.custom()\n        .maxThreadPoolSize(30)\n        .coreThreadPoolSize(15)\n        .queueCapacity(20)          // Queue depth before rejecting\n        .keepAliveDuration(Duration.ofSeconds(20))\n        .build();\n\n    return registry.bulkhead(\"payment\", config);\n}\n\npublic CompletableFuture\u0026#x3C;PaymentResult\u003e chargeAsync(PaymentRequest request) {\n    return paymentBulkhead.executeSupplier(() -\u003e\n        CompletableFuture.supplyAsync(() -\u003e paymentClient.charge(request))\n    );\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen the payment service is slow, only the 30 payment threads are affected. Search, profile, and cart keep running with their own thread pools.\u003c/p\u003e\n\u003ch2\u003eDead Letter Queues\u003c/h2\u003e\n\u003cp\u003eMessages that consistently fail need to go somewhere that isn't \"try again forever.\" DLQ design:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eDLQ requirements:\n1. Not lost (durable storage)\n2. Observable (alert on DLQ message arrival)\n3. Reprocessable (ability to replay after fix)\n4. Auditable (track when message failed and why)\n\nDLQ record schema:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ original_payload: \u0026#x3C;message bytes\u003e      â”‚\nâ”‚ original_topic: \"payments\"             â”‚\nâ”‚ original_partition: 7                  â”‚\nâ”‚ original_offset: 10034567              â”‚\nâ”‚ failure_count: 4                       â”‚\nâ”‚ last_failure_time: 2025-04-15T14:32:00 â”‚\nâ”‚ last_error: \"GatewayTimeoutException\"  â”‚\nâ”‚ last_error_trace: \"...\"               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eKafka Retry Topics\u003c/h2\u003e\n\u003cp\u003eKafka's at-least-once delivery means consumers must handle retries. The naive approach â€” retry in the consumer loop â€” holds the partition and blocks other messages. Use a retry topic pattern instead:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eKafka Retry Topology:\n\npayments (main)\n    â”‚\n    â–¼\nConsumer (payments-group)\n    â”‚\n    â”œâ”€â”€ Success â†’ ack, continue\n    â”‚\n    â”œâ”€â”€ Retryable failure (1st time)\n    â”‚       â””â”€â”€ Publish to payments-retry-30s\n    â”‚\n    â””â”€â”€ Non-retryable â†’ payments-dlq (immediately)\n\npayments-retry-30s\n    â”‚  Consumer pauses 30s before consuming\n    â–¼\nConsumer (retry-group-30s)\n    â”‚\n    â”œâ”€â”€ Success â†’ ack\n    â”œâ”€â”€ Retryable â†’ payments-retry-5m\n    â””â”€â”€ Max retries â†’ payments-dlq\n\npayments-retry-5m â†’ payments-retry-30m â†’ payments-dlq\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@RetryableTopic(\n    attempts = \"4\",\n    backoff = @Backoff(\n        delay = 30_000,           // 30 seconds initial\n        multiplier = 6,           // Ã— 6 each attempt: 30s, 3m, 18m\n        maxDelay = 1_800_000      // Cap at 30 minutes\n    ),\n    dltStrategy = DltStrategy.FAIL_ON_ERROR,\n    autoCreateTopics = \"false\",   // Create topics via IaC, not code\n    include = {\n        RetryablePaymentException.class,\n        GatewayTimeoutException.class\n    },\n    exclude = {\n        NonRetryablePaymentException.class,\n        InvalidRequestException.class\n    }\n)\n@KafkaListener(topics = \"payments\", groupId = \"payment-processor\")\npublic void process(ConsumerRecord\u0026#x3C;String, PaymentEvent\u003e record) {\n    paymentService.process(record.value());\n}\n\n@DltHandler\npublic void handleDeadLetter(\n        PaymentEvent event,\n        @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,\n        @Header(KafkaHeaders.EXCEPTION_FQCN) String exceptionFqcn) {\n    deadLetterService.record(event, topic, exceptionFqcn);\n    alertingService.notifyDltArrival(event);\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eIdempotency Considerations\u003c/h2\u003e\n\u003cp\u003eRetries without idempotent consumers cause duplicate processing. Every message that might be retried must be safe to process twice:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Transactional\npublic void processPayment(PaymentEvent event) {\n    // Idempotency check upfront\n    if (processedPayments.existsByEventId(event.getEventId())) {\n        log.info(\"Duplicate event skipped: {}\", event.getEventId());\n        return;\n    }\n\n    // Mark as processing (atomic insert, fails on duplicate)\n    processedPayments.markProcessing(event.getEventId());\n\n    try {\n        PaymentResult result = gateway.charge(event);\n        processedPayments.markComplete(event.getEventId(), result);\n    } catch (Exception e) {\n        processedPayments.markFailed(event.getEventId(), e.getMessage());\n        throw e; // Re-throw for retry\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eRetry Amplification Problem\u003c/h2\u003e\n\u003cp\u003eEach service in a call chain that retries independently amplifies the total request count:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eService A calls B calls C calls D\nEach service retries 3 times on failure\n\nD fails:\nC retries D 3 times â†’ 3 calls to D\nB retries C 3 times â†’ 3 Ã— 3 = 9 calls to D\nA retries B 3 times â†’ 3 Ã— 3 Ã— 3 = 27 calls to D\n\n27 calls to D for 1 user request\nAt 1,000 concurrent users: 27,000 calls to D\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe fix: \u003cstrong\u003eretry at the edge, not in the interior of a call chain\u003c/strong\u003e. Services B and C should not retry â€” they should propagate errors back to A. A (the edge service, closest to the user) retries the full request.\u003c/p\u003e\n\u003cp\u003eAlternatively, use idempotency keys in the retry headers so interior services can deduplicate:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@GetMapping(\"/order\")\npublic ResponseEntity\u0026#x3C;OrderResponse\u003e createOrder(@RequestBody OrderRequest request) {\n    String idempotencyKey = UUID.randomUUID().toString();\n\n    return retryTemplate.execute(context -\u003e {\n        // Same idempotency key on all retries\n        return orderService.createOrder(request, idempotencyKey);\n    });\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eReal Outage Scenario\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eSystem:\u003c/strong\u003e Payment processing service, Spring Boot, calls external payment gateway.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTimeline:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e09:00: Payment gateway experiences degraded performance (30% of requests timing out at 10 seconds)\u003c/li\u003e\n\u003cli\u003e09:01: Payment service's \u003ccode\u003eRestTemplate\u003c/code\u003e has \u003ccode\u003econnectTimeout=10s, readTimeout=10s\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e09:01: Retry logic: 3 retries with 1-second delay (linear, no jitter)\u003c/li\u003e\n\u003cli\u003e09:02: 1,000 concurrent payment requests Ã— 3 retries Ã— 10s timeout = 30,000 seconds of thread holding. 200 Tomcat threads exhausted within 2 minutes.\u003c/li\u003e\n\u003cli\u003e09:03: Payment service appears down. API gateway returns 503. Alert fires.\u003c/li\u003e\n\u003cli\u003e09:03: On-call restarts payment service. Gateway still degraded. Service exhausts threads again in 90 seconds.\u003c/li\u003e\n\u003cli\u003e09:04: Retry on restart causes 3,000 requests to hit the still-struggling gateway simultaneously (retry storm on service startup)\u003c/li\u003e\n\u003cli\u003e09:15: Gateway recovers. Payment service recovers.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal downtime:\u003c/strong\u003e 15 minutes. Payment service fully available for only 10 minutes of that window.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eRoot causes:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e10-second timeout was too long â€” threads held too long during degradation\u003c/li\u003e\n\u003cli\u003eLinear retry with no jitter created synchronized load spikes\u003c/li\u003e\n\u003cli\u003eNo circuit breaker â€” service kept sending requests to a known-degraded gateway\u003c/li\u003e\n\u003cli\u003eNo bulkhead â€” payment calls exhausted the shared thread pool\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eFixes applied:\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Before:\nrestTemplate.setConnectTimeout(10_000);\nrestTemplate.setReadTimeout(10_000);\n// 3 retries, 1-second delay\n\n// After:\nrestTemplate.setConnectTimeout(2_000);   // 2 seconds - fail fast\nrestTemplate.setReadTimeout(5_000);      // 5 seconds max read\n\nCircuitBreakerConfig config = CircuitBreakerConfig.custom()\n    .slowCallDurationThreshold(Duration.ofSeconds(3))\n    .slowCallRateThreshold(50)\n    .failureRateThreshold(30)\n    .waitDurationInOpenState(Duration.ofSeconds(20))\n    .build();\n\nRetryConfig retryConfig = RetryConfig.custom()\n    .maxAttempts(3)\n    .intervalFunction(IntervalFunction.ofExponentialRandomBackoff(\n        Duration.ofMillis(500),  // base delay\n        2.0,                     // multiplier\n        Duration.ofSeconds(10)   // max delay\n    ))\n    .build();\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult: During the next gateway degradation event (4 weeks later), the circuit breaker opened after 30 seconds of degraded performance, fast-failing requests instead of holding threads, payment service remained available (returning \"payment gateway temporarily unavailable\" to users), gateway recovered, circuit breaker closed, normal operations resumed. Total user-visible downtime: 30 seconds.\u003c/p\u003e\n\u003ch2\u003eMonitoring Retry Rates\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Component\npublic class RetryMetrics {\n\n    @EventListener\n    public void onRetry(RetryOnRetryEvent event) {\n        meterRegistry.counter(\"retry.attempts\",\n            \"service\", event.getName(),\n            \"attempt\", String.valueOf(event.getNumberOfRetryAttempts())\n        ).increment();\n    }\n\n    @EventListener\n    public void onError(RetryOnErrorEvent event) {\n        meterRegistry.counter(\"retry.failures\",\n            \"service\", event.getName(),\n            \"exception\", event.getLastThrowable().getClass().getSimpleName()\n        ).increment();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGrafana alert: \u003ccode\u003erate(retry.attempts[5m]) \u003e 100\u003c/code\u003e â€” indicates upstream degradation in progress before it becomes an outage.\u003c/p\u003e\n\u003ch2\u003eArchitecture Diagram\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eResilient Retry Architecture:\n\nUser Request\n     â”‚\n     â–¼\nAPI Gateway\n(rate limiting, auth)\n     â”‚\n     â–¼\nEdge Service\n     â”‚\n     â”œâ”€â”€ Bulkhead: Payment pool (30 threads)\n     â”‚       â”‚\n     â”‚       â”œâ”€â”€ Circuit Breaker (open/closed/half-open)\n     â”‚       â”‚       â”‚\n     â”‚       â”‚       â–¼\n     â”‚       â”‚   Retry (3 attempts, exponential + jitter)\n     â”‚       â”‚       â”‚\n     â”‚       â”‚       â–¼\n     â”‚       â”‚   Payment Gateway (external)\n     â”‚       â”‚\n     â”‚       â””â”€â”€ Circuit open â†’ Return cached/degraded response\n     â”‚\n     â”œâ”€â”€ Bulkhead: Inventory pool (20 threads)\n     â”‚       â””â”€â”€ [same pattern]\n     â”‚\n     â””â”€â”€ Bulkhead: User profile pool (15 threads)\n             â””â”€â”€ [same pattern]\n\nAsync Kafka path:\nEvent â†’ payments topic â†’ Consumer\n                              â”‚\n                              â”œâ”€â”€ Success â†’ payments-processed\n                              â”œâ”€â”€ Retry   â†’ payments-retry-* topics\n                              â””â”€â”€ Max retry â†’ payments-dlq â†’ alert\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRetry logic is not a feature â€” it's infrastructure. It needs the same rigor as your deployment pipeline. An untested retry strategy will fail exactly when you need it most: during an outage, when the retry storm amplifies the problem it was meant to solve.\u003c/p\u003e\n","tableOfContents":[{"id":"exponential-backoff-why-linear-retry-is-wrong","text":"Exponential Backoff: Why Linear Retry Is Wrong","level":2},{"id":"jitter-the-fix-for-synchronized-retries","text":"Jitter: The Fix for Synchronized Retries","level":2},{"id":"circuit-breaker","text":"Circuit Breaker","level":2},{"id":"bulkhead-pattern","text":"Bulkhead Pattern","level":2},{"id":"dead-letter-queues","text":"Dead Letter Queues","level":2},{"id":"kafka-retry-topics","text":"Kafka Retry Topics","level":2},{"id":"idempotency-considerations","text":"Idempotency Considerations","level":2},{"id":"retry-amplification-problem","text":"Retry Amplification Problem","level":2},{"id":"real-outage-scenario","text":"Real Outage Scenario","level":2},{"id":"monitoring-retry-rates","text":"Monitoring Retry Rates","level":2},{"id":"architecture-diagram","text":"Architecture Diagram","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why â€” by exploring system state through metrics, traces, and logs without needing to know in advanceâ€¦"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory â€” store events instead of state, derive state by replaying events â€” is souâ€¦"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to tâ€¦"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"retry-storm-prevention"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>