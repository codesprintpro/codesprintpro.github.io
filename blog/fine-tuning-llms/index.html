<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Fine-Tuning LLMs: When to Fine-Tune, When to Prompt<!-- --> | CodeSprintPro</title><meta name="description" content="Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/fine-tuning-llms/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Fine-Tuning LLMs: When to Fine-Tune, When to Prompt" data-next-head=""/><meta property="og:description" content="Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/fine-tuning-llms/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-03-27" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="AI/ML" data-next-head=""/><meta property="article:tag" content="ai" data-next-head=""/><meta property="article:tag" content="llm" data-next-head=""/><meta property="article:tag" content="fine-tuning" data-next-head=""/><meta property="article:tag" content="lora" data-next-head=""/><meta property="article:tag" content="hugging face" data-next-head=""/><meta property="article:tag" content="openai" data-next-head=""/><meta property="article:tag" content="machine learning" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Fine-Tuning LLMs: When to Fine-Tune, When to Prompt" data-next-head=""/><meta name="twitter:description" content="Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Fine-Tuning LLMs: When to Fine-Tune, When to Prompt","description":"Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-03-27","dateModified":"2025-03-27","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/fine-tuning-llms/"},"keywords":"ai, llm, fine-tuning, lora, hugging face, openai, machine learning","articleSection":"AI/ML"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Fine-Tuning LLMs: When to Fine-Tune, When to Prompt</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">AI/ML</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Fine-Tuning LLMs: When to Fine-Tune, When to Prompt</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>¬∑</span><span>March 27, 2025</span><span>¬∑</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->ai</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->llm</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->fine-tuning</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->lora</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->hugging face</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->openai</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->machine learning</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning ‚Äî for style consistency, domain-specific knowledge not in training data, or latency reduction ‚Äî it's transformative. This article helps you make the right choice and do the fine-tuning correctly.</p>
<h2>When Fine-Tuning is the Wrong Choice</h2>
<p>Before spending days preparing data and dollars on GPU time, work through this decision tree. The examples below show the most common fine-tuning mistakes ‚Äî each one has a simpler, cheaper solution that most engineers overlook because fine-tuning feels like the "serious" approach.</p>
<pre><code>Problem: "The model doesn't know our internal coding conventions"
Wrong answer: Fine-tune
Right answer: Add coding conventions to the system prompt
              Or create a few-shot example template

Problem: "The model doesn't know our product documentation"
Wrong answer: Fine-tune (your docs won't fit in training anyway)
Right answer: RAG ‚Äî embed docs, retrieve relevant chunks at query time

Problem: "The model sometimes doesn't follow the output format"
Wrong answer: Fine-tune on format examples
Right answer: Use structured outputs (JSON mode) or better prompt instructions

Problem: "I want a cheaper, faster model"
Wrong answer: Fine-tune GPT-4 (won't make it cheaper)
Right answer: Distillation ‚Äî generate training data from GPT-4,
              fine-tune GPT-4o-mini on that data
              (can get GPT-4 quality at GPT-4o-mini price for specific tasks)
</code></pre>
<h2>When Fine-Tuning IS the Right Choice</h2>
<p>With the anti-patterns clear, here are the scenarios where fine-tuning genuinely outperforms alternatives. The common thread across all of them is that you have a well-defined, high-volume, repeatable task where the investment in training data pays off over thousands or millions of inferences.</p>
<pre><code>‚úì Consistent style/tone at scale
  - Customer service bot that must always match brand voice
  - Code generation in company-specific patterns
  - Technical writing with domain-specific terminology

‚úì Short, repetitive prompts ‚Üí latency reduction
  - System prompt has 10,000 tokens ‚Üí adds 500ms per call
  - Fine-tuning "bakes in" the instructions ‚Üí 100-token system prompt
  - Result: 80% latency reduction for same quality

‚úì Specialized domain not in base training
  - Medical codes (ICD-10, CPT) with specific formatting rules
  - Legal document classification with jurisdiction-specific standards
  - Proprietary data formats (your company's SQL dialect)

‚úì Distillation (most common good use case)
  - Use expensive GPT-4 to generate 1,000 high-quality examples
  - Fine-tune cheap GPT-4o-mini on those examples
  - Get 90% of GPT-4 quality at 10% of GPT-4 cost for your specific task
</code></pre>
<h2>Data Preparation: The Most Important Step</h2>
<p>Fine-tuning quality is almost entirely determined by data quality. 500 excellent examples beat 5,000 mediocre ones.</p>
<p>Your training examples are the blueprint the model learns from ‚Äî if your examples have inconsistent tone, vary in format, or include sloppy responses, the model will learn those patterns faithfully. The gold standard is examples that represent exactly the behavior you want the model to exhibit at inference time, written by the person or team with the deepest knowledge of what "good" looks like for your use case.</p>
<pre><code class="language-python"># Training data format for chat fine-tuning (OpenAI compatible)
training_examples = [
    {
        "messages": [
            {
                "role": "system",
                "content": "You are a customer support agent for TechCorp. "
                           "Always be concise, empathetic, and offer concrete next steps."
            },
            {
                "role": "user",
                "content": "My order hasn't arrived and it's been 10 days"
            },
            {
                "role": "assistant",
                "content": "I'm sorry for the delay ‚Äî 10 days is longer than expected. "
                           "I've checked your order #TC-123456 and can see it's still in transit. "
                           "I'll escalate this to our shipping team and you'll receive an update "
                           "within 4 hours. Would you like me to also initiate a replacement shipment?"
            }
        ]
    },
    # ... 499 more examples
]

# Save in JSONL format (one JSON object per line)
import json
with open("training_data.jsonl", "w") as f:
    for example in training_examples:
        f.write(json.dumps(example) + "\n")
</code></pre>
<p>Notice the structure: each example has a system prompt that defines the persona, a realistic user message, and an ideal assistant response. The system prompt is the same across all examples because you want the model to internalize that persona ‚Äî if you vary the system prompt per example, the model learns multiple conflicting behaviors.</p>
<p>Running the validation script below before submitting your training job will catch the common issues that silently degrade fine-tune quality, such as missing assistant turns or responses that are too short to teach the model anything useful.</p>
<pre><code class="language-python"># Data quality checklist script
def validate_training_data(filepath: str) -> dict:
    issues = []
    examples = []

    with open(filepath) as f:
        for i, line in enumerate(f):
            try:
                example = json.loads(line)
                examples.append(example)
            except json.JSONDecodeError as e:
                issues.append(f"Line {i+1}: Invalid JSON: {e}")

    # Check for common issues
    for i, example in enumerate(examples):
        messages = example.get("messages", [])

        # Check: has system, user, and assistant
        roles = [m["role"] for m in messages]
        if "assistant" not in roles:
            issues.append(f"Example {i+1}: Missing assistant message")

        # Check: assistant response length distribution
        for m in messages:
            if m["role"] == "assistant":
                if len(m["content"]) &#x3C; 10:
                    issues.append(f"Example {i+1}: Very short assistant response")
                if len(m["content"]) > 2000:
                    issues.append(f"Example {i+1}: Very long assistant response (fine-tuning works best on focused, shorter outputs)")

    # Token count estimate
    total_tokens = sum(
        sum(len(m["content"].split()) * 1.3 for m in ex["messages"])  # rough estimate
        for ex in examples
    )

    return {
        "total_examples": len(examples),
        "issues": issues,
        "estimated_tokens": int(total_tokens),
        "estimated_cost_usd": total_tokens / 1_000_000 * 8  # GPT-4o-mini: ~$8/M tokens for training
    }

report = validate_training_data("training_data.jsonl")
print(f"Examples: {report['total_examples']}")
print(f"Issues: {len(report['issues'])}")
print(f"Estimated training cost: ${report['estimated_cost_usd']:.2f}")
</code></pre>
<p>The estimated cost output lets you make a go/no-go decision before spending money. For most tasks, a 500-example dataset with typical response lengths costs well under $10 to train on GPT-4o-mini ‚Äî the data preparation effort is the real cost, not the compute.</p>
<h2>OpenAI Fine-Tuning</h2>
<p>With your data validated, the OpenAI fine-tuning API makes the actual training a four-step process: upload your file, create a job, monitor until completion, and deploy the resulting model. The polling loop below is important ‚Äî training can take anywhere from minutes to hours depending on dataset size, and you need to handle both success and failure gracefully.</p>
<pre><code class="language-python">from openai import OpenAI

client = OpenAI()

# Step 1: Upload training file
with open("training_data.jsonl", "rb") as f:
    file = client.files.create(file=f, purpose="fine-tune")

print(f"File ID: {file.id}")

# Step 2: Create fine-tuning job
job = client.fine_tuning.jobs.create(
    training_file=file.id,
    model="gpt-4o-mini-2024-07-18",  # Base model to fine-tune
    hyperparameters={
        "n_epochs": 3,              # 3-10 typically; more = risk of overfitting
        "batch_size": "auto",       # Let OpenAI optimize
        "learning_rate_multiplier": "auto"
    },
    suffix="customer-support-v1"    # Your fine-tuned model name suffix
)

print(f"Job ID: {job.id}")

# Step 3: Monitor progress
import time

while True:
    job = client.fine_tuning.jobs.retrieve(job.id)
    print(f"Status: {job.status}, events: {len(job.integrations)}")

    if job.status in ("succeeded", "failed", "cancelled"):
        break
    time.sleep(60)

if job.status == "succeeded":
    model_id = job.fine_tuned_model
    print(f"Fine-tuned model: {model_id}")
    # e.g., "ft:gpt-4o-mini-2024-07-18:your-org:customer-support-v1:abc123"

# Step 4: Use the fine-tuned model
response = client.chat.completions.create(
    model=model_id,
    messages=[
        {"role": "user", "content": "My subscription charge looks wrong"}
    ]
)
print(response.choices[0].message.content)
</code></pre>
<p>The <code>n_epochs</code> hyperparameter controls how many times the model sees your entire dataset during training. With 3 epochs and 500 examples, the model sees each example 3 times ‚Äî enough to learn the pattern without memorizing individual responses. If you see the fine-tuned model producing verbatim copies of training examples on similar inputs, that is a sign of overfitting and you should reduce epochs or increase dataset diversity.</p>
<h2>Open-Source: LoRA Fine-Tuning with Hugging Face</h2>
<p>For open-source models (Llama, Mistral, Qwen), LoRA (Low-Rank Adaptation) fine-tunes efficiently on consumer hardware.</p>
<p>Full fine-tuning of an 8B parameter model would require updating all 8 billion weights ‚Äî computationally prohibitive without expensive server-grade GPUs. LoRA solves this elegantly: instead of modifying the original weights, it adds small "adapter" matrices alongside specific layers and only trains those. The original model is frozen, and only the adapters (about 0.1% of total parameters) are updated.</p>
<pre><code class="language-python"># pip install transformers peft datasets trl accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer
from datasets import Dataset

# Load base model in 4-bit quantization (fits in 8GB VRAM)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")

# LoRA configuration: only fine-tune small adapters
# Adapts 0.1-1% of parameters vs 100% for full fine-tuning
lora_config = LoraConfig(
    r=16,                           # Rank: higher = more expressive, more memory
    lora_alpha=32,                  # Scaling factor (typically 2√ór)
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # Which layers to adapt
    lora_dropout=0.05,
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable params: 6,815,744 || all params: 8,036,065,280 || trainable: 0.085%

# Train
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=Dataset.from_json("training_data.jsonl"),
    max_seq_length=2048,
    dataset_text_field="text",
    peft_config=lora_config,
    args=TrainingArguments(
        output_dir="./fine-tuned-model",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2,
        warmup_ratio=0.1,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        save_strategy="epoch",
    )
)

trainer.train()
trainer.save_model("./fine-tuned-model")
</code></pre>
<p>The <code>print_trainable_parameters()</code> output confirms the LoRA efficiency: only 6.8M parameters are trained out of 8B total ‚Äî about 0.085%. This is why LoRA fits in 8GB VRAM where full fine-tuning would need 80GB or more. The <code>r=16</code> rank parameter is the primary knob controlling the adapter expressiveness; start here and only increase if the fine-tuned model underfits your task.</p>
<h2>Evaluating Fine-Tuned Models</h2>
<p>No fine-tuning project is complete without a rigorous evaluation against a held-out test set. The evaluation code below runs both the base model and your fine-tuned model against the same examples and compares scores side by side ‚Äî this is the only way to confirm that your fine-tuning actually improved the behavior you cared about rather than introducing regressions elsewhere.</p>
<pre><code class="language-python"># Build an evaluation set (separate from training data!)
# 10-20% of your data, never seen during training
eval_examples = load_eval_set("eval_data.jsonl")

def evaluate_model(model_id: str, eval_set: list) -> dict:
    results = []

    for example in eval_set:
        user_message = example["messages"][-2]["content"]  # User turn
        expected = example["messages"][-1]["content"]       # Expected assistant response

        # Generate response
        response = client.chat.completions.create(
            model=model_id,
            messages=[m for m in example["messages"][:-1]]  # All but last
        )
        actual = response.choices[0].message.content

        # Score: use LLM-as-judge for open-ended quality
        score = llm_judge_score(expected=expected, actual=actual)
        results.append({"score": score, "expected": expected, "actual": actual})

    return {
        "mean_score": sum(r["score"] for r in results) / len(results),
        "worst_examples": sorted(results, key=lambda x: x["score"])[:5]
    }

# Compare base model vs fine-tuned
base_eval = evaluate_model("gpt-4o-mini-2024-07-18", eval_examples)
ft_eval = evaluate_model(fine_tuned_model_id, eval_examples)

print(f"Base model score: {base_eval['mean_score']:.2%}")
print(f"Fine-tuned score: {ft_eval['mean_score']:.2%}")
# Target: fine-tuned should be 15-30% better for your specific task
</code></pre>
<p>The <code>worst_examples</code> output is as valuable as the mean score. Reviewing the five lowest-scoring responses reveals patterns in where your model still fails ‚Äî whether that is a missing edge case in your training data, a formatting inconsistency, or a category of user input that your training examples never covered. Fix those gaps and retrain rather than shipping a model you have not stress-tested.</p>
<p>The decision tree for LLM customization: start with prompt engineering (free, instant). If quality is insufficient after careful prompt iteration, try RAG (if the issue is missing knowledge). If style/format is the issue and prompts aren't working, try few-shot examples in the prompt. Only if all of the above fail for a well-defined, high-volume task does fine-tuning become worthwhile. The data preparation and evaluation infrastructure is the real investment ‚Äî the actual fine-tuning is the easy part.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">üìö</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Building LLM Apps with LangChain ‚Äî Udemy</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Hot</span></div><p class="text-xs text-gray-600">Build RAG systems, agents, and LLM-powered apps with Python and LangChain.</p></div><a href="https://www.udemy.com/course/langchain/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Hands-On Large Language Models</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">New</span></div><p class="text-xs text-gray-600">Practical guide to training, fine-tuning, and deploying LLMs.</p></div><a href="https://amzn.to/3Vpd8h5" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">AI Engineering by Chip Huyen</span></div><p class="text-xs text-gray-600">Building intelligent systems with foundation models ‚Äî from retrieval to agents.</p></div><a href="https://amzn.to/3Vrd1Rd" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Fine-Tuning%20LLMs%3A%20When%20to%20Fine-Tune%2C%20When%20to%20Prompt&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Ffine-tuning-llms%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Ffine-tuning-llms%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#when-fine-tuning-is-the-wrong-choice" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">When Fine-Tuning is the Wrong Choice</a></li><li class=""><a href="#when-fine-tuning-is-the-right-choice" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">When Fine-Tuning IS the Right Choice</a></li><li class=""><a href="#data-preparation-the-most-important-step" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Data Preparation: The Most Important Step</a></li><li class=""><a href="#openai-fine-tuning" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">OpenAI Fine-Tuning</a></li><li class=""><a href="#open-source-lora-fine-tuning-with-hugging-face" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Open-Source: LoRA Fine-Tuning with Hugging Face</a></li><li class=""><a href="#evaluating-fine-tuned-models" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Evaluating Fine-Tuned Models</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/llm-agents-tool-use/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Building AI Agents with Tool Use: From Chatbot to Autonomous Agent</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 23, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->agents</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->claude</span></div></article></a><a href="/blog/vector-embeddings-deep-dive/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Vector Embeddings: The Foundation of Modern AI Applications</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Every modern AI application ‚Äî semantic search, RAG, recommendations, duplicate detection ‚Äî is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantic‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 11, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->embeddings</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->vector database</span></div></article></a><a href="/blog/prompt-engineering-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Prompt Engineering: Advanced Techniques for Production LLMs</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Most prompt engineering tutorials stop at &quot;be specific and provide context.&quot; That&#x27;s necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-gra‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Feb 26, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->llm</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->prompt engineering</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">‚Üê Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS ‚Äî by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">¬© <!-- -->2026<!-- --> CodeSprintPro ¬∑ Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js ¬∑ TailwindCSS ¬∑ Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Fine-Tuning LLMs: When to Fine-Tune, When to Prompt","description":"Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.","date":"2025-03-27","category":"AI/ML","tags":["ai","llm","fine-tuning","lora","hugging face","openai","machine learning"],"featured":false,"affiliateSection":"ai-ml-books","slug":"fine-tuning-llms","readingTime":"10 min read","excerpt":"Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning ‚Äî fo‚Ä¶","contentHtml":"\u003cp\u003eFine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning ‚Äî for style consistency, domain-specific knowledge not in training data, or latency reduction ‚Äî it's transformative. This article helps you make the right choice and do the fine-tuning correctly.\u003c/p\u003e\n\u003ch2\u003eWhen Fine-Tuning is the Wrong Choice\u003c/h2\u003e\n\u003cp\u003eBefore spending days preparing data and dollars on GPU time, work through this decision tree. The examples below show the most common fine-tuning mistakes ‚Äî each one has a simpler, cheaper solution that most engineers overlook because fine-tuning feels like the \"serious\" approach.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eProblem: \"The model doesn't know our internal coding conventions\"\nWrong answer: Fine-tune\nRight answer: Add coding conventions to the system prompt\n              Or create a few-shot example template\n\nProblem: \"The model doesn't know our product documentation\"\nWrong answer: Fine-tune (your docs won't fit in training anyway)\nRight answer: RAG ‚Äî embed docs, retrieve relevant chunks at query time\n\nProblem: \"The model sometimes doesn't follow the output format\"\nWrong answer: Fine-tune on format examples\nRight answer: Use structured outputs (JSON mode) or better prompt instructions\n\nProblem: \"I want a cheaper, faster model\"\nWrong answer: Fine-tune GPT-4 (won't make it cheaper)\nRight answer: Distillation ‚Äî generate training data from GPT-4,\n              fine-tune GPT-4o-mini on that data\n              (can get GPT-4 quality at GPT-4o-mini price for specific tasks)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eWhen Fine-Tuning IS the Right Choice\u003c/h2\u003e\n\u003cp\u003eWith the anti-patterns clear, here are the scenarios where fine-tuning genuinely outperforms alternatives. The common thread across all of them is that you have a well-defined, high-volume, repeatable task where the investment in training data pays off over thousands or millions of inferences.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e‚úì Consistent style/tone at scale\n  - Customer service bot that must always match brand voice\n  - Code generation in company-specific patterns\n  - Technical writing with domain-specific terminology\n\n‚úì Short, repetitive prompts ‚Üí latency reduction\n  - System prompt has 10,000 tokens ‚Üí adds 500ms per call\n  - Fine-tuning \"bakes in\" the instructions ‚Üí 100-token system prompt\n  - Result: 80% latency reduction for same quality\n\n‚úì Specialized domain not in base training\n  - Medical codes (ICD-10, CPT) with specific formatting rules\n  - Legal document classification with jurisdiction-specific standards\n  - Proprietary data formats (your company's SQL dialect)\n\n‚úì Distillation (most common good use case)\n  - Use expensive GPT-4 to generate 1,000 high-quality examples\n  - Fine-tune cheap GPT-4o-mini on those examples\n  - Get 90% of GPT-4 quality at 10% of GPT-4 cost for your specific task\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eData Preparation: The Most Important Step\u003c/h2\u003e\n\u003cp\u003eFine-tuning quality is almost entirely determined by data quality. 500 excellent examples beat 5,000 mediocre ones.\u003c/p\u003e\n\u003cp\u003eYour training examples are the blueprint the model learns from ‚Äî if your examples have inconsistent tone, vary in format, or include sloppy responses, the model will learn those patterns faithfully. The gold standard is examples that represent exactly the behavior you want the model to exhibit at inference time, written by the person or team with the deepest knowledge of what \"good\" looks like for your use case.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Training data format for chat fine-tuning (OpenAI compatible)\ntraining_examples = [\n    {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a customer support agent for TechCorp. \"\n                           \"Always be concise, empathetic, and offer concrete next steps.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"My order hasn't arrived and it's been 10 days\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"I'm sorry for the delay ‚Äî 10 days is longer than expected. \"\n                           \"I've checked your order #TC-123456 and can see it's still in transit. \"\n                           \"I'll escalate this to our shipping team and you'll receive an update \"\n                           \"within 4 hours. Would you like me to also initiate a replacement shipment?\"\n            }\n        ]\n    },\n    # ... 499 more examples\n]\n\n# Save in JSONL format (one JSON object per line)\nimport json\nwith open(\"training_data.jsonl\", \"w\") as f:\n    for example in training_examples:\n        f.write(json.dumps(example) + \"\\n\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNotice the structure: each example has a system prompt that defines the persona, a realistic user message, and an ideal assistant response. The system prompt is the same across all examples because you want the model to internalize that persona ‚Äî if you vary the system prompt per example, the model learns multiple conflicting behaviors.\u003c/p\u003e\n\u003cp\u003eRunning the validation script below before submitting your training job will catch the common issues that silently degrade fine-tune quality, such as missing assistant turns or responses that are too short to teach the model anything useful.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Data quality checklist script\ndef validate_training_data(filepath: str) -\u003e dict:\n    issues = []\n    examples = []\n\n    with open(filepath) as f:\n        for i, line in enumerate(f):\n            try:\n                example = json.loads(line)\n                examples.append(example)\n            except json.JSONDecodeError as e:\n                issues.append(f\"Line {i+1}: Invalid JSON: {e}\")\n\n    # Check for common issues\n    for i, example in enumerate(examples):\n        messages = example.get(\"messages\", [])\n\n        # Check: has system, user, and assistant\n        roles = [m[\"role\"] for m in messages]\n        if \"assistant\" not in roles:\n            issues.append(f\"Example {i+1}: Missing assistant message\")\n\n        # Check: assistant response length distribution\n        for m in messages:\n            if m[\"role\"] == \"assistant\":\n                if len(m[\"content\"]) \u0026#x3C; 10:\n                    issues.append(f\"Example {i+1}: Very short assistant response\")\n                if len(m[\"content\"]) \u003e 2000:\n                    issues.append(f\"Example {i+1}: Very long assistant response (fine-tuning works best on focused, shorter outputs)\")\n\n    # Token count estimate\n    total_tokens = sum(\n        sum(len(m[\"content\"].split()) * 1.3 for m in ex[\"messages\"])  # rough estimate\n        for ex in examples\n    )\n\n    return {\n        \"total_examples\": len(examples),\n        \"issues\": issues,\n        \"estimated_tokens\": int(total_tokens),\n        \"estimated_cost_usd\": total_tokens / 1_000_000 * 8  # GPT-4o-mini: ~$8/M tokens for training\n    }\n\nreport = validate_training_data(\"training_data.jsonl\")\nprint(f\"Examples: {report['total_examples']}\")\nprint(f\"Issues: {len(report['issues'])}\")\nprint(f\"Estimated training cost: ${report['estimated_cost_usd']:.2f}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe estimated cost output lets you make a go/no-go decision before spending money. For most tasks, a 500-example dataset with typical response lengths costs well under $10 to train on GPT-4o-mini ‚Äî the data preparation effort is the real cost, not the compute.\u003c/p\u003e\n\u003ch2\u003eOpenAI Fine-Tuning\u003c/h2\u003e\n\u003cp\u003eWith your data validated, the OpenAI fine-tuning API makes the actual training a four-step process: upload your file, create a job, monitor until completion, and deploy the resulting model. The polling loop below is important ‚Äî training can take anywhere from minutes to hours depending on dataset size, and you need to handle both success and failure gracefully.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom openai import OpenAI\n\nclient = OpenAI()\n\n# Step 1: Upload training file\nwith open(\"training_data.jsonl\", \"rb\") as f:\n    file = client.files.create(file=f, purpose=\"fine-tune\")\n\nprint(f\"File ID: {file.id}\")\n\n# Step 2: Create fine-tuning job\njob = client.fine_tuning.jobs.create(\n    training_file=file.id,\n    model=\"gpt-4o-mini-2024-07-18\",  # Base model to fine-tune\n    hyperparameters={\n        \"n_epochs\": 3,              # 3-10 typically; more = risk of overfitting\n        \"batch_size\": \"auto\",       # Let OpenAI optimize\n        \"learning_rate_multiplier\": \"auto\"\n    },\n    suffix=\"customer-support-v1\"    # Your fine-tuned model name suffix\n)\n\nprint(f\"Job ID: {job.id}\")\n\n# Step 3: Monitor progress\nimport time\n\nwhile True:\n    job = client.fine_tuning.jobs.retrieve(job.id)\n    print(f\"Status: {job.status}, events: {len(job.integrations)}\")\n\n    if job.status in (\"succeeded\", \"failed\", \"cancelled\"):\n        break\n    time.sleep(60)\n\nif job.status == \"succeeded\":\n    model_id = job.fine_tuned_model\n    print(f\"Fine-tuned model: {model_id}\")\n    # e.g., \"ft:gpt-4o-mini-2024-07-18:your-org:customer-support-v1:abc123\"\n\n# Step 4: Use the fine-tuned model\nresponse = client.chat.completions.create(\n    model=model_id,\n    messages=[\n        {\"role\": \"user\", \"content\": \"My subscription charge looks wrong\"}\n    ]\n)\nprint(response.choices[0].message.content)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003en_epochs\u003c/code\u003e hyperparameter controls how many times the model sees your entire dataset during training. With 3 epochs and 500 examples, the model sees each example 3 times ‚Äî enough to learn the pattern without memorizing individual responses. If you see the fine-tuned model producing verbatim copies of training examples on similar inputs, that is a sign of overfitting and you should reduce epochs or increase dataset diversity.\u003c/p\u003e\n\u003ch2\u003eOpen-Source: LoRA Fine-Tuning with Hugging Face\u003c/h2\u003e\n\u003cp\u003eFor open-source models (Llama, Mistral, Qwen), LoRA (Low-Rank Adaptation) fine-tunes efficiently on consumer hardware.\u003c/p\u003e\n\u003cp\u003eFull fine-tuning of an 8B parameter model would require updating all 8 billion weights ‚Äî computationally prohibitive without expensive server-grade GPUs. LoRA solves this elegantly: instead of modifying the original weights, it adds small \"adapter\" matrices alongside specific layers and only trains those. The original model is frozen, and only the adapters (about 0.1% of total parameters) are updated.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# pip install transformers peft datasets trl accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom trl import SFTTrainer\nfrom datasets import Dataset\n\n# Load base model in 4-bit quantization (fits in 8GB VRAM)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=\"float16\",\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.1-8B-Instruct\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n\n# LoRA configuration: only fine-tune small adapters\n# Adapts 0.1-1% of parameters vs 100% for full fine-tuning\nlora_config = LoraConfig(\n    r=16,                           # Rank: higher = more expressive, more memory\n    lora_alpha=32,                  # Scaling factor (typically 2√ór)\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Which layers to adapt\n    lora_dropout=0.05,\n    task_type=TaskType.CAUSAL_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n# trainable params: 6,815,744 || all params: 8,036,065,280 || trainable: 0.085%\n\n# Train\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=Dataset.from_json(\"training_data.jsonl\"),\n    max_seq_length=2048,\n    dataset_text_field=\"text\",\n    peft_config=lora_config,\n    args=TrainingArguments(\n        output_dir=\"./fine-tuned-model\",\n        num_train_epochs=3,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=2,\n        warmup_ratio=0.1,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=10,\n        save_strategy=\"epoch\",\n    )\n)\n\ntrainer.train()\ntrainer.save_model(\"./fine-tuned-model\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eprint_trainable_parameters()\u003c/code\u003e output confirms the LoRA efficiency: only 6.8M parameters are trained out of 8B total ‚Äî about 0.085%. This is why LoRA fits in 8GB VRAM where full fine-tuning would need 80GB or more. The \u003ccode\u003er=16\u003c/code\u003e rank parameter is the primary knob controlling the adapter expressiveness; start here and only increase if the fine-tuned model underfits your task.\u003c/p\u003e\n\u003ch2\u003eEvaluating Fine-Tuned Models\u003c/h2\u003e\n\u003cp\u003eNo fine-tuning project is complete without a rigorous evaluation against a held-out test set. The evaluation code below runs both the base model and your fine-tuned model against the same examples and compares scores side by side ‚Äî this is the only way to confirm that your fine-tuning actually improved the behavior you cared about rather than introducing regressions elsewhere.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Build an evaluation set (separate from training data!)\n# 10-20% of your data, never seen during training\neval_examples = load_eval_set(\"eval_data.jsonl\")\n\ndef evaluate_model(model_id: str, eval_set: list) -\u003e dict:\n    results = []\n\n    for example in eval_set:\n        user_message = example[\"messages\"][-2][\"content\"]  # User turn\n        expected = example[\"messages\"][-1][\"content\"]       # Expected assistant response\n\n        # Generate response\n        response = client.chat.completions.create(\n            model=model_id,\n            messages=[m for m in example[\"messages\"][:-1]]  # All but last\n        )\n        actual = response.choices[0].message.content\n\n        # Score: use LLM-as-judge for open-ended quality\n        score = llm_judge_score(expected=expected, actual=actual)\n        results.append({\"score\": score, \"expected\": expected, \"actual\": actual})\n\n    return {\n        \"mean_score\": sum(r[\"score\"] for r in results) / len(results),\n        \"worst_examples\": sorted(results, key=lambda x: x[\"score\"])[:5]\n    }\n\n# Compare base model vs fine-tuned\nbase_eval = evaluate_model(\"gpt-4o-mini-2024-07-18\", eval_examples)\nft_eval = evaluate_model(fine_tuned_model_id, eval_examples)\n\nprint(f\"Base model score: {base_eval['mean_score']:.2%}\")\nprint(f\"Fine-tuned score: {ft_eval['mean_score']:.2%}\")\n# Target: fine-tuned should be 15-30% better for your specific task\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eworst_examples\u003c/code\u003e output is as valuable as the mean score. Reviewing the five lowest-scoring responses reveals patterns in where your model still fails ‚Äî whether that is a missing edge case in your training data, a formatting inconsistency, or a category of user input that your training examples never covered. Fix those gaps and retrain rather than shipping a model you have not stress-tested.\u003c/p\u003e\n\u003cp\u003eThe decision tree for LLM customization: start with prompt engineering (free, instant). If quality is insufficient after careful prompt iteration, try RAG (if the issue is missing knowledge). If style/format is the issue and prompts aren't working, try few-shot examples in the prompt. Only if all of the above fail for a well-defined, high-volume task does fine-tuning become worthwhile. The data preparation and evaluation infrastructure is the real investment ‚Äî the actual fine-tuning is the easy part.\u003c/p\u003e\n","tableOfContents":[{"id":"when-fine-tuning-is-the-wrong-choice","text":"When Fine-Tuning is the Wrong Choice","level":2},{"id":"when-fine-tuning-is-the-right-choice","text":"When Fine-Tuning IS the Right Choice","level":2},{"id":"data-preparation-the-most-important-step","text":"Data Preparation: The Most Important Step","level":2},{"id":"openai-fine-tuning","text":"OpenAI Fine-Tuning","level":2},{"id":"open-source-lora-fine-tuning-with-hugging-face","text":"Open-Source: LoRA Fine-Tuning with Hugging Face","level":2},{"id":"evaluating-fine-tuned-models","text":"Evaluating Fine-Tuned Models","level":2}]},"relatedPosts":[{"title":"Building AI Agents with Tool Use: From Chatbot to Autonomous Agent","description":"Build production AI agents using Claude's tool use API. Learn the agentic loop, error handling, multi-step reasoning, human-in-the-loop patterns, and how to build reliable autonomous systems.","date":"2025-03-23","category":"AI/ML","tags":["ai","agents","claude","tool use","llm","autonomous systems","python"],"featured":false,"affiliateSection":"ai-ml-books","slug":"llm-agents-tool-use","readingTime":"10 min read","excerpt":"A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio‚Ä¶"},{"title":"Vector Embeddings: The Foundation of Modern AI Applications","description":"Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.","date":"2025-03-11","category":"AI/ML","tags":["ai","embeddings","vector database","semantic search","rag","pgvector","pinecone"],"featured":false,"affiliateSection":"ai-ml-books","slug":"vector-embeddings-deep-dive","readingTime":"11 min read","excerpt":"Every modern AI application ‚Äî semantic search, RAG, recommendations, duplicate detection ‚Äî is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantic‚Ä¶"},{"title":"Prompt Engineering: Advanced Techniques for Production LLMs","description":"Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications.","date":"2025-02-26","category":"AI/ML","tags":["ai","llm","prompt engineering","gpt","claude","production"],"featured":false,"affiliateSection":"ai-ml-books","slug":"prompt-engineering-production","readingTime":"11 min read","excerpt":"Most prompt engineering tutorials stop at \"be specific and provide context.\" That's necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-gra‚Ä¶"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"fine-tuning-llms"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>