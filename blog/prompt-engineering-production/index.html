<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Prompt Engineering: Advanced Techniques for Production LLMs<!-- --> | CodeSprintPro</title><meta name="description" content="Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/prompt-engineering-production/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Prompt Engineering: Advanced Techniques for Production LLMs" data-next-head=""/><meta property="og:description" content="Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/prompt-engineering-production/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-02-26" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="AI/ML" data-next-head=""/><meta property="article:tag" content="ai" data-next-head=""/><meta property="article:tag" content="llm" data-next-head=""/><meta property="article:tag" content="prompt engineering" data-next-head=""/><meta property="article:tag" content="gpt" data-next-head=""/><meta property="article:tag" content="claude" data-next-head=""/><meta property="article:tag" content="production" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Prompt Engineering: Advanced Techniques for Production LLMs" data-next-head=""/><meta name="twitter:description" content="Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Prompt Engineering: Advanced Techniques for Production LLMs","description":"Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-02-26","dateModified":"2025-02-26","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/prompt-engineering-production/"},"keywords":"ai, llm, prompt engineering, gpt, claude, production","articleSection":"AI/ML"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Prompt Engineering: Advanced Techniques for Production LLMs</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">AI/ML</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Prompt Engineering: Advanced Techniques for Production LLMs</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>¬∑</span><span>February 26, 2025</span><span>¬∑</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->ai</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->llm</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->prompt engineering</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->gpt</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->claude</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->production</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Most prompt engineering tutorials stop at "be specific and provide context." That's necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-grade LLM applications: structured output, chain-of-thought, self-consistency, and the ReAct framework for agents.</p>
<h2>Mental Model: LLMs as Next-Token Predictors</h2>
<p>Every prompt engineering technique becomes intuitive once you internalize this: <strong>an LLM predicts the most probable next token given the context</strong>. This means:</p>
<ol>
<li>The model will continue patterns it sees in the prompt</li>
<li>Few-shot examples work because they shift the probability distribution</li>
<li>"Think step by step" works because showing the intermediate tokens makes the final answer more probable</li>
<li>The model doesn't "understand" your intent ‚Äî it finds the most statistically likely completion</li>
</ol>
<h2>Technique 1: Zero-Shot vs Few-Shot</h2>
<p>Zero-shot prompting asks the model to perform a task with instructions alone, while few-shot provides concrete examples first. Think of zero-shot as handing someone a job description and asking them to start immediately, versus few-shot as showing them three completed examples of the work before they begin. For simple tasks zero-shot is sufficient, but for nuanced classifications with subtle category distinctions, examples are far more reliable.</p>
<h3>Zero-Shot</h3>
<pre><code>Prompt: "Classify this email as spam or not spam:
Email: 'Congratulations! You've won $1,000,000. Click here to claim.'
Classification:"

Response: "Spam"
</code></pre>
<h3>Few-Shot (Better for Complex Classifications)</h3>
<p>Notice how each example below covers a distinct scenario ‚Äî billing, technical, and account issues. You're not just showing the format; you're showing the model the boundaries between categories by example, which is far more effective than trying to describe those boundaries in words.</p>
<pre><code>Prompt: "Classify customer support tickets. Categories: BILLING, TECHNICAL, ACCOUNT, GENERAL.

Example 1:
Ticket: "My invoice shows a charge I didn't authorize."
Category: BILLING

Example 2:
Ticket: "The app crashes when I try to export to PDF."
Category: TECHNICAL

Example 3:
Ticket: "I need to transfer my account to a new email address."
Category: ACCOUNT

Now classify:
Ticket: "How do I upgrade my subscription plan?"
Category:"

Response: "BILLING"
</code></pre>
<p><strong>Few-shot guidelines:</strong></p>
<ul>
<li>Use 3-8 examples (diminishing returns beyond 8 for most tasks)</li>
<li>Examples should cover edge cases, not just typical cases</li>
<li>Maintain consistent format between examples and the query</li>
<li>Order matters: recent examples have more influence (recency bias)</li>
</ul>
<h2>Technique 2: Chain-of-Thought (CoT)</h2>
<p>CoT dramatically improves reasoning on math, logic, and multi-step problems by showing the model that intermediate reasoning is expected.</p>
<p>The intuition is simple: when you force the model to write out its reasoning step by step, each intermediate conclusion becomes part of the context for the next step. This is why you get correct multi-step answers with CoT that you'd never reliably get from a direct "just tell me the answer" prompt ‚Äî the intermediate steps guide the model toward the right final token.</p>
<pre><code># Without CoT ‚Äî often wrong on math
Prompt: "A store sells apples for $0.50 each and oranges for $0.75 each.
If John bought 3 apples and 5 oranges, how much did he spend?"

Response: "$3.25"  # Correct, but unreliable for harder problems

# With CoT ‚Äî much more reliable
Prompt: "A store sells apples for $0.50 each and oranges for $0.75 each.
If John bought 3 apples and 5 oranges, how much did he spend?

Let's think step by step:"

Response:
"1. Cost of apples: 3 √ó $0.50 = $1.50
2. Cost of oranges: 5 √ó $0.75 = $3.75
3. Total: $1.50 + $3.75 = $5.25

John spent $5.25."
</code></pre>
<p><strong>Zero-shot CoT trigger phrases</strong> (any of these work):</p>
<ul>
<li>"Let's think step by step"</li>
<li>"Think through this carefully"</li>
<li>"Reason through each step"</li>
<li>"Let me work through this"</li>
</ul>
<p><strong>Few-shot CoT</strong> ‚Äî provide examples with explicit reasoning. By showing worked examples, you are training the model's in-context behavior to produce the same style of detailed, step-by-step trace before arriving at an answer.</p>
<pre><code class="language-python">cot_examples = """
Q: If a train travels 60 mph and needs to cover 150 miles, how long does it take?
A: Let me work through this step by step:
   - Distance = 150 miles
   - Speed = 60 mph
   - Time = Distance / Speed = 150 / 60 = 2.5 hours
   The answer is 2.5 hours (2 hours and 30 minutes).

Q: A recipe calls for 2 cups of flour for 12 cookies.
   How much flour is needed for 30 cookies?
A: Let me work through this step by step:
   - Ratio: 2 cups / 12 cookies = 1/6 cup per cookie
   - For 30 cookies: 30 √ó (1/6) = 5 cups
   The answer is 5 cups of flour.
"""
</code></pre>
<p>With few-shot CoT in hand, you have a powerful building block for reliable reasoning. The next challenge is making structured outputs that your application can actually parse.</p>
<h2>Technique 3: Structured Output</h2>
<p>Production systems need parseable output, not prose.</p>
<p>When you build a real application around an LLM, you almost always need to extract specific fields from the response ‚Äî a sentiment score, a list of issues, a priority level. The approach below uses Pydantic models to define the exact schema you expect, which both constrains the model's output and gives you Python objects you can work with directly in your code.</p>
<pre><code class="language-python">from openai import OpenAI
from pydantic import BaseModel
from typing import Optional
import json

client = OpenAI()

class ProductAnalysis(BaseModel):
    sentiment: str          # "positive" | "negative" | "neutral"
    score: float            # 0.0 to 1.0
    key_issues: list[str]
    recommended_action: str
    priority: str           # "low" | "medium" | "high"

# Method 1: JSON mode (OpenAI)
response = client.chat.completions.create(
    model="gpt-4o",
    response_format={"type": "json_object"},
    messages=[
        {"role": "system", "content": """You are a customer feedback analyst.
        Always respond with valid JSON matching this schema:
        {
          "sentiment": "positive|negative|neutral",
          "score": &#x3C;float 0-1>,
          "key_issues": [&#x3C;list of strings>],
          "recommended_action": &#x3C;string>,
          "priority": "low|medium|high"
        }"""},
        {"role": "user", "content": f"Analyze this review: {review_text}"}
    ]
)
result = ProductAnalysis(**json.loads(response.choices[0].message.content))

# Method 2: Structured outputs (OpenAI, more reliable)
response = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[...],
    response_format=ProductAnalysis,
)
result: ProductAnalysis = response.choices[0].message.parsed
</code></pre>
<p>Method 2 (structured outputs) is preferred over Method 1 (JSON mode) when available: structured outputs use constrained decoding to guarantee the schema is satisfied at the token level, whereas JSON mode merely requests JSON and can still produce invalid or mismatched structures under edge cases.</p>
<p><strong>Prompt patterns for structured output:</strong></p>
<pre><code>System: You must respond ONLY with valid JSON. No explanation, no markdown, no prose.
        The JSON must match this exact schema: {...}

User: [Your request]

# Common mistakes:
‚ùå "Respond in JSON format" ‚Äî model may wrap in markdown ```json
‚ùå No schema ‚Äî model invents fields
‚úì Provide exact schema + "ONLY valid JSON" instruction
‚úì Use JSON mode or structured outputs API
</code></pre>
<h2>Technique 4: Self-Consistency</h2>
<p>For high-stakes questions, generate multiple responses and take the majority vote. This reduces variance from stochastic generation.</p>
<p>Think of self-consistency like polling multiple experts: any single expert might reason to the wrong conclusion on a hard problem, but if you ask five experts independently and four of them agree, you can be much more confident in that answer. The key is using <code>temperature > 0</code> so each sample takes a genuinely different reasoning path.</p>
<pre><code class="language-python">def self_consistent_answer(question: str, n_samples: int = 5) -> str:
    """Generate n answers with temperature > 0, take majority vote."""

    answers = []
    for _ in range(n_samples):
        response = client.chat.completions.create(
            model="gpt-4o",
            temperature=0.7,  # Non-zero for diversity
            messages=[
                {"role": "system", "content": "Solve this step by step. End with 'Answer: &#x3C;value>'"},
                {"role": "user", "content": question}
            ]
        )
        text = response.choices[0].message.content
        # Extract final answer
        if "Answer:" in text:
            answer = text.split("Answer:")[-1].strip()
            answers.append(answer)

    # Majority vote
    from collections import Counter
    return Counter(answers).most_common(1)[0][0]

# Best for: math, logic, classification (discrete answers)
# Not useful for: creative writing, open-ended questions
</code></pre>
<p><strong>Accuracy improvement (GSM8K math benchmark):</strong></p>
<ul>
<li>GPT-4 zero-shot: 87%</li>
<li>GPT-4 CoT: 92%</li>
<li>GPT-4 CoT + self-consistency (n=40): 97%</li>
</ul>
<p>The 5-point jump from CoT to self-consistency (92% ‚Üí 97%) comes entirely from running the same prompt multiple times and voting ‚Äî no additional model capability required. This is a powerful lever when accuracy matters more than cost.</p>
<h2>Technique 5: ReAct ‚Äî Reasoning + Acting for Agents</h2>
<p>ReAct interleaves <strong>Re</strong>asoning and <strong>Act</strong>ing ‚Äî the model thinks, takes an action (tool call), observes the result, and continues reasoning. This is the backbone of most LLM agents.</p>
<p>The key insight of ReAct is that you don't need to give the model all information upfront. Instead, you give it tools and let it pull in exactly the information it needs, when it needs it. The structured <code>Thought ‚Üí Action ‚Üí Observation</code> format keeps the reasoning transparent and makes it easy to debug where the agent went wrong.</p>
<pre><code>System: You are an assistant that can use tools to answer questions.
Available tools:
- search(query): Search the web
- calculate(expression): Evaluate a math expression
- get_stock_price(ticker): Get current stock price

Think in this format:
Thought: [Your reasoning]
Action: tool_name(arguments)
Observation: [Tool result]
... (repeat as needed)
Final Answer: [Your answer]

User: What is 15% of the current price of AAPL?

Response:
Thought: I need to get the current price of AAPL first, then calculate 15% of it.
Action: get_stock_price(AAPL)
Observation: AAPL current price: $195.42

Thought: Now I'll calculate 15% of $195.42.
Action: calculate(195.42 * 0.15)
Observation: 29.313

Final Answer: 15% of the current AAPL price ($195.42) is approximately $29.31.
</code></pre>
<p>The implementation below translates this pattern into code. The <code>stop=["Observation:"]</code> parameter is the key mechanism: it makes the model halt before writing the observation, so your code can inject the actual tool result rather than letting the model hallucinate one.</p>
<pre><code class="language-python"># ReAct loop implementation
def react_agent(user_query: str, tools: dict, max_iterations: int = 10) -> str:
    messages = [
        {"role": "system", "content": REACT_SYSTEM_PROMPT},
        {"role": "user", "content": user_query}
    ]

    for iteration in range(max_iterations):
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            stop=["Observation:"]  # Stop before observation to inject tool result
        )

        thought_action = response.choices[0].message.content
        messages.append({"role": "assistant", "content": thought_action})

        if "Final Answer:" in thought_action:
            return thought_action.split("Final Answer:")[-1].strip()

        # Parse and execute action
        if "Action:" in thought_action:
            action_line = [l for l in thought_action.split("\n") if l.startswith("Action:")][0]
            tool_name, args = parse_action(action_line)

            result = tools[tool_name](args)
            observation = f"Observation: {result}\n"
            messages.append({"role": "user", "content": observation})

    return "Max iterations reached"
</code></pre>
<h2>Technique 6: Prompt Caching</h2>
<p>For production systems with expensive prompts, use prompt caching to reduce latency and cost.</p>
<p>When your system prompt contains a large document ‚Äî a legal contract, a product catalog, a policy manual ‚Äî you pay embedding cost for that document on every single API call. Prompt caching solves this by storing the KV cache of the processed prompt server-side, so repeated calls reuse the expensive computation instead of redoing it.</p>
<pre><code class="language-python"># Anthropic Claude: prefix caching
# Mark expensive context (documents, instructions) for caching
response = anthropic.messages.create(
    model="claude-opus-4-6",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "You are a legal contract analyzer...",
        },
        {
            "type": "text",
            "text": full_contract_text,  # 50,000 tokens ‚Äî expensive
            "cache_control": {"type": "ephemeral"}  # Cache this for 5 minutes
        }
    ],
    messages=[{"role": "user", "content": "What are the termination clauses?"}]
)
# First call: full cost. Subsequent calls within 5 min: 90% cost reduction
# Cached reads: $0.30/MTok vs write $3.75/MTok (Claude Sonnet)
</code></pre>
<p>The 90% cost reduction on cache hits makes prompt caching one of the highest-ROI optimizations for production systems that serve many users against a shared, large context. It also reduces latency since the model skips re-processing the cached prefix.</p>
<h2>Evaluation: Measuring Prompt Quality</h2>
<p>Building an eval suite is the step most teams skip ‚Äî and then they wonder why their prompts feel unpredictable. The approach below treats prompt iteration the same way you'd treat code iteration: define expected outputs, measure actual outputs, and only ship changes that improve the metric.</p>
<pre><code class="language-python"># Build an eval dataset ‚Äî sample 50-200 real examples
eval_dataset = [
    {
        "input": "This product broke after 2 days",
        "expected": {"sentiment": "negative", "priority": "high"},
    },
    # ...
]

def evaluate_prompt(prompt_template: str, dataset: list) -> dict:
    results = []
    for example in dataset:
        response = run_with_prompt(prompt_template, example["input"])
        results.append({
            "correct": response == example["expected"],
            "latency_ms": response.latency,
            "tokens": response.usage.total_tokens,
        })

    return {
        "accuracy": sum(r["correct"] for r in results) / len(results),
        "avg_latency_ms": sum(r["latency_ms"] for r in results) / len(results),
        "avg_tokens": sum(r["tokens"] for r in results) / len(results),
        "cost_per_1k": sum(r["tokens"] for r in results) / 1000 * TOKEN_PRICE,
    }

# A/B test prompts before shipping to production
baseline = evaluate_prompt(OLD_PROMPT, eval_dataset)
candidate = evaluate_prompt(NEW_PROMPT, eval_dataset)
print(f"Accuracy: {baseline['accuracy']:.1%} ‚Üí {candidate['accuracy']:.1%}")
</code></pre>
<p>The highest-leverage prompt engineering investment is building an evaluation suite. Without measurement, you cannot know if a prompt change improves quality or regresses it. Treat prompt changes like code changes ‚Äî test them before deploying.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">üìö</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Building LLM Apps with LangChain ‚Äî Udemy</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Hot</span></div><p class="text-xs text-gray-600">Build RAG systems, agents, and LLM-powered apps with Python and LangChain.</p></div><a href="https://www.udemy.com/course/langchain/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Hands-On Large Language Models</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">New</span></div><p class="text-xs text-gray-600">Practical guide to training, fine-tuning, and deploying LLMs.</p></div><a href="https://amzn.to/3Vpd8h5" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">AI Engineering by Chip Huyen</span></div><p class="text-xs text-gray-600">Building intelligent systems with foundation models ‚Äî from retrieval to agents.</p></div><a href="https://amzn.to/3Vrd1Rd" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Prompt%20Engineering%3A%20Advanced%20Techniques%20for%20Production%20LLMs&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fprompt-engineering-production%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fprompt-engineering-production%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#mental-model-llms-as-next-token-predictors" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Mental Model: LLMs as Next-Token Predictors</a></li><li class=""><a href="#technique-1-zero-shot-vs-few-shot" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Technique 1: Zero-Shot vs Few-Shot</a></li><li class="ml-4"><a href="#zero-shot" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Zero-Shot</a></li><li class="ml-4"><a href="#few-shot-better-for-complex-classifications" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Few-Shot (Better for Complex Classifications)</a></li><li class=""><a href="#technique-2-chain-of-thought-cot" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Technique 2: Chain-of-Thought (CoT)</a></li><li class=""><a href="#technique-3-structured-output" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Technique 3: Structured Output</a></li><li class=""><a href="#technique-4-self-consistency" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Technique 4: Self-Consistency</a></li><li class=""><a href="#technique-5-react-reasoning-acting-for-agents" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Technique 5: ReAct ‚Äî Reasoning + Acting for Agents</a></li><li class=""><a href="#technique-6-prompt-caching" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Technique 6: Prompt Caching</a></li><li class=""><a href="#evaluation-measuring-prompt-quality" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Evaluation: Measuring Prompt Quality</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/fine-tuning-llms/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Fine-Tuning LLMs: When to Fine-Tune, When to Prompt</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning ‚Äî fo‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 27, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->llm</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->fine-tuning</span></div></article></a><a href="/blog/llm-agents-tool-use/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Building AI Agents with Tool Use: From Chatbot to Autonomous Agent</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 23, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->agents</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->claude</span></div></article></a><a href="/blog/vector-embeddings-deep-dive/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Vector Embeddings: The Foundation of Modern AI Applications</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Every modern AI application ‚Äî semantic search, RAG, recommendations, duplicate detection ‚Äî is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantic‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 11, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->embeddings</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->vector database</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">‚Üê Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS ‚Äî by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">¬© <!-- -->2026<!-- --> CodeSprintPro ¬∑ Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js ¬∑ TailwindCSS ¬∑ Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Prompt Engineering: Advanced Techniques for Production LLMs","description":"Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications.","date":"2025-02-26","category":"AI/ML","tags":["ai","llm","prompt engineering","gpt","claude","production"],"featured":false,"affiliateSection":"ai-ml-books","slug":"prompt-engineering-production","readingTime":"11 min read","excerpt":"Most prompt engineering tutorials stop at \"be specific and provide context.\" That's necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-gra‚Ä¶","contentHtml":"\u003cp\u003eMost prompt engineering tutorials stop at \"be specific and provide context.\" That's necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-grade LLM applications: structured output, chain-of-thought, self-consistency, and the ReAct framework for agents.\u003c/p\u003e\n\u003ch2\u003eMental Model: LLMs as Next-Token Predictors\u003c/h2\u003e\n\u003cp\u003eEvery prompt engineering technique becomes intuitive once you internalize this: \u003cstrong\u003ean LLM predicts the most probable next token given the context\u003c/strong\u003e. This means:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe model will continue patterns it sees in the prompt\u003c/li\u003e\n\u003cli\u003eFew-shot examples work because they shift the probability distribution\u003c/li\u003e\n\u003cli\u003e\"Think step by step\" works because showing the intermediate tokens makes the final answer more probable\u003c/li\u003e\n\u003cli\u003eThe model doesn't \"understand\" your intent ‚Äî it finds the most statistically likely completion\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eTechnique 1: Zero-Shot vs Few-Shot\u003c/h2\u003e\n\u003cp\u003eZero-shot prompting asks the model to perform a task with instructions alone, while few-shot provides concrete examples first. Think of zero-shot as handing someone a job description and asking them to start immediately, versus few-shot as showing them three completed examples of the work before they begin. For simple tasks zero-shot is sufficient, but for nuanced classifications with subtle category distinctions, examples are far more reliable.\u003c/p\u003e\n\u003ch3\u003eZero-Shot\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003ePrompt: \"Classify this email as spam or not spam:\nEmail: 'Congratulations! You've won $1,000,000. Click here to claim.'\nClassification:\"\n\nResponse: \"Spam\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eFew-Shot (Better for Complex Classifications)\u003c/h3\u003e\n\u003cp\u003eNotice how each example below covers a distinct scenario ‚Äî billing, technical, and account issues. You're not just showing the format; you're showing the model the boundaries between categories by example, which is far more effective than trying to describe those boundaries in words.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ePrompt: \"Classify customer support tickets. Categories: BILLING, TECHNICAL, ACCOUNT, GENERAL.\n\nExample 1:\nTicket: \"My invoice shows a charge I didn't authorize.\"\nCategory: BILLING\n\nExample 2:\nTicket: \"The app crashes when I try to export to PDF.\"\nCategory: TECHNICAL\n\nExample 3:\nTicket: \"I need to transfer my account to a new email address.\"\nCategory: ACCOUNT\n\nNow classify:\nTicket: \"How do I upgrade my subscription plan?\"\nCategory:\"\n\nResponse: \"BILLING\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eFew-shot guidelines:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUse 3-8 examples (diminishing returns beyond 8 for most tasks)\u003c/li\u003e\n\u003cli\u003eExamples should cover edge cases, not just typical cases\u003c/li\u003e\n\u003cli\u003eMaintain consistent format between examples and the query\u003c/li\u003e\n\u003cli\u003eOrder matters: recent examples have more influence (recency bias)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eTechnique 2: Chain-of-Thought (CoT)\u003c/h2\u003e\n\u003cp\u003eCoT dramatically improves reasoning on math, logic, and multi-step problems by showing the model that intermediate reasoning is expected.\u003c/p\u003e\n\u003cp\u003eThe intuition is simple: when you force the model to write out its reasoning step by step, each intermediate conclusion becomes part of the context for the next step. This is why you get correct multi-step answers with CoT that you'd never reliably get from a direct \"just tell me the answer\" prompt ‚Äî the intermediate steps guide the model toward the right final token.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Without CoT ‚Äî often wrong on math\nPrompt: \"A store sells apples for $0.50 each and oranges for $0.75 each.\nIf John bought 3 apples and 5 oranges, how much did he spend?\"\n\nResponse: \"$3.25\"  # Correct, but unreliable for harder problems\n\n# With CoT ‚Äî much more reliable\nPrompt: \"A store sells apples for $0.50 each and oranges for $0.75 each.\nIf John bought 3 apples and 5 oranges, how much did he spend?\n\nLet's think step by step:\"\n\nResponse:\n\"1. Cost of apples: 3 √ó $0.50 = $1.50\n2. Cost of oranges: 5 √ó $0.75 = $3.75\n3. Total: $1.50 + $3.75 = $5.25\n\nJohn spent $5.25.\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eZero-shot CoT trigger phrases\u003c/strong\u003e (any of these work):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"Let's think step by step\"\u003c/li\u003e\n\u003cli\u003e\"Think through this carefully\"\u003c/li\u003e\n\u003cli\u003e\"Reason through each step\"\u003c/li\u003e\n\u003cli\u003e\"Let me work through this\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eFew-shot CoT\u003c/strong\u003e ‚Äî provide examples with explicit reasoning. By showing worked examples, you are training the model's in-context behavior to produce the same style of detailed, step-by-step trace before arriving at an answer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ecot_examples = \"\"\"\nQ: If a train travels 60 mph and needs to cover 150 miles, how long does it take?\nA: Let me work through this step by step:\n   - Distance = 150 miles\n   - Speed = 60 mph\n   - Time = Distance / Speed = 150 / 60 = 2.5 hours\n   The answer is 2.5 hours (2 hours and 30 minutes).\n\nQ: A recipe calls for 2 cups of flour for 12 cookies.\n   How much flour is needed for 30 cookies?\nA: Let me work through this step by step:\n   - Ratio: 2 cups / 12 cookies = 1/6 cup per cookie\n   - For 30 cookies: 30 √ó (1/6) = 5 cups\n   The answer is 5 cups of flour.\n\"\"\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWith few-shot CoT in hand, you have a powerful building block for reliable reasoning. The next challenge is making structured outputs that your application can actually parse.\u003c/p\u003e\n\u003ch2\u003eTechnique 3: Structured Output\u003c/h2\u003e\n\u003cp\u003eProduction systems need parseable output, not prose.\u003c/p\u003e\n\u003cp\u003eWhen you build a real application around an LLM, you almost always need to extract specific fields from the response ‚Äî a sentiment score, a list of issues, a priority level. The approach below uses Pydantic models to define the exact schema you expect, which both constrains the model's output and gives you Python objects you can work with directly in your code.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import Optional\nimport json\n\nclient = OpenAI()\n\nclass ProductAnalysis(BaseModel):\n    sentiment: str          # \"positive\" | \"negative\" | \"neutral\"\n    score: float            # 0.0 to 1.0\n    key_issues: list[str]\n    recommended_action: str\n    priority: str           # \"low\" | \"medium\" | \"high\"\n\n# Method 1: JSON mode (OpenAI)\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    response_format={\"type\": \"json_object\"},\n    messages=[\n        {\"role\": \"system\", \"content\": \"\"\"You are a customer feedback analyst.\n        Always respond with valid JSON matching this schema:\n        {\n          \"sentiment\": \"positive|negative|neutral\",\n          \"score\": \u0026#x3C;float 0-1\u003e,\n          \"key_issues\": [\u0026#x3C;list of strings\u003e],\n          \"recommended_action\": \u0026#x3C;string\u003e,\n          \"priority\": \"low|medium|high\"\n        }\"\"\"},\n        {\"role\": \"user\", \"content\": f\"Analyze this review: {review_text}\"}\n    ]\n)\nresult = ProductAnalysis(**json.loads(response.choices[0].message.content))\n\n# Method 2: Structured outputs (OpenAI, more reliable)\nresponse = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[...],\n    response_format=ProductAnalysis,\n)\nresult: ProductAnalysis = response.choices[0].message.parsed\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMethod 2 (structured outputs) is preferred over Method 1 (JSON mode) when available: structured outputs use constrained decoding to guarantee the schema is satisfied at the token level, whereas JSON mode merely requests JSON and can still produce invalid or mismatched structures under edge cases.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePrompt patterns for structured output:\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSystem: You must respond ONLY with valid JSON. No explanation, no markdown, no prose.\n        The JSON must match this exact schema: {...}\n\nUser: [Your request]\n\n# Common mistakes:\n‚ùå \"Respond in JSON format\" ‚Äî model may wrap in markdown ```json\n‚ùå No schema ‚Äî model invents fields\n‚úì Provide exact schema + \"ONLY valid JSON\" instruction\n‚úì Use JSON mode or structured outputs API\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eTechnique 4: Self-Consistency\u003c/h2\u003e\n\u003cp\u003eFor high-stakes questions, generate multiple responses and take the majority vote. This reduces variance from stochastic generation.\u003c/p\u003e\n\u003cp\u003eThink of self-consistency like polling multiple experts: any single expert might reason to the wrong conclusion on a hard problem, but if you ask five experts independently and four of them agree, you can be much more confident in that answer. The key is using \u003ccode\u003etemperature \u003e 0\u003c/code\u003e so each sample takes a genuinely different reasoning path.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef self_consistent_answer(question: str, n_samples: int = 5) -\u003e str:\n    \"\"\"Generate n answers with temperature \u003e 0, take majority vote.\"\"\"\n\n    answers = []\n    for _ in range(n_samples):\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            temperature=0.7,  # Non-zero for diversity\n            messages=[\n                {\"role\": \"system\", \"content\": \"Solve this step by step. End with 'Answer: \u0026#x3C;value\u003e'\"},\n                {\"role\": \"user\", \"content\": question}\n            ]\n        )\n        text = response.choices[0].message.content\n        # Extract final answer\n        if \"Answer:\" in text:\n            answer = text.split(\"Answer:\")[-1].strip()\n            answers.append(answer)\n\n    # Majority vote\n    from collections import Counter\n    return Counter(answers).most_common(1)[0][0]\n\n# Best for: math, logic, classification (discrete answers)\n# Not useful for: creative writing, open-ended questions\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eAccuracy improvement (GSM8K math benchmark):\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGPT-4 zero-shot: 87%\u003c/li\u003e\n\u003cli\u003eGPT-4 CoT: 92%\u003c/li\u003e\n\u003cli\u003eGPT-4 CoT + self-consistency (n=40): 97%\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe 5-point jump from CoT to self-consistency (92% ‚Üí 97%) comes entirely from running the same prompt multiple times and voting ‚Äî no additional model capability required. This is a powerful lever when accuracy matters more than cost.\u003c/p\u003e\n\u003ch2\u003eTechnique 5: ReAct ‚Äî Reasoning + Acting for Agents\u003c/h2\u003e\n\u003cp\u003eReAct interleaves \u003cstrong\u003eRe\u003c/strong\u003easoning and \u003cstrong\u003eAct\u003c/strong\u003eing ‚Äî the model thinks, takes an action (tool call), observes the result, and continues reasoning. This is the backbone of most LLM agents.\u003c/p\u003e\n\u003cp\u003eThe key insight of ReAct is that you don't need to give the model all information upfront. Instead, you give it tools and let it pull in exactly the information it needs, when it needs it. The structured \u003ccode\u003eThought ‚Üí Action ‚Üí Observation\u003c/code\u003e format keeps the reasoning transparent and makes it easy to debug where the agent went wrong.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSystem: You are an assistant that can use tools to answer questions.\nAvailable tools:\n- search(query): Search the web\n- calculate(expression): Evaluate a math expression\n- get_stock_price(ticker): Get current stock price\n\nThink in this format:\nThought: [Your reasoning]\nAction: tool_name(arguments)\nObservation: [Tool result]\n... (repeat as needed)\nFinal Answer: [Your answer]\n\nUser: What is 15% of the current price of AAPL?\n\nResponse:\nThought: I need to get the current price of AAPL first, then calculate 15% of it.\nAction: get_stock_price(AAPL)\nObservation: AAPL current price: $195.42\n\nThought: Now I'll calculate 15% of $195.42.\nAction: calculate(195.42 * 0.15)\nObservation: 29.313\n\nFinal Answer: 15% of the current AAPL price ($195.42) is approximately $29.31.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe implementation below translates this pattern into code. The \u003ccode\u003estop=[\"Observation:\"]\u003c/code\u003e parameter is the key mechanism: it makes the model halt before writing the observation, so your code can inject the actual tool result rather than letting the model hallucinate one.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# ReAct loop implementation\ndef react_agent(user_query: str, tools: dict, max_iterations: int = 10) -\u003e str:\n    messages = [\n        {\"role\": \"system\", \"content\": REACT_SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": user_query}\n    ]\n\n    for iteration in range(max_iterations):\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages,\n            stop=[\"Observation:\"]  # Stop before observation to inject tool result\n        )\n\n        thought_action = response.choices[0].message.content\n        messages.append({\"role\": \"assistant\", \"content\": thought_action})\n\n        if \"Final Answer:\" in thought_action:\n            return thought_action.split(\"Final Answer:\")[-1].strip()\n\n        # Parse and execute action\n        if \"Action:\" in thought_action:\n            action_line = [l for l in thought_action.split(\"\\n\") if l.startswith(\"Action:\")][0]\n            tool_name, args = parse_action(action_line)\n\n            result = tools[tool_name](args)\n            observation = f\"Observation: {result}\\n\"\n            messages.append({\"role\": \"user\", \"content\": observation})\n\n    return \"Max iterations reached\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eTechnique 6: Prompt Caching\u003c/h2\u003e\n\u003cp\u003eFor production systems with expensive prompts, use prompt caching to reduce latency and cost.\u003c/p\u003e\n\u003cp\u003eWhen your system prompt contains a large document ‚Äî a legal contract, a product catalog, a policy manual ‚Äî you pay embedding cost for that document on every single API call. Prompt caching solves this by storing the KV cache of the processed prompt server-side, so repeated calls reuse the expensive computation instead of redoing it.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Anthropic Claude: prefix caching\n# Mark expensive context (documents, instructions) for caching\nresponse = anthropic.messages.create(\n    model=\"claude-opus-4-6\",\n    max_tokens=1024,\n    system=[\n        {\n            \"type\": \"text\",\n            \"text\": \"You are a legal contract analyzer...\",\n        },\n        {\n            \"type\": \"text\",\n            \"text\": full_contract_text,  # 50,000 tokens ‚Äî expensive\n            \"cache_control\": {\"type\": \"ephemeral\"}  # Cache this for 5 minutes\n        }\n    ],\n    messages=[{\"role\": \"user\", \"content\": \"What are the termination clauses?\"}]\n)\n# First call: full cost. Subsequent calls within 5 min: 90% cost reduction\n# Cached reads: $0.30/MTok vs write $3.75/MTok (Claude Sonnet)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe 90% cost reduction on cache hits makes prompt caching one of the highest-ROI optimizations for production systems that serve many users against a shared, large context. It also reduces latency since the model skips re-processing the cached prefix.\u003c/p\u003e\n\u003ch2\u003eEvaluation: Measuring Prompt Quality\u003c/h2\u003e\n\u003cp\u003eBuilding an eval suite is the step most teams skip ‚Äî and then they wonder why their prompts feel unpredictable. The approach below treats prompt iteration the same way you'd treat code iteration: define expected outputs, measure actual outputs, and only ship changes that improve the metric.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Build an eval dataset ‚Äî sample 50-200 real examples\neval_dataset = [\n    {\n        \"input\": \"This product broke after 2 days\",\n        \"expected\": {\"sentiment\": \"negative\", \"priority\": \"high\"},\n    },\n    # ...\n]\n\ndef evaluate_prompt(prompt_template: str, dataset: list) -\u003e dict:\n    results = []\n    for example in dataset:\n        response = run_with_prompt(prompt_template, example[\"input\"])\n        results.append({\n            \"correct\": response == example[\"expected\"],\n            \"latency_ms\": response.latency,\n            \"tokens\": response.usage.total_tokens,\n        })\n\n    return {\n        \"accuracy\": sum(r[\"correct\"] for r in results) / len(results),\n        \"avg_latency_ms\": sum(r[\"latency_ms\"] for r in results) / len(results),\n        \"avg_tokens\": sum(r[\"tokens\"] for r in results) / len(results),\n        \"cost_per_1k\": sum(r[\"tokens\"] for r in results) / 1000 * TOKEN_PRICE,\n    }\n\n# A/B test prompts before shipping to production\nbaseline = evaluate_prompt(OLD_PROMPT, eval_dataset)\ncandidate = evaluate_prompt(NEW_PROMPT, eval_dataset)\nprint(f\"Accuracy: {baseline['accuracy']:.1%} ‚Üí {candidate['accuracy']:.1%}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe highest-leverage prompt engineering investment is building an evaluation suite. Without measurement, you cannot know if a prompt change improves quality or regresses it. Treat prompt changes like code changes ‚Äî test them before deploying.\u003c/p\u003e\n","tableOfContents":[{"id":"mental-model-llms-as-next-token-predictors","text":"Mental Model: LLMs as Next-Token Predictors","level":2},{"id":"technique-1-zero-shot-vs-few-shot","text":"Technique 1: Zero-Shot vs Few-Shot","level":2},{"id":"zero-shot","text":"Zero-Shot","level":3},{"id":"few-shot-better-for-complex-classifications","text":"Few-Shot (Better for Complex Classifications)","level":3},{"id":"technique-2-chain-of-thought-cot","text":"Technique 2: Chain-of-Thought (CoT)","level":2},{"id":"technique-3-structured-output","text":"Technique 3: Structured Output","level":2},{"id":"technique-4-self-consistency","text":"Technique 4: Self-Consistency","level":2},{"id":"technique-5-react-reasoning-acting-for-agents","text":"Technique 5: ReAct ‚Äî Reasoning + Acting for Agents","level":2},{"id":"technique-6-prompt-caching","text":"Technique 6: Prompt Caching","level":2},{"id":"evaluation-measuring-prompt-quality","text":"Evaluation: Measuring Prompt Quality","level":2}]},"relatedPosts":[{"title":"Fine-Tuning LLMs: When to Fine-Tune, When to Prompt","description":"Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.","date":"2025-03-27","category":"AI/ML","tags":["ai","llm","fine-tuning","lora","hugging face","openai","machine learning"],"featured":false,"affiliateSection":"ai-ml-books","slug":"fine-tuning-llms","readingTime":"10 min read","excerpt":"Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning ‚Äî fo‚Ä¶"},{"title":"Building AI Agents with Tool Use: From Chatbot to Autonomous Agent","description":"Build production AI agents using Claude's tool use API. Learn the agentic loop, error handling, multi-step reasoning, human-in-the-loop patterns, and how to build reliable autonomous systems.","date":"2025-03-23","category":"AI/ML","tags":["ai","agents","claude","tool use","llm","autonomous systems","python"],"featured":false,"affiliateSection":"ai-ml-books","slug":"llm-agents-tool-use","readingTime":"10 min read","excerpt":"A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio‚Ä¶"},{"title":"Vector Embeddings: The Foundation of Modern AI Applications","description":"Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.","date":"2025-03-11","category":"AI/ML","tags":["ai","embeddings","vector database","semantic search","rag","pgvector","pinecone"],"featured":false,"affiliateSection":"ai-ml-books","slug":"vector-embeddings-deep-dive","readingTime":"11 min read","excerpt":"Every modern AI application ‚Äî semantic search, RAG, recommendations, duplicate detection ‚Äî is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantic‚Ä¶"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"prompt-engineering-production"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>