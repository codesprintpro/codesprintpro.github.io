<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">System Design: Real-Time Chat Application at Scale<!-- --> | CodeSprintPro</title><meta name="description" content="Design a real-time chat system like WhatsApp or Slack handling 1 billion messages per day. Covers WebSocket connection management, message delivery guarantees, presence detection, and storage." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/system-design-real-time-chat/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="System Design: Real-Time Chat Application at Scale" data-next-head=""/><meta property="og:description" content="Design a real-time chat system like WhatsApp or Slack handling 1 billion messages per day. Covers WebSocket connection management, message delivery guarantees, presence detection, and storage." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/system-design-real-time-chat/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-03-17" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="System Design" data-next-head=""/><meta property="article:tag" content="system design" data-next-head=""/><meta property="article:tag" content="websocket" data-next-head=""/><meta property="article:tag" content="real-time" data-next-head=""/><meta property="article:tag" content="kafka" data-next-head=""/><meta property="article:tag" content="redis" data-next-head=""/><meta property="article:tag" content="cassandra" data-next-head=""/><meta property="article:tag" content="distributed systems" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="System Design: Real-Time Chat Application at Scale" data-next-head=""/><meta name="twitter:description" content="Design a real-time chat system like WhatsApp or Slack handling 1 billion messages per day. Covers WebSocket connection management, message delivery guarantees, presence detection, and storage." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"System Design: Real-Time Chat Application at Scale","description":"Design a real-time chat system like WhatsApp or Slack handling 1 billion messages per day. Covers WebSocket connection management, message delivery guarantees, presence detection, and storage.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-03-17","dateModified":"2025-03-17","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/system-design-real-time-chat/"},"keywords":"system design, websocket, real-time, kafka, redis, cassandra, distributed systems","articleSection":"System Design"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">System Design: Real-Time Chat Application at Scale</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">System Design</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">System Design: Real-Time Chat Application at Scale</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Design a real-time chat system like WhatsApp or Slack handling 1 billion messages per day. Covers WebSocket connection management, message delivery guarantees, presence detection, and storage.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>Â·</span><span>March 17, 2025</span><span>Â·</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->system design</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->websocket</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->real-time</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->kafka</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->redis</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->cassandra</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->distributed systems</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Real-time chat systems are among the most architecturally interesting distributed systems. They require persistent connections at massive scale, exactly-once message delivery guarantees, presence detection across millions of users, and message history queries that span years. This article designs a system comparable to WhatsApp or Slack.</p>
<h2>Requirements</h2>
<p>Before designing anything, you need to quantify the problem precisely. Requirements drive every architectural decision that follows â€” the choice of database, the number of Kafka partitions, the size of the connection pool. Notice that the numbers below aren't arbitrary; each one is derived from real-world usage patterns and drives specific design choices.</p>
<p><strong>Functional:</strong></p>
<ul>
<li>1:1 messaging and group chats (up to 1000 members)</li>
<li>Real-time delivery (&#x3C; 100ms end-to-end for online users)</li>
<li>Message delivery receipts (sent â†’ delivered â†’ read)</li>
<li>Message history (searchable, up to 5 years)</li>
<li>User presence (online/offline, last seen)</li>
<li>Message types: text, images, files, reactions</li>
</ul>
<p><strong>Non-Functional:</strong></p>
<ul>
<li>500M daily active users, 1B messages/day (~11,600 msg/sec average, 3x peak = 35,000/sec)</li>
<li>Message delivery guarantee: at-least-once (deduplication client-side)</li>
<li>Message ordering: within a conversation, strict order</li>
<li>Storage: ~100 bytes/message Ã— 1B/day Ã— 365 days Ã— 5 years = 180TB</li>
</ul>
<h2>High-Level Architecture</h2>
<p>The architecture is driven by one fundamental constraint: with 500M daily active users and real-time delivery requirements, you cannot use traditional request-response HTTP. Each online user needs a persistent, low-latency connection. The diagram below shows how the system is organized around this constraint.</p>
<pre><code>Mobile/Web Client
    â”‚
    â”‚ WebSocket (persistent connection)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   WebSocket Gateway Cluster              â”‚
â”‚   (stateful: each server holds N open WebSocket conns)  â”‚
â”‚   ws-1: [user_A, user_B, user_C, ...]                    â”‚
â”‚   ws-2: [user_D, user_E, user_F, ...]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Publish messages to Kafka
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Kafka (message bus, ordered per partition)      â”‚
â”‚   Topic: chat-messages, partitioned by conversation_id   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼           â–¼               â–¼
   Message Service  Storage Service  Notification Service
   (delivery,       (Cassandra)      (APNs, FCM for
    routing)                          offline users)

Redis: User â†’ WebSocket server mapping (presence registry)
</code></pre>
<p>The WebSocket gateways are stateful â€” each server maintains thousands of open connections in memory. This statefulness creates the core routing challenge: when User A sends to User B, the system must find which server holds User B's connection. Redis serves as the global directory that maps user IDs to server IDs.</p>
<h2>WebSocket Connection Management</h2>
<p>The key challenge: when User A sends to User B, which WebSocket server holds User B's connection?</p>
<p>Your WebSocket gateway is like an airport's gate assignment system. Each gate (server) handles a set of flights (connections), and the central directory (Redis) tells everyone which gate a given flight is at. When a new connection lands, you register it. When it leaves, you deregister it. The code below handles the full connection lifecycle.</p>
<pre><code class="language-java">// WebSocket Gateway â€” each server handles 50,000-100,000 persistent connections
@Component
public class ChatWebSocketHandler extends TextWebSocketHandler {

    @Autowired
    private UserConnectionRegistry connectionRegistry;

    @Autowired
    private KafkaTemplate&#x3C;String, ChatMessage> kafkaTemplate;

    // In-process connection map: userId â†’ WebSocket session
    private final ConcurrentHashMap&#x3C;String, WebSocketSession> localConnections =
        new ConcurrentHashMap&#x3C;>();

    @Override
    public void afterConnectionEstablished(WebSocketSession session) {
        String userId = extractUserId(session);  // From JWT in handshake header

        localConnections.put(userId, session);

        // Register in Redis: userId â†’ this server's ID
        connectionRegistry.register(userId, getServerId());

        // Send buffered messages (messages received while user was offline)
        sendOfflineMessages(userId, session);
    }

    @Override
    protected void handleTextMessage(WebSocketSession session, TextMessage message) {
        String userId = extractUserId(session);
        ChatMessage chatMessage = parseMessage(message, userId);

        // Publish to Kafka (partitioned by conversationId for ordering)
        kafkaTemplate.send(
            "chat-messages",
            chatMessage.getConversationId(),  // partition key â€” ensures ordering
        chatMessage
        );

        // Ack to sender immediately
        sendAck(session, chatMessage.getClientMessageId());
    }

    @Override
    public void afterConnectionClosed(WebSocketSession session, CloseStatus status) {
        String userId = extractUserId(session);
        localConnections.remove(userId);
        connectionRegistry.unregister(userId);
        updateLastSeen(userId);
    }

    // Deliver message to a local user (called by Message Delivery Service)
    public boolean deliverLocally(String userId, ChatMessage message) {
        WebSocketSession session = localConnections.get(userId);
        if (session == null || !session.isOpen()) return false;

        try {
            session.sendMessage(new TextMessage(serialize(message)));
            return true;
        } catch (IOException e) {
            return false;
        }
    }
}
</code></pre>
<p>Notice that when a message arrives from a client, it's immediately published to Kafka rather than routed directly to the recipient. This decoupling is intentional â€” Kafka provides durability (the message is persisted even if the delivery service crashes), ordering (messages in the same conversation stay in order), and fan-out (multiple consumers like storage and notification services process the same event).</p>
<h2>Message Delivery Service</h2>
<p>With messages flowing through Kafka, the delivery service consumes them and routes each one to the right WebSocket server. This service handles the three cases that arise in any real deployment: the recipient is online on this server, on a different server, or offline entirely.</p>
<pre><code class="language-java">// Kafka consumer: route messages to the right WebSocket server
@Service
public class MessageDeliveryService {

    @Autowired
    private UserConnectionRegistry connectionRegistry;  // Redis

    @Autowired
    private Map&#x3C;String, ChatWebSocketHandler> wsHandlers;  // Local + remote stubs

    @Autowired
    private OfflineMessageStore offlineStore;

    @KafkaListener(
        topics = "chat-messages",
        concurrency = "12"  // One thread per Kafka partition
    )
    public void deliver(ChatMessage message) {
        List&#x3C;String> recipients = getRecipients(message);  // From conversation members

        for (String recipientId : recipients) {
            String serverIdForRecipient = connectionRegistry.getServer(recipientId);

            if (serverIdForRecipient == null) {
                // User is offline: store for later delivery + send push notification
                offlineStore.store(recipientId, message);
                pushNotificationService.send(recipientId, message);
            } else if (serverIdForRecipient.equals(getServerId())) {
                // User is on THIS server: deliver directly
                boolean delivered = localWsHandler.deliverLocally(recipientId, message);
                if (!delivered) {
                    // Session closed between check and delivery
                    offlineStore.store(recipientId, message);
                }
            } else {
                // User is on a DIFFERENT server: route via internal HTTP/gRPC
                deliverToServer(serverIdForRecipient, recipientId, message);
            }
        }

        // Store in Cassandra (message history)
        messageStore.save(message);

        // Update delivery status
        updateDeliveryStatus(message.getId(), recipients, DeliveryStatus.DELIVERED);
    }
}
</code></pre>
<p>The <code>concurrency = "12"</code> setting means 12 threads consume from 12 Kafka partitions in parallel â€” matching consumer threads to partitions is the correct way to maximize Kafka throughput. The fallback to offline storage when a session has closed between the Redis check and the delivery attempt is an important edge case â€” without it, messages would silently drop during the brief window of a connection closing.</p>
<h2>User Presence with Redis</h2>
<p>Presence detection is the feature users take for granted but which requires careful engineering. The approach below uses Redis TTL as the mechanism â€” a heartbeat keeps the key alive, and natural expiry signals the user has gone offline. This is simpler and more scalable than maintaining a connection registry with explicit disconnect events, which can be missed during network failures.</p>
<pre><code class="language-java">@Service
public class PresenceService {

    @Autowired
    private StringRedisTemplate redis;

    private static final String ONLINE_KEY_PREFIX = "presence:online:";
    private static final Duration ONLINE_TTL = Duration.ofSeconds(30);

    // Called by WebSocket heartbeat every 20 seconds
    public void heartbeat(String userId) {
        redis.opsForValue().set(
            ONLINE_KEY_PREFIX + userId,
            Instant.now().toString(),
            ONLINE_TTL
        );
    }

    public boolean isOnline(String userId) {
        return redis.hasKey(ONLINE_KEY_PREFIX + userId);
    }

    // Batch check: are these 100 users online?
    public Map&#x3C;String, Boolean> getBulkPresence(List&#x3C;String> userIds) {
        List&#x3C;String> keys = userIds.stream()
            .map(id -> ONLINE_KEY_PREFIX + id).toList();

        List&#x3C;String> values = redis.opsForValue().multiGet(keys);

        Map&#x3C;String, Boolean> result = new HashMap&#x3C;>();
        for (int i = 0; i &#x3C; userIds.size(); i++) {
            result.put(userIds.get(i), values.get(i) != null);
        }
        return result;
    }

    // Last seen: stored in Redis when user goes offline
    public void markOffline(String userId) {
        redis.delete(ONLINE_KEY_PREFIX + userId);
        redis.opsForValue().set(
            "presence:lastseen:" + userId,
            Instant.now().toString()
        );
    }
}
</code></pre>
<p>The <code>getBulkPresence</code> method using <code>multiGet</code> is a critical optimization for group chats â€” instead of making 100 Redis round trips to check 100 members' presence, you make one. In a 1000-member group chat, the difference between single and batch presence checks is the difference between acceptable and unusable latency.</p>
<h2>Message Storage: Cassandra</h2>
<p>Messages require high write throughput and time-ordered reads per conversation.</p>
<p>Relational databases aren't a good fit for chat message storage at this scale. The access pattern is extremely predictable â€” you always fetch the latest N messages for a specific conversation â€” and you need write throughput far beyond what a single PostgreSQL instance can handle. Cassandra's data model is designed exactly for this: partition by a natural key (conversation ID + time bucket) and cluster by time within each partition.</p>
<pre><code class="language-sql">-- Cassandra schema (optimized for "get last 50 messages in conversation X")
CREATE TABLE messages (
    conversation_id  UUID,
    bucket           INT,          -- Time bucket (year-month): limits partition size
    message_id       TIMEUUID,     -- TimeUUID: unique + ordered by time
    sender_id        UUID,
    content          TEXT,
    message_type     TEXT,         -- text, image, file, reaction
    metadata         MAP&#x3C;TEXT, TEXT>,
    deleted_at       TIMESTAMP,    -- Soft delete

    PRIMARY KEY ((conversation_id, bucket), message_id)
) WITH CLUSTERING ORDER BY (message_id DESC)
  AND compaction = {'class': 'TimeWindowCompactionStrategy',
                    'compaction_window_unit': 'DAYS',
                    'compaction_window_size': 7};

-- Query: latest 50 messages in conversation
SELECT * FROM messages
WHERE conversation_id = ? AND bucket = ?
ORDER BY message_id DESC
LIMIT 50;

-- Bucket calculation: ensures no partition grows unbounded
-- bucket = year * 100 + month (e.g., 202502 for Feb 2025)
-- Active conversations: query current bucket + previous if needed
</code></pre>
<p>The <code>bucket</code> column is the key design insight here â€” without it, a very active conversation could accumulate millions of rows in a single Cassandra partition, which degrades read and compaction performance. By bucketing per month, you cap each partition at roughly one month's worth of messages per conversation. <code>TimeWindowCompactionStrategy</code> then efficiently compacts data within time windows, which matches the write pattern perfectly.</p>
<h2>Message Deduplication</h2>
<p>At-least-once delivery means duplicates are possible. Handle client-side:</p>
<p>Because the system guarantees at-least-once delivery (not exactly-once), the same message can arrive at a client more than once â€” for example, during a network reconnect where the client re-requests messages from the last received position. The deduplication service below prevents duplicates from appearing in the UI by tracking which client-generated IDs have already been processed.</p>
<pre><code class="language-java">// Each message has a client-generated idempotency key
// Client generates: clientMessageId = UUID.randomUUID()
// Server stores: (conversationId, clientMessageId) â†’ messageId

@Service
public class MessageDeduplicationService {

    @Autowired
    private RedisTemplate&#x3C;String, String> redis;

    private static final Duration DEDUP_TTL = Duration.ofHours(24);

    public Optional&#x3C;String> isDuplicate(String conversationId, String clientMessageId) {
        String key = "dedup:" + conversationId + ":" + clientMessageId;
        String existingMessageId = redis.opsForValue().get(key);
        return Optional.ofNullable(existingMessageId);
    }

    public void markProcessed(String conversationId, String clientMessageId, String messageId) {
        String key = "dedup:" + conversationId + ":" + clientMessageId;
        redis.opsForValue().set(key, messageId, DEDUP_TTL);
    }
}
</code></pre>
<p>The 24-hour TTL is a deliberate trade-off â€” it covers the window in which a duplicate is likely to arrive (network retries, reconnects), while preventing unlimited Redis growth. Messages older than 24 hours will simply be re-stored if re-delivered, which is acceptable because the client can deduplicate by <code>message_id</code> in its local database.</p>
<h2>Scalability Analysis</h2>
<p>With the components understood, here's how each one scales to meet the requirements. These numbers give you a concrete target for capacity planning.</p>
<pre><code>Component         Scale           Technology
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WebSocket GW      50K conn/server  Netty/Spring WebFlux
                  â†’ 10K servers for 500M users
                  â†’ Use consistent hashing to route users

Kafka             35K msg/sec peak  30 partitions (1K/partition)
                  Retention: 24h for real-time delivery

Cassandra         35K writes/sec    10 nodes, RF=3
                  100TB/year        Use TTL for 5-year retention

Redis (presence)  500M keys         Redis Cluster, 6 shards
                  30s TTL â†’ natural expiry

Message routing   gRPC between GW   P2P mesh for intra-cluster
                  servers           pub/sub for cross-cluster

CDN               Images/files      S3 + CloudFront
                  Pre-signed URLs   Direct upload from client
</code></pre>
<h2>Guarantees and Trade-offs</h2>
<p>Every distributed system design involves deliberate trade-offs. The choices below reflect the reality that chat applications prioritize availability â€” users should always be able to send messages, even under network partitions â€” over strict consistency guarantees that would require coordination overhead.</p>
<pre><code>Message ordering:
  Within a conversation: guaranteed (Kafka partitioned by conversationId,
                                     Cassandra TIMEUUID ordering)
  Across conversations: best-effort

Delivery guarantee:
  Online users: at-least-once (deduplicated client-side)
  Offline users: at-least-once (stored + retried)
  No guarantee of exactly-once end-to-end (intentional trade-off for performance)

Consistency:
  Message storage: eventual (Cassandra RF=3, quorum reads)
  Delivery status: eventual (Redis, replicated)
  Group membership: strong (PostgreSQL for group metadata)

CAP theorem position: AP (availability + partition tolerance)
  During network partition: accept writes (store in Kafka),
  deliver when partition heals â†’ may deliver out of order
  Trade-off: always available vs always consistent
</code></pre>
<p>The hardest part of chat system design isn't the message passing â€” it's the edge cases. What happens when a user is on 3 devices? (deliver to all) What if a conversation has 1000 members and all are online? (fan-out at scale requires message fan-out service) What if a server crashes mid-delivery? (Kafka offset management + at-least-once). Design the happy path first, then systematically find failure modes.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">ğŸ“š</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">System Design Interview â€” Alex Xu</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Best Seller</span></div><p class="text-xs text-gray-600">Step-by-step guide to ace system design interviews with real-world examples.</p></div><a href="https://amzn.to/3TqsPRp" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> â†’</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Grokking System Design on Educative</span></div><p class="text-xs text-gray-600">Interactive course teaching system design with visual diagrams and practice problems.</p></div><a href="https://www.educative.io/courses/grokking-the-system-design-interview" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> â†’</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Designing Data-Intensive Applications</span></div><p class="text-xs text-gray-600">Martin Kleppmann&#x27;s book is essential reading for any system design role.</p></div><a href="https://amzn.to/3RyKzOA" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> â†’</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=System%20Design%3A%20Real-Time%20Chat%20Application%20at%20Scale&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fsystem-design-real-time-chat%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fsystem-design-real-time-chat%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#requirements" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Requirements</a></li><li class=""><a href="#high-level-architecture" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">High-Level Architecture</a></li><li class=""><a href="#websocket-connection-management" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">WebSocket Connection Management</a></li><li class=""><a href="#message-delivery-service" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Message Delivery Service</a></li><li class=""><a href="#user-presence-with-redis" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">User Presence with Redis</a></li><li class=""><a href="#message-storage-cassandra" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Message Storage: Cassandra</a></li><li class=""><a href="#message-deduplication" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Message Deduplication</a></li><li class=""><a href="#scalability-analysis" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Scalability Analysis</a></li><li class=""><a href="#guarantees-and-trade-offs" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Guarantees and Trade-offs</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/observability-opentelemetry-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Building Production Observability with OpenTelemetry and Grafana Stack</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why â€” by exploring system state through metrics, traces, and logs without needing to know in advanceâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jul 3, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>6 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->observability</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->opentelemetry</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->prometheus</span></div></article></a><a href="/blog/event-sourcing-cqrs-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Event Sourcing and CQRS in Production: Beyond the Theory</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory â€” store events instead of state, derive state by replaying events â€” is souâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 23, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->event sourcing</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->cqrs</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->system design</span></div></article></a><a href="/blog/grpc-vs-rest-vs-graphql/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">gRPC vs REST vs GraphQL: Choosing the Right API Protocol</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to tâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 18, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->grpc</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->rest</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->graphql</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">â† Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS â€” by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">Â© <!-- -->2026<!-- --> CodeSprintPro Â· Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js Â· TailwindCSS Â· Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"System Design: Real-Time Chat Application at Scale","description":"Design a real-time chat system like WhatsApp or Slack handling 1 billion messages per day. Covers WebSocket connection management, message delivery guarantees, presence detection, and storage.","date":"2025-03-17","category":"System Design","tags":["system design","websocket","real-time","kafka","redis","cassandra","distributed systems"],"featured":false,"affiliateSection":"system-design-courses","slug":"system-design-real-time-chat","readingTime":"11 min read","excerpt":"Real-time chat systems are among the most architecturally interesting distributed systems. They require persistent connections at massive scale, exactly-once message delivery guarantees, presence detection across millionâ€¦","contentHtml":"\u003cp\u003eReal-time chat systems are among the most architecturally interesting distributed systems. They require persistent connections at massive scale, exactly-once message delivery guarantees, presence detection across millions of users, and message history queries that span years. This article designs a system comparable to WhatsApp or Slack.\u003c/p\u003e\n\u003ch2\u003eRequirements\u003c/h2\u003e\n\u003cp\u003eBefore designing anything, you need to quantify the problem precisely. Requirements drive every architectural decision that follows â€” the choice of database, the number of Kafka partitions, the size of the connection pool. Notice that the numbers below aren't arbitrary; each one is derived from real-world usage patterns and drives specific design choices.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFunctional:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1:1 messaging and group chats (up to 1000 members)\u003c/li\u003e\n\u003cli\u003eReal-time delivery (\u0026#x3C; 100ms end-to-end for online users)\u003c/li\u003e\n\u003cli\u003eMessage delivery receipts (sent â†’ delivered â†’ read)\u003c/li\u003e\n\u003cli\u003eMessage history (searchable, up to 5 years)\u003c/li\u003e\n\u003cli\u003eUser presence (online/offline, last seen)\u003c/li\u003e\n\u003cli\u003eMessage types: text, images, files, reactions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNon-Functional:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e500M daily active users, 1B messages/day (~11,600 msg/sec average, 3x peak = 35,000/sec)\u003c/li\u003e\n\u003cli\u003eMessage delivery guarantee: at-least-once (deduplication client-side)\u003c/li\u003e\n\u003cli\u003eMessage ordering: within a conversation, strict order\u003c/li\u003e\n\u003cli\u003eStorage: ~100 bytes/message Ã— 1B/day Ã— 365 days Ã— 5 years = 180TB\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eHigh-Level Architecture\u003c/h2\u003e\n\u003cp\u003eThe architecture is driven by one fundamental constraint: with 500M daily active users and real-time delivery requirements, you cannot use traditional request-response HTTP. Each online user needs a persistent, low-latency connection. The diagram below shows how the system is organized around this constraint.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eMobile/Web Client\n    â”‚\n    â”‚ WebSocket (persistent connection)\n    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                   WebSocket Gateway Cluster              â”‚\nâ”‚   (stateful: each server holds N open WebSocket conns)  â”‚\nâ”‚   ws-1: [user_A, user_B, user_C, ...]                    â”‚\nâ”‚   ws-2: [user_D, user_E, user_F, ...]                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                     â”‚ Publish messages to Kafka\n                     â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚          Kafka (message bus, ordered per partition)      â”‚\nâ”‚   Topic: chat-messages, partitioned by conversation_id   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                     â”‚\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â–¼           â–¼               â–¼\n   Message Service  Storage Service  Notification Service\n   (delivery,       (Cassandra)      (APNs, FCM for\n    routing)                          offline users)\n\nRedis: User â†’ WebSocket server mapping (presence registry)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe WebSocket gateways are stateful â€” each server maintains thousands of open connections in memory. This statefulness creates the core routing challenge: when User A sends to User B, the system must find which server holds User B's connection. Redis serves as the global directory that maps user IDs to server IDs.\u003c/p\u003e\n\u003ch2\u003eWebSocket Connection Management\u003c/h2\u003e\n\u003cp\u003eThe key challenge: when User A sends to User B, which WebSocket server holds User B's connection?\u003c/p\u003e\n\u003cp\u003eYour WebSocket gateway is like an airport's gate assignment system. Each gate (server) handles a set of flights (connections), and the central directory (Redis) tells everyone which gate a given flight is at. When a new connection lands, you register it. When it leaves, you deregister it. The code below handles the full connection lifecycle.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// WebSocket Gateway â€” each server handles 50,000-100,000 persistent connections\n@Component\npublic class ChatWebSocketHandler extends TextWebSocketHandler {\n\n    @Autowired\n    private UserConnectionRegistry connectionRegistry;\n\n    @Autowired\n    private KafkaTemplate\u0026#x3C;String, ChatMessage\u003e kafkaTemplate;\n\n    // In-process connection map: userId â†’ WebSocket session\n    private final ConcurrentHashMap\u0026#x3C;String, WebSocketSession\u003e localConnections =\n        new ConcurrentHashMap\u0026#x3C;\u003e();\n\n    @Override\n    public void afterConnectionEstablished(WebSocketSession session) {\n        String userId = extractUserId(session);  // From JWT in handshake header\n\n        localConnections.put(userId, session);\n\n        // Register in Redis: userId â†’ this server's ID\n        connectionRegistry.register(userId, getServerId());\n\n        // Send buffered messages (messages received while user was offline)\n        sendOfflineMessages(userId, session);\n    }\n\n    @Override\n    protected void handleTextMessage(WebSocketSession session, TextMessage message) {\n        String userId = extractUserId(session);\n        ChatMessage chatMessage = parseMessage(message, userId);\n\n        // Publish to Kafka (partitioned by conversationId for ordering)\n        kafkaTemplate.send(\n            \"chat-messages\",\n            chatMessage.getConversationId(),  // partition key â€” ensures ordering\n        chatMessage\n        );\n\n        // Ack to sender immediately\n        sendAck(session, chatMessage.getClientMessageId());\n    }\n\n    @Override\n    public void afterConnectionClosed(WebSocketSession session, CloseStatus status) {\n        String userId = extractUserId(session);\n        localConnections.remove(userId);\n        connectionRegistry.unregister(userId);\n        updateLastSeen(userId);\n    }\n\n    // Deliver message to a local user (called by Message Delivery Service)\n    public boolean deliverLocally(String userId, ChatMessage message) {\n        WebSocketSession session = localConnections.get(userId);\n        if (session == null || !session.isOpen()) return false;\n\n        try {\n            session.sendMessage(new TextMessage(serialize(message)));\n            return true;\n        } catch (IOException e) {\n            return false;\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNotice that when a message arrives from a client, it's immediately published to Kafka rather than routed directly to the recipient. This decoupling is intentional â€” Kafka provides durability (the message is persisted even if the delivery service crashes), ordering (messages in the same conversation stay in order), and fan-out (multiple consumers like storage and notification services process the same event).\u003c/p\u003e\n\u003ch2\u003eMessage Delivery Service\u003c/h2\u003e\n\u003cp\u003eWith messages flowing through Kafka, the delivery service consumes them and routes each one to the right WebSocket server. This service handles the three cases that arise in any real deployment: the recipient is online on this server, on a different server, or offline entirely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Kafka consumer: route messages to the right WebSocket server\n@Service\npublic class MessageDeliveryService {\n\n    @Autowired\n    private UserConnectionRegistry connectionRegistry;  // Redis\n\n    @Autowired\n    private Map\u0026#x3C;String, ChatWebSocketHandler\u003e wsHandlers;  // Local + remote stubs\n\n    @Autowired\n    private OfflineMessageStore offlineStore;\n\n    @KafkaListener(\n        topics = \"chat-messages\",\n        concurrency = \"12\"  // One thread per Kafka partition\n    )\n    public void deliver(ChatMessage message) {\n        List\u0026#x3C;String\u003e recipients = getRecipients(message);  // From conversation members\n\n        for (String recipientId : recipients) {\n            String serverIdForRecipient = connectionRegistry.getServer(recipientId);\n\n            if (serverIdForRecipient == null) {\n                // User is offline: store for later delivery + send push notification\n                offlineStore.store(recipientId, message);\n                pushNotificationService.send(recipientId, message);\n            } else if (serverIdForRecipient.equals(getServerId())) {\n                // User is on THIS server: deliver directly\n                boolean delivered = localWsHandler.deliverLocally(recipientId, message);\n                if (!delivered) {\n                    // Session closed between check and delivery\n                    offlineStore.store(recipientId, message);\n                }\n            } else {\n                // User is on a DIFFERENT server: route via internal HTTP/gRPC\n                deliverToServer(serverIdForRecipient, recipientId, message);\n            }\n        }\n\n        // Store in Cassandra (message history)\n        messageStore.save(message);\n\n        // Update delivery status\n        updateDeliveryStatus(message.getId(), recipients, DeliveryStatus.DELIVERED);\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003econcurrency = \"12\"\u003c/code\u003e setting means 12 threads consume from 12 Kafka partitions in parallel â€” matching consumer threads to partitions is the correct way to maximize Kafka throughput. The fallback to offline storage when a session has closed between the Redis check and the delivery attempt is an important edge case â€” without it, messages would silently drop during the brief window of a connection closing.\u003c/p\u003e\n\u003ch2\u003eUser Presence with Redis\u003c/h2\u003e\n\u003cp\u003ePresence detection is the feature users take for granted but which requires careful engineering. The approach below uses Redis TTL as the mechanism â€” a heartbeat keeps the key alive, and natural expiry signals the user has gone offline. This is simpler and more scalable than maintaining a connection registry with explicit disconnect events, which can be missed during network failures.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Service\npublic class PresenceService {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    private static final String ONLINE_KEY_PREFIX = \"presence:online:\";\n    private static final Duration ONLINE_TTL = Duration.ofSeconds(30);\n\n    // Called by WebSocket heartbeat every 20 seconds\n    public void heartbeat(String userId) {\n        redis.opsForValue().set(\n            ONLINE_KEY_PREFIX + userId,\n            Instant.now().toString(),\n            ONLINE_TTL\n        );\n    }\n\n    public boolean isOnline(String userId) {\n        return redis.hasKey(ONLINE_KEY_PREFIX + userId);\n    }\n\n    // Batch check: are these 100 users online?\n    public Map\u0026#x3C;String, Boolean\u003e getBulkPresence(List\u0026#x3C;String\u003e userIds) {\n        List\u0026#x3C;String\u003e keys = userIds.stream()\n            .map(id -\u003e ONLINE_KEY_PREFIX + id).toList();\n\n        List\u0026#x3C;String\u003e values = redis.opsForValue().multiGet(keys);\n\n        Map\u0026#x3C;String, Boolean\u003e result = new HashMap\u0026#x3C;\u003e();\n        for (int i = 0; i \u0026#x3C; userIds.size(); i++) {\n            result.put(userIds.get(i), values.get(i) != null);\n        }\n        return result;\n    }\n\n    // Last seen: stored in Redis when user goes offline\n    public void markOffline(String userId) {\n        redis.delete(ONLINE_KEY_PREFIX + userId);\n        redis.opsForValue().set(\n            \"presence:lastseen:\" + userId,\n            Instant.now().toString()\n        );\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003egetBulkPresence\u003c/code\u003e method using \u003ccode\u003emultiGet\u003c/code\u003e is a critical optimization for group chats â€” instead of making 100 Redis round trips to check 100 members' presence, you make one. In a 1000-member group chat, the difference between single and batch presence checks is the difference between acceptable and unusable latency.\u003c/p\u003e\n\u003ch2\u003eMessage Storage: Cassandra\u003c/h2\u003e\n\u003cp\u003eMessages require high write throughput and time-ordered reads per conversation.\u003c/p\u003e\n\u003cp\u003eRelational databases aren't a good fit for chat message storage at this scale. The access pattern is extremely predictable â€” you always fetch the latest N messages for a specific conversation â€” and you need write throughput far beyond what a single PostgreSQL instance can handle. Cassandra's data model is designed exactly for this: partition by a natural key (conversation ID + time bucket) and cluster by time within each partition.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003e-- Cassandra schema (optimized for \"get last 50 messages in conversation X\")\nCREATE TABLE messages (\n    conversation_id  UUID,\n    bucket           INT,          -- Time bucket (year-month): limits partition size\n    message_id       TIMEUUID,     -- TimeUUID: unique + ordered by time\n    sender_id        UUID,\n    content          TEXT,\n    message_type     TEXT,         -- text, image, file, reaction\n    metadata         MAP\u0026#x3C;TEXT, TEXT\u003e,\n    deleted_at       TIMESTAMP,    -- Soft delete\n\n    PRIMARY KEY ((conversation_id, bucket), message_id)\n) WITH CLUSTERING ORDER BY (message_id DESC)\n  AND compaction = {'class': 'TimeWindowCompactionStrategy',\n                    'compaction_window_unit': 'DAYS',\n                    'compaction_window_size': 7};\n\n-- Query: latest 50 messages in conversation\nSELECT * FROM messages\nWHERE conversation_id = ? AND bucket = ?\nORDER BY message_id DESC\nLIMIT 50;\n\n-- Bucket calculation: ensures no partition grows unbounded\n-- bucket = year * 100 + month (e.g., 202502 for Feb 2025)\n-- Active conversations: query current bucket + previous if needed\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003ebucket\u003c/code\u003e column is the key design insight here â€” without it, a very active conversation could accumulate millions of rows in a single Cassandra partition, which degrades read and compaction performance. By bucketing per month, you cap each partition at roughly one month's worth of messages per conversation. \u003ccode\u003eTimeWindowCompactionStrategy\u003c/code\u003e then efficiently compacts data within time windows, which matches the write pattern perfectly.\u003c/p\u003e\n\u003ch2\u003eMessage Deduplication\u003c/h2\u003e\n\u003cp\u003eAt-least-once delivery means duplicates are possible. Handle client-side:\u003c/p\u003e\n\u003cp\u003eBecause the system guarantees at-least-once delivery (not exactly-once), the same message can arrive at a client more than once â€” for example, during a network reconnect where the client re-requests messages from the last received position. The deduplication service below prevents duplicates from appearing in the UI by tracking which client-generated IDs have already been processed.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Each message has a client-generated idempotency key\n// Client generates: clientMessageId = UUID.randomUUID()\n// Server stores: (conversationId, clientMessageId) â†’ messageId\n\n@Service\npublic class MessageDeduplicationService {\n\n    @Autowired\n    private RedisTemplate\u0026#x3C;String, String\u003e redis;\n\n    private static final Duration DEDUP_TTL = Duration.ofHours(24);\n\n    public Optional\u0026#x3C;String\u003e isDuplicate(String conversationId, String clientMessageId) {\n        String key = \"dedup:\" + conversationId + \":\" + clientMessageId;\n        String existingMessageId = redis.opsForValue().get(key);\n        return Optional.ofNullable(existingMessageId);\n    }\n\n    public void markProcessed(String conversationId, String clientMessageId, String messageId) {\n        String key = \"dedup:\" + conversationId + \":\" + clientMessageId;\n        redis.opsForValue().set(key, messageId, DEDUP_TTL);\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe 24-hour TTL is a deliberate trade-off â€” it covers the window in which a duplicate is likely to arrive (network retries, reconnects), while preventing unlimited Redis growth. Messages older than 24 hours will simply be re-stored if re-delivered, which is acceptable because the client can deduplicate by \u003ccode\u003emessage_id\u003c/code\u003e in its local database.\u003c/p\u003e\n\u003ch2\u003eScalability Analysis\u003c/h2\u003e\n\u003cp\u003eWith the components understood, here's how each one scales to meet the requirements. These numbers give you a concrete target for capacity planning.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eComponent         Scale           Technology\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nWebSocket GW      50K conn/server  Netty/Spring WebFlux\n                  â†’ 10K servers for 500M users\n                  â†’ Use consistent hashing to route users\n\nKafka             35K msg/sec peak  30 partitions (1K/partition)\n                  Retention: 24h for real-time delivery\n\nCassandra         35K writes/sec    10 nodes, RF=3\n                  100TB/year        Use TTL for 5-year retention\n\nRedis (presence)  500M keys         Redis Cluster, 6 shards\n                  30s TTL â†’ natural expiry\n\nMessage routing   gRPC between GW   P2P mesh for intra-cluster\n                  servers           pub/sub for cross-cluster\n\nCDN               Images/files      S3 + CloudFront\n                  Pre-signed URLs   Direct upload from client\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eGuarantees and Trade-offs\u003c/h2\u003e\n\u003cp\u003eEvery distributed system design involves deliberate trade-offs. The choices below reflect the reality that chat applications prioritize availability â€” users should always be able to send messages, even under network partitions â€” over strict consistency guarantees that would require coordination overhead.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eMessage ordering:\n  Within a conversation: guaranteed (Kafka partitioned by conversationId,\n                                     Cassandra TIMEUUID ordering)\n  Across conversations: best-effort\n\nDelivery guarantee:\n  Online users: at-least-once (deduplicated client-side)\n  Offline users: at-least-once (stored + retried)\n  No guarantee of exactly-once end-to-end (intentional trade-off for performance)\n\nConsistency:\n  Message storage: eventual (Cassandra RF=3, quorum reads)\n  Delivery status: eventual (Redis, replicated)\n  Group membership: strong (PostgreSQL for group metadata)\n\nCAP theorem position: AP (availability + partition tolerance)\n  During network partition: accept writes (store in Kafka),\n  deliver when partition heals â†’ may deliver out of order\n  Trade-off: always available vs always consistent\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe hardest part of chat system design isn't the message passing â€” it's the edge cases. What happens when a user is on 3 devices? (deliver to all) What if a conversation has 1000 members and all are online? (fan-out at scale requires message fan-out service) What if a server crashes mid-delivery? (Kafka offset management + at-least-once). Design the happy path first, then systematically find failure modes.\u003c/p\u003e\n","tableOfContents":[{"id":"requirements","text":"Requirements","level":2},{"id":"high-level-architecture","text":"High-Level Architecture","level":2},{"id":"websocket-connection-management","text":"WebSocket Connection Management","level":2},{"id":"message-delivery-service","text":"Message Delivery Service","level":2},{"id":"user-presence-with-redis","text":"User Presence with Redis","level":2},{"id":"message-storage-cassandra","text":"Message Storage: Cassandra","level":2},{"id":"message-deduplication","text":"Message Deduplication","level":2},{"id":"scalability-analysis","text":"Scalability Analysis","level":2},{"id":"guarantees-and-trade-offs","text":"Guarantees and Trade-offs","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why â€” by exploring system state through metrics, traces, and logs without needing to know in advanceâ€¦"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory â€” store events instead of state, derive state by replaying events â€” is souâ€¦"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to tâ€¦"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"system-design-real-time-chat"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>