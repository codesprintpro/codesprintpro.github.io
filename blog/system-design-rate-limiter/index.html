<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">System Design: Distributed Rate Limiter ‚Äî Token Bucket vs Sliding Window<!-- --> | CodeSprintPro</title><meta name="description" content="Design a rate limiter that handles millions of requests across distributed servers. Compare token bucket, leaky bucket, fixed window, and sliding window algorithms with Redis-backed implementations." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/system-design-rate-limiter/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="System Design: Distributed Rate Limiter ‚Äî Token Bucket vs Sliding Window" data-next-head=""/><meta property="og:description" content="Design a rate limiter that handles millions of requests across distributed servers. Compare token bucket, leaky bucket, fixed window, and sliding window algorithms with Redis-backed implementations." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/system-design-rate-limiter/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-01-08" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="System Design" data-next-head=""/><meta property="article:tag" content="system design" data-next-head=""/><meta property="article:tag" content="rate limiting" data-next-head=""/><meta property="article:tag" content="redis" data-next-head=""/><meta property="article:tag" content="distributed systems" data-next-head=""/><meta property="article:tag" content="api" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="System Design: Distributed Rate Limiter ‚Äî Token Bucket vs Sliding Window" data-next-head=""/><meta name="twitter:description" content="Design a rate limiter that handles millions of requests across distributed servers. Compare token bucket, leaky bucket, fixed window, and sliding window algorithms with Redis-backed implementations." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"System Design: Distributed Rate Limiter ‚Äî Token Bucket vs Sliding Window","description":"Design a rate limiter that handles millions of requests across distributed servers. Compare token bucket, leaky bucket, fixed window, and sliding window algorithms with Redis-backed implementations.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-01-08","dateModified":"2025-01-08","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/system-design-rate-limiter/"},"keywords":"system design, rate limiting, redis, distributed systems, api","articleSection":"System Design"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">System Design: Distributed Rate Limiter ‚Äî Token Bucket vs Sliding Window</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">System Design</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">System Design: Distributed Rate Limiter ‚Äî Token Bucket vs Sliding Window</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Design a rate limiter that handles millions of requests across distributed servers. Compare token bucket, leaky bucket, fixed window, and sliding window algorithms with Redis-backed implementations.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>¬∑</span><span>January 8, 2025</span><span>¬∑</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->system design</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->rate limiting</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->redis</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->distributed systems</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->api</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Rate limiting is deceptively simple in concept and surprisingly tricky in distributed systems. Every API at scale ‚Äî Stripe, GitHub, Twitter ‚Äî implements rate limiting. Done wrong, it allows bursts that overwhelm backends. Done right, it's invisible to legitimate users and impenetrable to abusers.</p>
<p>This article covers every algorithm, compares their tradeoffs, and shows production-ready Redis implementations.</p>
<h2>Why Rate Limiting?</h2>
<ul>
<li><strong>DoS protection</strong>: Prevent any single client from consuming all resources</li>
<li><strong>Cost control</strong>: Metered APIs bill per request ‚Äî enforce usage quotas</li>
<li><strong>Fairness</strong>: Ensure no tenant starves others in a shared system</li>
<li><strong>Backend protection</strong>: Databases and downstream services have capacity limits</li>
</ul>
<h2>Algorithm Comparison</h2>
<p>Choosing the right algorithm is not about technical preference ‚Äî it is about which failure mode you can tolerate. Each algorithm below trades memory, accuracy, and burst behavior in different ways. Understanding the tradeoffs lets you pick the right one for your specific workload.</p>
<h3>Fixed Window Counter</h3>
<p>Divide time into fixed windows (e.g., 1-minute buckets). Count requests per window. This approach is appealing because it requires only a single integer counter per client per window, but it has a dangerous edge case that has caused real production incidents.</p>
<pre><code>Window: 12:00:00 - 12:01:00
  Client A: 95 requests ‚úì
  Client A: 96th request at 12:00:59 ‚úó (over limit of 95)

Problem: Boundary burst
  Client A sends 95 requests at 12:00:55 ‚úì
  Client A sends 95 requests at 12:01:05 ‚úì
  = 190 requests in 10 seconds (2x the intended rate)
</code></pre>
<p><strong>Verdict</strong>: Simple, but the boundary burst is a real vulnerability.</p>
<h3>Sliding Window Log</h3>
<p>Record the exact timestamp of every request. Count requests within the last N seconds. This approach is conceptually the most correct ‚Äî there are no windows and no boundary artifacts ‚Äî but the memory cost grows linearly with request volume.</p>
<pre><code>Rate limit: 100 requests per minute

At 12:01:30:
  Log: [12:00:31, 12:00:45, ..., 12:01:28, 12:01:29, 12:01:30]
  Filter to last 60 seconds: count requests since 12:00:30
  If count &#x3C; 100: allow, else reject
</code></pre>
<p><strong>Verdict</strong>: Accurate, no boundary bursts. Memory-intensive (store all timestamps). Impractical for >10K RPS.</p>
<h3>Sliding Window Counter</h3>
<p>Approximate the sliding window using two fixed window counters and a weighted average. This is the sweet spot for most production systems: it eliminates the boundary burst problem of fixed windows while using only two integers per client instead of a full timestamp log.</p>
<pre><code>Rate limit: 100 requests/minute

At 12:01:45 (45 seconds into current window):
  Previous window (12:00 - 12:01): 80 requests
  Current window (12:01 - 12:02): 30 requests so far

  Weight of previous window = (60 - 45) / 60 = 25%
  Estimated requests in last 60s = 80 √ó 0.25 + 30 = 50
  Under limit ‚Üí allow
</code></pre>
<p><strong>Verdict</strong>: Memory-efficient, approximation error is &#x3C;0.1% in practice. The best tradeoff for most systems.</p>
<h3>Token Bucket</h3>
<p>Tokens accumulate at a fixed rate (refill rate). Each request consumes one token. Burst is allowed up to the bucket capacity. Think of it like a prepaid phone plan: you accumulate credit over time and can spend it in bursts, but you can never spend more than you have.</p>
<pre><code>Bucket capacity: 10 tokens (max burst)
Refill rate: 1 token/second

Timeline:
  T=0: bucket=10 (full), 8 requests ‚Üí bucket=2
  T=3: bucket=5 (3 tokens added), 3 requests ‚Üí bucket=2
  T=5: bucket=4 (2 tokens added), 6 requests ‚Üí REJECT (only 4 available)

Properties:
  - Allows bursting up to bucket capacity
  - Average rate controlled by refill rate
  - Intuitive for "requests per second with burst allowance"
</code></pre>
<p><strong>Verdict</strong>: Best for APIs where controlled bursting is acceptable (e.g., initial page load).</p>
<h3>Leaky Bucket</h3>
<p>Requests enter a queue. A worker drains the queue at a fixed rate. Overflow requests are rejected. Unlike the token bucket, which smooths the average rate but allows bursts, the leaky bucket smooths the output rate ‚Äî it is useful when your downstream system needs a steady, predictable call rate rather than handling bursty traffic.</p>
<pre><code>Queue size: 10 requests
Drain rate: 5 requests/second

  Burst of 15 requests arrives:
  ‚Üí 10 queued, 5 rejected immediately
  ‚Üí Queue drains at 5/s ‚Üí smooth outgoing traffic
</code></pre>
<p><strong>Verdict</strong>: Smoothes traffic for backends that need steady input. Adds latency (queue wait). Use for outbound call shaping, not incoming API protection.</p>
<h2>Redis Implementation: Token Bucket with Lua</h2>
<p>The critical requirement for distributed rate limiting: <strong>atomicity</strong>. Read-modify-write must be atomic, or concurrent requests on different servers will both read "under limit" and both write "at limit+1" ‚Äî a race condition. Without atomicity, a client could send 10 simultaneous requests on 10 different app servers, and all 10 would pass the rate check before any of them had a chance to decrement the counter.</p>
<p>Redis Lua scripts run atomically on the Redis server, solving this problem without requiring distributed locks:</p>
<pre><code class="language-lua">-- token_bucket.lua
-- KEYS[1]: rate limit key (e.g., "rate:user:123")
-- ARGV[1]: max tokens (bucket capacity)
-- ARGV[2]: refill rate (tokens per second)
-- ARGV[3]: current timestamp (Unix seconds with milliseconds)
-- ARGV[4]: requested tokens (usually 1)

local key = KEYS[1]
local max_tokens = tonumber(ARGV[1])
local refill_rate = tonumber(ARGV[2])
local now = tonumber(ARGV[3])
local requested = tonumber(ARGV[4])

-- Get current state
local data = redis.call('HMGET', key, 'tokens', 'last_refill')
local tokens = tonumber(data[1]) or max_tokens
local last_refill = tonumber(data[2]) or now

-- Calculate tokens to add based on elapsed time
local elapsed = math.max(0, now - last_refill)
local tokens_to_add = elapsed * refill_rate
tokens = math.min(max_tokens, tokens + tokens_to_add)

-- Check if request can be served
local allowed = 0
if tokens >= requested then
    tokens = tokens - requested
    allowed = 1
end

-- Save state with TTL (auto-cleanup for idle clients)
redis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)
redis.call('EXPIRE', key, math.ceil(max_tokens / refill_rate) + 1)

return {allowed, math.floor(tokens), math.floor(tokens_to_add)}
</code></pre>
<p>The <code>EXPIRE</code> call at the end is a crucial operational detail: it automatically removes rate limit state for idle clients, preventing unbounded Redis memory growth. The TTL is calculated to be just long enough that a fully drained bucket would refill to capacity.</p>
<p>Now that you have the Lua logic, the Java service below wires it into a callable interface. The key design choice here is passing the current timestamp from the application rather than reading it inside Lua ‚Äî this keeps the script deterministic and easier to test.</p>
<pre><code class="language-java">@Service
public class TokenBucketRateLimiter {

    @Autowired
    private StringRedisTemplate redis;

    private final DefaultRedisScript&#x3C;List> luaScript;

    public TokenBucketRateLimiter() {
        this.luaScript = new DefaultRedisScript&#x3C;>();
        this.luaScript.setScriptText(LUA_SCRIPT); // Load from classpath
        this.luaScript.setResultType(List.class);
    }

    public RateLimitResult checkLimit(String clientId, RateLimitConfig config) {
        String key = "rate:" + clientId;
        double now = System.currentTimeMillis() / 1000.0;

        List&#x3C;Long> result = redis.execute(
            luaScript,
            List.of(key),
            String.valueOf(config.getMaxTokens()),
            String.valueOf(config.getRefillRate()),
            String.valueOf(now),
            "1"
        );

        boolean allowed = result.get(0) == 1L;
        long remainingTokens = result.get(1);

        return new RateLimitResult(allowed, remainingTokens, config.getMaxTokens());
    }
}
</code></pre>
<h2>Spring Boot Integration: Rate Limit Filter</h2>
<p>With the core limiter in place, you need to intercept every HTTP request before it reaches your business logic. A servlet filter runs before any controller code and can short-circuit the request with a <code>429</code> response without touching your application logic at all. This separation means you can add rate limiting to any endpoint without modifying it.</p>
<pre><code class="language-java">@Component
@Order(Ordered.HIGHEST_PRECEDENCE)
public class RateLimitFilter extends OncePerRequestFilter {

    @Autowired
    private TokenBucketRateLimiter rateLimiter;

    private static final Map&#x3C;String, RateLimitConfig> TIER_CONFIGS = Map.of(
        "free",       new RateLimitConfig(60, 1.0),   // 60 burst, 1 RPS sustained
        "pro",        new RateLimitConfig(600, 10.0),  // 600 burst, 10 RPS sustained
        "enterprise", new RateLimitConfig(6000, 100.0) // 6000 burst, 100 RPS
    );

    @Override
    protected void doFilterInternal(
            HttpServletRequest request,
            HttpServletResponse response,
            FilterChain chain) throws ServletException, IOException {

        String clientId = extractClientId(request); // From API key or JWT
        String tier = extractTier(clientId);
        RateLimitConfig config = TIER_CONFIGS.getOrDefault(tier, TIER_CONFIGS.get("free"));

        RateLimitResult result = rateLimiter.checkLimit(clientId, config);

        // Always set rate limit headers (RFC 6585)
        response.setHeader("X-RateLimit-Limit", String.valueOf(config.getMaxTokens()));
        response.setHeader("X-RateLimit-Remaining", String.valueOf(result.getRemainingTokens()));
        response.setHeader("X-RateLimit-Reset", String.valueOf(System.currentTimeMillis() / 1000 + 1));

        if (!result.isAllowed()) {
            response.setStatus(HttpStatus.TOO_MANY_REQUESTS.value());
            response.setHeader("Retry-After", "1");
            response.getWriter().write("{\"error\": \"Rate limit exceeded\", \"tier\": \"" + tier + "\"}");
            return;
        }

        chain.doFilter(request, response);
    }

    private String extractClientId(HttpServletRequest request) {
        String apiKey = request.getHeader("X-API-Key");
        if (apiKey != null) return "api:" + apiKey;

        // Fall back to IP-based limiting for unauthenticated requests
        return "ip:" + request.getRemoteAddr();
    }
}
</code></pre>
<p>Always setting the rate limit headers ‚Äî even for successful requests ‚Äî is important because clients use <code>X-RateLimit-Remaining</code> to implement self-throttling. A well-behaved SDK will slow down proactively when remaining tokens run low, reducing the number of rejected requests and improving the overall user experience.</p>
<h2>Distributed Challenges</h2>
<p>The single-server implementation above is correct, but distributed systems introduce new problems. The following two challenges are the ones interviewers most commonly expect you to address.</p>
<h3>Redis Cluster Consistency</h3>
<p>In a Redis cluster, rate limit keys can sit on different shards. Ensure the key lands on the same shard using hash tags:</p>
<pre><code class="language-java">// Without hash tag: different shards for different clients (fine)
String key = "rate:" + clientId;

// With hash tag: force all rate limit keys to same shard (avoid for large deployments)
String key = "{rate}:" + clientId;

// Better: use consistent hashing at the application level
// Partition clients by modulo and direct to specific Redis nodes
</code></pre>
<h3>Multi-Region Rate Limiting</h3>
<p>Strict global rate limiting requires cross-region coordination ‚Äî expensive. In practice, use <strong>local + global</strong> hybrid. Think of it like allocating traffic quotas across airport terminals: each terminal enforces its own cap, and the central authority adjusts allocations periodically rather than approving every single passenger.</p>
<pre><code>Approach: Divide global limit across regions proportionally

Global limit: 1000 requests/minute
  Region us-east-1: 500 req/min (50% of traffic)
  Region eu-west-1: 300 req/min (30%)
  Region ap-south-1: 200 req/min (20%)

Each region enforces its limit independently.
Sync global counters asynchronously every 10 seconds.
A client can exceed global limit by up to 10 seconds √ó regional rate ‚Äî acceptable for most use cases.
</code></pre>
<p>The 10-second sync window means a determined attacker can exceed the global limit by at most 10 seconds worth of regional traffic ‚Äî a bounded and acceptable overshoot for most business requirements.</p>
<h2>Where to Apply Rate Limiting</h2>
<p>Rate limiting is most effective when applied at multiple layers simultaneously, creating defense in depth. Each layer handles a different threat model, and together they prevent both accidental and intentional overload.</p>
<pre><code>Request path:
  Client ‚Üí CDN ‚Üí API Gateway ‚Üí Load Balancer ‚Üí Service ‚Üí Database

Rate limiting layers:
  CDN:         IP-based blocking for known abusers (bot traffic)
  API Gateway: Per-client limits (recommended primary enforcement point)
  Service:     Per-endpoint limits (e.g., expensive endpoints get stricter limits)
  Database:    Connection pool limits (implicit rate limiting)

Recommendation:
  - API Gateway for business rate limits (per API key, per tier)
  - Service layer for resource protection (expensive endpoints)
  - Both together for defense in depth
</code></pre>
<p>Rate limiting is a foundational API design concern. The token bucket algorithm with Redis Lua provides the best combination of correctness, performance, and operational simplicity. The boundary burst of fixed window algorithms has caused real production incidents ‚Äî don't ship that in critical systems.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">üìö</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">System Design Interview ‚Äî Alex Xu</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Best Seller</span></div><p class="text-xs text-gray-600">Step-by-step guide to ace system design interviews with real-world examples.</p></div><a href="https://amzn.to/3TqsPRp" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Grokking System Design on Educative</span></div><p class="text-xs text-gray-600">Interactive course teaching system design with visual diagrams and practice problems.</p></div><a href="https://www.educative.io/courses/grokking-the-system-design-interview" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Designing Data-Intensive Applications</span></div><p class="text-xs text-gray-600">Martin Kleppmann&#x27;s book is essential reading for any system design role.</p></div><a href="https://amzn.to/3RyKzOA" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=System%20Design%3A%20Distributed%20Rate%20Limiter%20%E2%80%94%20Token%20Bucket%20vs%20Sliding%20Window&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fsystem-design-rate-limiter%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fsystem-design-rate-limiter%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#why-rate-limiting" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Why Rate Limiting?</a></li><li class=""><a href="#algorithm-comparison" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Algorithm Comparison</a></li><li class="ml-4"><a href="#fixed-window-counter" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Fixed Window Counter</a></li><li class="ml-4"><a href="#sliding-window-log" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Sliding Window Log</a></li><li class="ml-4"><a href="#sliding-window-counter" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Sliding Window Counter</a></li><li class="ml-4"><a href="#token-bucket" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Token Bucket</a></li><li class="ml-4"><a href="#leaky-bucket" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Leaky Bucket</a></li><li class=""><a href="#redis-implementation-token-bucket-with-lua" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Redis Implementation: Token Bucket with Lua</a></li><li class=""><a href="#spring-boot-integration-rate-limit-filter" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Spring Boot Integration: Rate Limit Filter</a></li><li class=""><a href="#distributed-challenges" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Distributed Challenges</a></li><li class="ml-4"><a href="#redis-cluster-consistency" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Redis Cluster Consistency</a></li><li class="ml-4"><a href="#multi-region-rate-limiting" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Multi-Region Rate Limiting</a></li><li class=""><a href="#where-to-apply-rate-limiting" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Where to Apply Rate Limiting</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/observability-opentelemetry-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Building Production Observability with OpenTelemetry and Grafana Stack</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why ‚Äî by exploring system state through metrics, traces, and logs without needing to know in advance‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jul 3, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>6 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->observability</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->opentelemetry</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->prometheus</span></div></article></a><a href="/blog/event-sourcing-cqrs-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Event Sourcing and CQRS in Production: Beyond the Theory</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory ‚Äî store events instead of state, derive state by replaying events ‚Äî is sou‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 23, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->event sourcing</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->cqrs</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->system design</span></div></article></a><a href="/blog/grpc-vs-rest-vs-graphql/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">gRPC vs REST vs GraphQL: Choosing the Right API Protocol</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 18, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->grpc</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->rest</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->graphql</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">‚Üê Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS ‚Äî by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">¬© <!-- -->2026<!-- --> CodeSprintPro ¬∑ Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js ¬∑ TailwindCSS ¬∑ Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"System Design: Distributed Rate Limiter ‚Äî Token Bucket vs Sliding Window","description":"Design a rate limiter that handles millions of requests across distributed servers. Compare token bucket, leaky bucket, fixed window, and sliding window algorithms with Redis-backed implementations.","date":"2025-01-08","category":"System Design","tags":["system design","rate limiting","redis","distributed systems","api"],"featured":false,"affiliateSection":"system-design-courses","slug":"system-design-rate-limiter","readingTime":"10 min read","excerpt":"Rate limiting is deceptively simple in concept and surprisingly tricky in distributed systems. Every API at scale ‚Äî Stripe, GitHub, Twitter ‚Äî implements rate limiting. Done wrong, it allows bursts that overwhelm backends‚Ä¶","contentHtml":"\u003cp\u003eRate limiting is deceptively simple in concept and surprisingly tricky in distributed systems. Every API at scale ‚Äî Stripe, GitHub, Twitter ‚Äî implements rate limiting. Done wrong, it allows bursts that overwhelm backends. Done right, it's invisible to legitimate users and impenetrable to abusers.\u003c/p\u003e\n\u003cp\u003eThis article covers every algorithm, compares their tradeoffs, and shows production-ready Redis implementations.\u003c/p\u003e\n\u003ch2\u003eWhy Rate Limiting?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDoS protection\u003c/strong\u003e: Prevent any single client from consuming all resources\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCost control\u003c/strong\u003e: Metered APIs bill per request ‚Äî enforce usage quotas\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFairness\u003c/strong\u003e: Ensure no tenant starves others in a shared system\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBackend protection\u003c/strong\u003e: Databases and downstream services have capacity limits\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAlgorithm Comparison\u003c/h2\u003e\n\u003cp\u003eChoosing the right algorithm is not about technical preference ‚Äî it is about which failure mode you can tolerate. Each algorithm below trades memory, accuracy, and burst behavior in different ways. Understanding the tradeoffs lets you pick the right one for your specific workload.\u003c/p\u003e\n\u003ch3\u003eFixed Window Counter\u003c/h3\u003e\n\u003cp\u003eDivide time into fixed windows (e.g., 1-minute buckets). Count requests per window. This approach is appealing because it requires only a single integer counter per client per window, but it has a dangerous edge case that has caused real production incidents.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eWindow: 12:00:00 - 12:01:00\n  Client A: 95 requests ‚úì\n  Client A: 96th request at 12:00:59 ‚úó (over limit of 95)\n\nProblem: Boundary burst\n  Client A sends 95 requests at 12:00:55 ‚úì\n  Client A sends 95 requests at 12:01:05 ‚úì\n  = 190 requests in 10 seconds (2x the intended rate)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eVerdict\u003c/strong\u003e: Simple, but the boundary burst is a real vulnerability.\u003c/p\u003e\n\u003ch3\u003eSliding Window Log\u003c/h3\u003e\n\u003cp\u003eRecord the exact timestamp of every request. Count requests within the last N seconds. This approach is conceptually the most correct ‚Äî there are no windows and no boundary artifacts ‚Äî but the memory cost grows linearly with request volume.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRate limit: 100 requests per minute\n\nAt 12:01:30:\n  Log: [12:00:31, 12:00:45, ..., 12:01:28, 12:01:29, 12:01:30]\n  Filter to last 60 seconds: count requests since 12:00:30\n  If count \u0026#x3C; 100: allow, else reject\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eVerdict\u003c/strong\u003e: Accurate, no boundary bursts. Memory-intensive (store all timestamps). Impractical for \u003e10K RPS.\u003c/p\u003e\n\u003ch3\u003eSliding Window Counter\u003c/h3\u003e\n\u003cp\u003eApproximate the sliding window using two fixed window counters and a weighted average. This is the sweet spot for most production systems: it eliminates the boundary burst problem of fixed windows while using only two integers per client instead of a full timestamp log.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRate limit: 100 requests/minute\n\nAt 12:01:45 (45 seconds into current window):\n  Previous window (12:00 - 12:01): 80 requests\n  Current window (12:01 - 12:02): 30 requests so far\n\n  Weight of previous window = (60 - 45) / 60 = 25%\n  Estimated requests in last 60s = 80 √ó 0.25 + 30 = 50\n  Under limit ‚Üí allow\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eVerdict\u003c/strong\u003e: Memory-efficient, approximation error is \u0026#x3C;0.1% in practice. The best tradeoff for most systems.\u003c/p\u003e\n\u003ch3\u003eToken Bucket\u003c/h3\u003e\n\u003cp\u003eTokens accumulate at a fixed rate (refill rate). Each request consumes one token. Burst is allowed up to the bucket capacity. Think of it like a prepaid phone plan: you accumulate credit over time and can spend it in bursts, but you can never spend more than you have.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBucket capacity: 10 tokens (max burst)\nRefill rate: 1 token/second\n\nTimeline:\n  T=0: bucket=10 (full), 8 requests ‚Üí bucket=2\n  T=3: bucket=5 (3 tokens added), 3 requests ‚Üí bucket=2\n  T=5: bucket=4 (2 tokens added), 6 requests ‚Üí REJECT (only 4 available)\n\nProperties:\n  - Allows bursting up to bucket capacity\n  - Average rate controlled by refill rate\n  - Intuitive for \"requests per second with burst allowance\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eVerdict\u003c/strong\u003e: Best for APIs where controlled bursting is acceptable (e.g., initial page load).\u003c/p\u003e\n\u003ch3\u003eLeaky Bucket\u003c/h3\u003e\n\u003cp\u003eRequests enter a queue. A worker drains the queue at a fixed rate. Overflow requests are rejected. Unlike the token bucket, which smooths the average rate but allows bursts, the leaky bucket smooths the output rate ‚Äî it is useful when your downstream system needs a steady, predictable call rate rather than handling bursty traffic.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eQueue size: 10 requests\nDrain rate: 5 requests/second\n\n  Burst of 15 requests arrives:\n  ‚Üí 10 queued, 5 rejected immediately\n  ‚Üí Queue drains at 5/s ‚Üí smooth outgoing traffic\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eVerdict\u003c/strong\u003e: Smoothes traffic for backends that need steady input. Adds latency (queue wait). Use for outbound call shaping, not incoming API protection.\u003c/p\u003e\n\u003ch2\u003eRedis Implementation: Token Bucket with Lua\u003c/h2\u003e\n\u003cp\u003eThe critical requirement for distributed rate limiting: \u003cstrong\u003eatomicity\u003c/strong\u003e. Read-modify-write must be atomic, or concurrent requests on different servers will both read \"under limit\" and both write \"at limit+1\" ‚Äî a race condition. Without atomicity, a client could send 10 simultaneous requests on 10 different app servers, and all 10 would pass the rate check before any of them had a chance to decrement the counter.\u003c/p\u003e\n\u003cp\u003eRedis Lua scripts run atomically on the Redis server, solving this problem without requiring distributed locks:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-lua\"\u003e-- token_bucket.lua\n-- KEYS[1]: rate limit key (e.g., \"rate:user:123\")\n-- ARGV[1]: max tokens (bucket capacity)\n-- ARGV[2]: refill rate (tokens per second)\n-- ARGV[3]: current timestamp (Unix seconds with milliseconds)\n-- ARGV[4]: requested tokens (usually 1)\n\nlocal key = KEYS[1]\nlocal max_tokens = tonumber(ARGV[1])\nlocal refill_rate = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\nlocal requested = tonumber(ARGV[4])\n\n-- Get current state\nlocal data = redis.call('HMGET', key, 'tokens', 'last_refill')\nlocal tokens = tonumber(data[1]) or max_tokens\nlocal last_refill = tonumber(data[2]) or now\n\n-- Calculate tokens to add based on elapsed time\nlocal elapsed = math.max(0, now - last_refill)\nlocal tokens_to_add = elapsed * refill_rate\ntokens = math.min(max_tokens, tokens + tokens_to_add)\n\n-- Check if request can be served\nlocal allowed = 0\nif tokens \u003e= requested then\n    tokens = tokens - requested\n    allowed = 1\nend\n\n-- Save state with TTL (auto-cleanup for idle clients)\nredis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)\nredis.call('EXPIRE', key, math.ceil(max_tokens / refill_rate) + 1)\n\nreturn {allowed, math.floor(tokens), math.floor(tokens_to_add)}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eEXPIRE\u003c/code\u003e call at the end is a crucial operational detail: it automatically removes rate limit state for idle clients, preventing unbounded Redis memory growth. The TTL is calculated to be just long enough that a fully drained bucket would refill to capacity.\u003c/p\u003e\n\u003cp\u003eNow that you have the Lua logic, the Java service below wires it into a callable interface. The key design choice here is passing the current timestamp from the application rather than reading it inside Lua ‚Äî this keeps the script deterministic and easier to test.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Service\npublic class TokenBucketRateLimiter {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    private final DefaultRedisScript\u0026#x3C;List\u003e luaScript;\n\n    public TokenBucketRateLimiter() {\n        this.luaScript = new DefaultRedisScript\u0026#x3C;\u003e();\n        this.luaScript.setScriptText(LUA_SCRIPT); // Load from classpath\n        this.luaScript.setResultType(List.class);\n    }\n\n    public RateLimitResult checkLimit(String clientId, RateLimitConfig config) {\n        String key = \"rate:\" + clientId;\n        double now = System.currentTimeMillis() / 1000.0;\n\n        List\u0026#x3C;Long\u003e result = redis.execute(\n            luaScript,\n            List.of(key),\n            String.valueOf(config.getMaxTokens()),\n            String.valueOf(config.getRefillRate()),\n            String.valueOf(now),\n            \"1\"\n        );\n\n        boolean allowed = result.get(0) == 1L;\n        long remainingTokens = result.get(1);\n\n        return new RateLimitResult(allowed, remainingTokens, config.getMaxTokens());\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eSpring Boot Integration: Rate Limit Filter\u003c/h2\u003e\n\u003cp\u003eWith the core limiter in place, you need to intercept every HTTP request before it reaches your business logic. A servlet filter runs before any controller code and can short-circuit the request with a \u003ccode\u003e429\u003c/code\u003e response without touching your application logic at all. This separation means you can add rate limiting to any endpoint without modifying it.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Component\n@Order(Ordered.HIGHEST_PRECEDENCE)\npublic class RateLimitFilter extends OncePerRequestFilter {\n\n    @Autowired\n    private TokenBucketRateLimiter rateLimiter;\n\n    private static final Map\u0026#x3C;String, RateLimitConfig\u003e TIER_CONFIGS = Map.of(\n        \"free\",       new RateLimitConfig(60, 1.0),   // 60 burst, 1 RPS sustained\n        \"pro\",        new RateLimitConfig(600, 10.0),  // 600 burst, 10 RPS sustained\n        \"enterprise\", new RateLimitConfig(6000, 100.0) // 6000 burst, 100 RPS\n    );\n\n    @Override\n    protected void doFilterInternal(\n            HttpServletRequest request,\n            HttpServletResponse response,\n            FilterChain chain) throws ServletException, IOException {\n\n        String clientId = extractClientId(request); // From API key or JWT\n        String tier = extractTier(clientId);\n        RateLimitConfig config = TIER_CONFIGS.getOrDefault(tier, TIER_CONFIGS.get(\"free\"));\n\n        RateLimitResult result = rateLimiter.checkLimit(clientId, config);\n\n        // Always set rate limit headers (RFC 6585)\n        response.setHeader(\"X-RateLimit-Limit\", String.valueOf(config.getMaxTokens()));\n        response.setHeader(\"X-RateLimit-Remaining\", String.valueOf(result.getRemainingTokens()));\n        response.setHeader(\"X-RateLimit-Reset\", String.valueOf(System.currentTimeMillis() / 1000 + 1));\n\n        if (!result.isAllowed()) {\n            response.setStatus(HttpStatus.TOO_MANY_REQUESTS.value());\n            response.setHeader(\"Retry-After\", \"1\");\n            response.getWriter().write(\"{\\\"error\\\": \\\"Rate limit exceeded\\\", \\\"tier\\\": \\\"\" + tier + \"\\\"}\");\n            return;\n        }\n\n        chain.doFilter(request, response);\n    }\n\n    private String extractClientId(HttpServletRequest request) {\n        String apiKey = request.getHeader(\"X-API-Key\");\n        if (apiKey != null) return \"api:\" + apiKey;\n\n        // Fall back to IP-based limiting for unauthenticated requests\n        return \"ip:\" + request.getRemoteAddr();\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlways setting the rate limit headers ‚Äî even for successful requests ‚Äî is important because clients use \u003ccode\u003eX-RateLimit-Remaining\u003c/code\u003e to implement self-throttling. A well-behaved SDK will slow down proactively when remaining tokens run low, reducing the number of rejected requests and improving the overall user experience.\u003c/p\u003e\n\u003ch2\u003eDistributed Challenges\u003c/h2\u003e\n\u003cp\u003eThe single-server implementation above is correct, but distributed systems introduce new problems. The following two challenges are the ones interviewers most commonly expect you to address.\u003c/p\u003e\n\u003ch3\u003eRedis Cluster Consistency\u003c/h3\u003e\n\u003cp\u003eIn a Redis cluster, rate limit keys can sit on different shards. Ensure the key lands on the same shard using hash tags:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Without hash tag: different shards for different clients (fine)\nString key = \"rate:\" + clientId;\n\n// With hash tag: force all rate limit keys to same shard (avoid for large deployments)\nString key = \"{rate}:\" + clientId;\n\n// Better: use consistent hashing at the application level\n// Partition clients by modulo and direct to specific Redis nodes\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eMulti-Region Rate Limiting\u003c/h3\u003e\n\u003cp\u003eStrict global rate limiting requires cross-region coordination ‚Äî expensive. In practice, use \u003cstrong\u003elocal + global\u003c/strong\u003e hybrid. Think of it like allocating traffic quotas across airport terminals: each terminal enforces its own cap, and the central authority adjusts allocations periodically rather than approving every single passenger.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eApproach: Divide global limit across regions proportionally\n\nGlobal limit: 1000 requests/minute\n  Region us-east-1: 500 req/min (50% of traffic)\n  Region eu-west-1: 300 req/min (30%)\n  Region ap-south-1: 200 req/min (20%)\n\nEach region enforces its limit independently.\nSync global counters asynchronously every 10 seconds.\nA client can exceed global limit by up to 10 seconds √ó regional rate ‚Äî acceptable for most use cases.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe 10-second sync window means a determined attacker can exceed the global limit by at most 10 seconds worth of regional traffic ‚Äî a bounded and acceptable overshoot for most business requirements.\u003c/p\u003e\n\u003ch2\u003eWhere to Apply Rate Limiting\u003c/h2\u003e\n\u003cp\u003eRate limiting is most effective when applied at multiple layers simultaneously, creating defense in depth. Each layer handles a different threat model, and together they prevent both accidental and intentional overload.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRequest path:\n  Client ‚Üí CDN ‚Üí API Gateway ‚Üí Load Balancer ‚Üí Service ‚Üí Database\n\nRate limiting layers:\n  CDN:         IP-based blocking for known abusers (bot traffic)\n  API Gateway: Per-client limits (recommended primary enforcement point)\n  Service:     Per-endpoint limits (e.g., expensive endpoints get stricter limits)\n  Database:    Connection pool limits (implicit rate limiting)\n\nRecommendation:\n  - API Gateway for business rate limits (per API key, per tier)\n  - Service layer for resource protection (expensive endpoints)\n  - Both together for defense in depth\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRate limiting is a foundational API design concern. The token bucket algorithm with Redis Lua provides the best combination of correctness, performance, and operational simplicity. The boundary burst of fixed window algorithms has caused real production incidents ‚Äî don't ship that in critical systems.\u003c/p\u003e\n","tableOfContents":[{"id":"why-rate-limiting","text":"Why Rate Limiting?","level":2},{"id":"algorithm-comparison","text":"Algorithm Comparison","level":2},{"id":"fixed-window-counter","text":"Fixed Window Counter","level":3},{"id":"sliding-window-log","text":"Sliding Window Log","level":3},{"id":"sliding-window-counter","text":"Sliding Window Counter","level":3},{"id":"token-bucket","text":"Token Bucket","level":3},{"id":"leaky-bucket","text":"Leaky Bucket","level":3},{"id":"redis-implementation-token-bucket-with-lua","text":"Redis Implementation: Token Bucket with Lua","level":2},{"id":"spring-boot-integration-rate-limit-filter","text":"Spring Boot Integration: Rate Limit Filter","level":2},{"id":"distributed-challenges","text":"Distributed Challenges","level":2},{"id":"redis-cluster-consistency","text":"Redis Cluster Consistency","level":3},{"id":"multi-region-rate-limiting","text":"Multi-Region Rate Limiting","level":3},{"id":"where-to-apply-rate-limiting","text":"Where to Apply Rate Limiting","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why ‚Äî by exploring system state through metrics, traces, and logs without needing to know in advance‚Ä¶"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory ‚Äî store events instead of state, derive state by replaying events ‚Äî is sou‚Ä¶"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t‚Ä¶"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"system-design-rate-limiter"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>