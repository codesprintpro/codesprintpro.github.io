<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Time-Series Databases: InfluxDB vs TimescaleDB vs Prometheus<!-- --> | CodeSprintPro</title><meta name="description" content="Choose the right time-series database for metrics, IoT, and observability workloads. Deep comparison of InfluxDB, TimescaleDB, and Prometheus with retention policies, downsampling, and query patterns." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/time-series-databases/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Time-Series Databases: InfluxDB vs TimescaleDB vs Prometheus" data-next-head=""/><meta property="og:description" content="Choose the right time-series database for metrics, IoT, and observability workloads. Deep comparison of InfluxDB, TimescaleDB, and Prometheus with retention policies, downsampling, and query patterns." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/time-series-databases/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-03-29" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="Databases" data-next-head=""/><meta property="article:tag" content="time-series" data-next-head=""/><meta property="article:tag" content="influxdb" data-next-head=""/><meta property="article:tag" content="timescaledb" data-next-head=""/><meta property="article:tag" content="prometheus" data-next-head=""/><meta property="article:tag" content="observability" data-next-head=""/><meta property="article:tag" content="iot" data-next-head=""/><meta property="article:tag" content="databases" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Time-Series Databases: InfluxDB vs TimescaleDB vs Prometheus" data-next-head=""/><meta name="twitter:description" content="Choose the right time-series database for metrics, IoT, and observability workloads. Deep comparison of InfluxDB, TimescaleDB, and Prometheus with retention policies, downsampling, and query patterns." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Time-Series Databases: InfluxDB vs TimescaleDB vs Prometheus","description":"Choose the right time-series database for metrics, IoT, and observability workloads. Deep comparison of InfluxDB, TimescaleDB, and Prometheus with retention policies, downsampling, and query patterns.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-03-29","dateModified":"2025-03-29","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/time-series-databases/"},"keywords":"time-series, influxdb, timescaledb, prometheus, observability, iot, databases","articleSection":"Databases"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Time-Series Databases: InfluxDB vs TimescaleDB vs Prometheus</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">Databases</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Time-Series Databases: InfluxDB vs TimescaleDB vs Prometheus</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Choose the right time-series database for metrics, IoT, and observability workloads. Deep comparison of InfluxDB, TimescaleDB, and Prometheus with retention policies, downsampling, and query patterns.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>¬∑</span><span>March 29, 2025</span><span>¬∑</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->time-series</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->influxdb</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->timescaledb</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->prometheus</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->observability</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->iot</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->databases</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Time-series data is fundamentally different from general-purpose data: it arrives in time order, is queried by time ranges, has predictable decay in value, and has write patterns that overwhelm traditional relational databases. InfluxDB, TimescaleDB, and Prometheus each solve this problem differently. Picking the wrong one means data loss, query timeouts, or a rewrite 6 months later.</p>
<h2>Why Regular Databases Fail for Time-Series</h2>
<p>Before looking at the solutions, it is worth understanding exactly why a general-purpose database like PostgreSQL struggles here. The problem is not just volume ‚Äî it is the combination of write velocity, append-only patterns, and the fact that most data becomes cold and rarely queried after a short window.</p>
<pre><code>IoT sensor data: 10,000 sensors √ó 1 reading/second = 10,000 inserts/second

PostgreSQL (without time-series extension):
  - B-tree index insertion: O(log n) per row
  - At 10K/sec: 1 billion rows/day = 864GB raw data
  - Index grows unbounded: 200GB+ index for 864GB data
  - Query: "average temperature last hour" ‚Üí full index scan of 36M rows
  - Write amplification: each insert touches multiple B-tree pages

Vacuum and bloat:
  - Sensor readings are append-only ‚Üí vacuum can't reclaim space well
  - Table bloat after months = 3√ó actual data size

TimescaleDB solves this by:
  - Chunking data by time window (1-day chunks by default)
  - Old chunks become immutable ‚Üí no vacuum overhead
  - Query planner prunes chunks by time ‚Üí only scan relevant chunks
  - Chunk-level compression: 90-95% size reduction
</code></pre>
<p>The core insight is that time-series data has a natural expiry: you care deeply about the last hour, somewhat about the last week, and almost never about data from 18 months ago. Time-series databases exploit this by automatically tiering, compressing, and eventually dropping old data ‚Äî something a general-purpose database makes you implement yourself.</p>
<h2>InfluxDB: Purpose-Built TSDB</h2>
<p>InfluxDB is designed exclusively for time-series data with its own query language (Flux) and data model.</p>
<p>InfluxDB's data model separates metadata (tags) from measurements (fields), which is a deliberate design choice: tags are indexed and meant for filtering and grouping, while fields are just stored values. If you mistakenly put a high-cardinality value (like a user ID) in a tag, InfluxDB's index grows unbounded and performance degrades dramatically. Get this distinction right and InfluxDB is extremely fast.</p>
<pre><code>Data model:
  Measurement: cpu_usage          (like a table)
  Tags: host=web-1, region=us-east (indexed metadata ‚Äî filtering)
  Fields: cpu_percent=89.2, load=1.23 (values ‚Äî not indexed)
  Timestamp: 2025-03-29T10:00:00Z (nanosecond precision)

Written as line protocol:
  cpu_usage,host=web-1,region=us-east cpu_percent=89.2,load=1.23 1711699200000000000
</code></pre>
<p>The Python client below shows the two most important write patterns: single-point writes for low-frequency data, and batch writes for high-frequency sensors. Always prefer batch writes for anything above a few points per second ‚Äî sending one HTTP request per data point at high volume is the fastest way to overwhelm both the client and the server.</p>
<pre><code class="language-python">from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS
from datetime import datetime, timedelta

client = InfluxDBClient(
    url="http://localhost:8086",
    token="your-admin-token",
    org="your-org"
)

write_api = client.write_api(write_options=SYNCHRONOUS)

# Write a point
point = (
    Point("cpu_usage")
    .tag("host", "web-1")
    .tag("region", "us-east")
    .field("cpu_percent", 89.2)
    .field("load_1m", 1.23)
    .time(datetime.utcnow())
)
write_api.write(bucket="metrics", record=point)

# Batch write (efficient for high-frequency data)
points = [
    Point("cpu_usage")
        .tag("host", f"web-{i}")
        .field("cpu_percent", 40 + i * 2.5)
        .time(datetime.utcnow())
    for i in range(100)
]
write_api.write(bucket="metrics", record=points)

# Query with Flux
query_api = client.query_api()

# Average CPU per host, last 1 hour
query = '''
from(bucket: "metrics")
  |> range(start: -1h)
  |> filter(fn: (r) => r._measurement == "cpu_usage")
  |> filter(fn: (r) => r._field == "cpu_percent")
  |> group(columns: ["host"])
  |> mean()
  |> sort(columns: ["_value"], desc: true)
'''

result = query_api.query(query=query, org="your-org")
for table in result:
    for record in table.records:
        print(f"Host: {record['host']}, Avg CPU: {record['_value']:.1f}%")

# Downsampled aggregation: 5-minute averages over last 7 days
query = '''
from(bucket: "metrics")
  |> range(start: -7d)
  |> filter(fn: (r) => r._measurement == "cpu_usage")
  |> filter(fn: (r) => r._field == "cpu_percent")
  |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)
'''
</code></pre>
<p>Retention policies and automatic downsampling are where InfluxDB really shines for IoT workloads. The tier structure below keeps raw data for just 7 days, then retains progressively coarser aggregates for months or years. This gives you fast recent queries on raw data and affordable long-term trend analysis on pre-aggregated data ‚Äî without any manual data management.</p>
<pre><code>InfluxDB Retention Policies:
  Bucket: metrics_raw        ‚Üí retention: 7 days   (high resolution)
  Bucket: metrics_hourly     ‚Üí retention: 90 days  (1-hour aggregates)
  Bucket: metrics_daily      ‚Üí retention: 2 years  (daily aggregates)

InfluxDB Task (continuous downsampling):
  option task = {name: "Downsample to hourly", every: 1h}

  from(bucket: "metrics_raw")
    |> range(start: -1h)
    |> filter(fn: (r) => r._measurement == "cpu_usage")
    |> aggregateWindow(every: 1h, fn: mean)
    |> to(bucket: "metrics_hourly")
</code></pre>
<h2>TimescaleDB: PostgreSQL for Time-Series</h2>
<p>TimescaleDB extends PostgreSQL with time-series superpowers while remaining fully PostgreSQL-compatible.</p>
<p>The biggest advantage of TimescaleDB is not performance ‚Äî it is familiarity. If your team already knows SQL, already operates PostgreSQL, and already has tooling around it, TimescaleDB adds time-series capability without introducing a new database engine to learn, operate, and monitor.</p>
<p>The setup process below is intentionally familiar: you create a normal PostgreSQL table, then call <code>create_hypertable</code> to activate TimescaleDB's automatic partitioning. Existing applications that query this table via standard SQL continue to work unchanged ‚Äî TimescaleDB is transparent to the query layer.</p>
<pre><code class="language-sql">-- Enable extension
CREATE EXTENSION IF NOT EXISTS timescaledb;

-- Create a regular table first
CREATE TABLE sensor_data (
    time        TIMESTAMPTZ NOT NULL,
    sensor_id   VARCHAR(50) NOT NULL,
    location    VARCHAR(100),
    temperature DOUBLE PRECISION,
    humidity    DOUBLE PRECISION,
    pressure    DOUBLE PRECISION
);

-- Convert to hypertable (TimescaleDB magic)
-- Creates automatic partitioning by time (7-day chunks by default)
SELECT create_hypertable('sensor_data', 'time');

-- Optional: partition by space dimension too (for IoT: partition by sensor_id)
SELECT create_hypertable('sensor_data', 'time',
    partitioning_column => 'sensor_id',
    number_partitions => 8
);

-- Indexes (TimescaleDB creates them per chunk ‚Üí much more efficient)
CREATE INDEX idx_sensor_data_sensor_time ON sensor_data (sensor_id, time DESC);
</code></pre>
<p>TimescaleDB's <code>time_bucket</code> function is the key abstraction for time-series aggregation. It divides the time axis into equal windows (5 minutes, 1 hour, 1 day) and lets you aggregate within each window using standard SQL aggregates. Gap filling is equally valuable ‚Äî real sensor networks have missing data, and <code>LOCF</code> (Last Observation Carry Forward) lets you produce clean, uniform time series for dashboards without preprocessing the data in your application.</p>
<pre><code class="language-sql">-- Queries: full PostgreSQL SQL + time-series functions
-- Last hour of readings for a sensor
SELECT time, temperature, humidity
FROM sensor_data
WHERE sensor_id = 'sensor-42'
  AND time > NOW() - INTERVAL '1 hour'
ORDER BY time DESC;

-- TimescaleDB time_bucket: aggregate by time window
SELECT
    time_bucket('5 minutes', time) AS bucket,
    sensor_id,
    AVG(temperature)   AS avg_temp,
    MIN(temperature)   AS min_temp,
    MAX(temperature)   AS max_temp,
    COUNT(*)           AS readings
FROM sensor_data
WHERE time > NOW() - INTERVAL '24 hours'
GROUP BY bucket, sensor_id
ORDER BY bucket DESC, sensor_id;

-- Gap filling: fill missing intervals with NULL or forward-fill
SELECT
    time_bucket_gapfill('5 minutes', time) AS bucket,
    sensor_id,
    LOCF(AVG(temperature)) AS temperature  -- Last observation carry forward
FROM sensor_data
WHERE time BETWEEN NOW() - INTERVAL '24h' AND NOW()
GROUP BY bucket, sensor_id
ORDER BY bucket;
</code></pre>
<p>Compression and continuous aggregates are the features that make TimescaleDB viable for long-running IoT deployments. The SQL below enables columnar compression on chunks older than 7 days ‚Äî yielding 90%+ storage reduction for typical sensor data ‚Äî and creates a materialized view that is automatically refreshed. Queries against <code>sensor_hourly</code> return pre-aggregated data instantly, rather than scanning millions of raw readings.</p>
<pre><code class="language-sql">-- Compression (90%+ reduction for IoT data)
-- Enable compression with 7-day delay (keep last 7 days uncompressed for fast inserts)
ALTER TABLE sensor_data SET (
    timescaledb.compress,
    timescaledb.compress_orderby = 'time DESC',
    timescaledb.compress_segmentby = 'sensor_id'
);

SELECT add_compression_policy('sensor_data', INTERVAL '7 days');

-- Automatic retention policy
SELECT add_retention_policy('sensor_data', INTERVAL '1 year');

-- Continuous aggregates (materialized, automatically updated)
CREATE MATERIALIZED VIEW sensor_hourly
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', time) AS hour,
    sensor_id,
    AVG(temperature) AS avg_temp,
    MIN(temperature) AS min_temp,
    MAX(temperature) AS max_temp
FROM sensor_data
GROUP BY hour, sensor_id
WITH NO DATA;

-- Policy: refresh aggregate every hour for last 3 hours
SELECT add_continuous_aggregate_policy('sensor_hourly',
    start_offset => INTERVAL '3 hours',
    end_offset   => INTERVAL '1 hour',
    schedule_interval => INTERVAL '1 hour'
);

-- Query the materialized aggregate (fast, pre-computed)
SELECT * FROM sensor_hourly
WHERE sensor_id = 'sensor-42'
  AND hour > NOW() - INTERVAL '7 days'
ORDER BY hour DESC;
</code></pre>
<h2>Prometheus: Metrics-First</h2>
<p>Prometheus is a pull-based metrics system ‚Äî services expose metrics, Prometheus scrapes them.</p>
<p>Prometheus works on an inverted model from the other two databases: instead of your application pushing data to Prometheus, Prometheus reaches out and pulls from your services on a schedule. This pull model makes it easy to see when a service is down (it stops being scrapeable), and it keeps the metrics pipeline decoupled from application code ‚Äî your service just needs to expose an HTTP endpoint.</p>
<p>The configuration below sets up Prometheus to scrape three Spring Boot microservices and a node exporter for server-level metrics. Services must expose a <code>/actuator/prometheus</code> endpoint, which Spring Boot's Micrometer integration provides automatically when you add the <code>micrometer-registry-prometheus</code> dependency.</p>
<pre><code class="language-yaml"># prometheus.yml
global:
  scrape_interval: 15s      # How often to scrape

scrape_configs:
  - job_name: spring-boot-services
    metrics_path: /actuator/prometheus
    static_configs:
      - targets:
        - order-service:8080
        - payment-service:8080
        - inventory-service:8080

  - job_name: node-exporter    # Server metrics (CPU, memory, disk)
    static_configs:
      - targets: ['node-exporter:9100']

# Retention
storage:
  tsdb:
    retention.time: 15d
    retention.size: 50GB

# For long-term storage: Thanos or VictoriaMetrics sidecar
</code></pre>
<p>Beyond the built-in HTTP metrics that Micrometer auto-instruments (request rate, latency histograms, JVM stats), you often want custom business metrics. The three metric types below cover the most common use cases: counters for things that only go up (orders created), histograms for distributions (order value), and gauges for current state (pending order count). Defining these in a dedicated metrics class keeps instrumentation organized and testable.</p>
<pre><code class="language-java">// Spring Boot Prometheus metrics (auto-instrumented)
// Add dependency: micrometer-registry-prometheus

// Custom business metrics
@Service
public class OrderMetrics {

    @Autowired
    private MeterRegistry registry;

    // Counter: total orders created
    private Counter ordersCreated = Counter.builder("orders.created.total")
        .tag("payment_method", "credit_card")
        .description("Total orders created")
        .register(registry);

    // Histogram: order value distribution
    private DistributionSummary orderValue = DistributionSummary
        .builder("orders.value.cents")
        .scale(0.01)  // Convert cents to dollars for display
        .publishPercentiles(0.5, 0.95, 0.99)
        .register(registry);

    // Gauge: current pending orders
    private AtomicInteger pendingOrders = registry.gauge(
        "orders.pending.current", new AtomicInteger(0)
    );
}
</code></pre>
<p>PromQL is the query language you use to turn raw metric samples into actionable signals in Grafana dashboards and alerting rules. The three queries below cover the most important patterns: computing a rate from a counter, calculating a high-percentile latency from a histogram, and expressing an alert threshold as a ratio. The <code>rate()</code> function is the workhorse of PromQL ‚Äî it handles counter resets (service restarts) automatically and computes a per-second rate over the specified window.</p>
<pre><code class="language-promql"># PromQL queries (used in Grafana dashboards)

# Request rate per second (5-minute window)
rate(http_server_requests_seconds_count[5m])

# P99 latency by service
histogram_quantile(0.99,
  sum by (service, le) (
    rate(http_server_requests_seconds_bucket[5m])
  )
)

# Alert: error rate > 5%
(
  sum(rate(http_server_requests_seconds_count{status=~"5.."}[5m]))
  /
  sum(rate(http_server_requests_seconds_count[5m]))
) > 0.05
</code></pre>
<h2>Comparison and Decision Guide</h2>
<table>
<thead>
<tr>
<th>Factor</th>
<th>InfluxDB</th>
<th>TimescaleDB</th>
<th>Prometheus</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Query language</strong></td>
<td>Flux (custom)</td>
<td>SQL</td>
<td>PromQL</td>
</tr>
<tr>
<td><strong>Write throughput</strong></td>
<td>Very high (500K/sec)</td>
<td>High (100K/sec)</td>
<td>Low (pull-based)</td>
</tr>
<tr>
<td><strong>Cardinality limit</strong></td>
<td>Medium (millions)</td>
<td>High (billions)</td>
<td>Low (millions)</td>
</tr>
<tr>
<td><strong>Long-term storage</strong></td>
<td>Native buckets</td>
<td>Native + compression</td>
<td>Needs Thanos/VictoriaMetrics</td>
</tr>
<tr>
<td><strong>Joins/analytics</strong></td>
<td>Limited</td>
<td>Full SQL</td>
<td>No</td>
</tr>
<tr>
<td><strong>Use case</strong></td>
<td>IoT, telemetry</td>
<td>General time-series</td>
<td>Service metrics</td>
</tr>
<tr>
<td><strong>Operational complexity</strong></td>
<td>Medium</td>
<td>Low (just PostgreSQL)</td>
<td>Low</td>
</tr>
</tbody>
</table>
<p>The comparison above shows that no single tool is best across all dimensions. Use the decision guide below to match your specific requirements to the right choice, keeping in mind that many production setups combine two of these tools ‚Äî typically Prometheus for short-term alerting and TimescaleDB or InfluxDB for long-term trend analysis.</p>
<pre><code>Choose InfluxDB when:
  - IoT data at high velocity (100K+ writes/sec)
  - Need built-in downsampling and retention policies
  - Time-series is your only data model

Choose TimescaleDB when:
  - Already using PostgreSQL (zero new infrastructure)
  - Need SQL joins (time-series + relational data together)
  - Want compression + retention + SQL in one system
  - Application data and metrics in same database

Choose Prometheus when:
  - Monitoring and alerting is the primary use case
  - Service metrics (latency, error rate, saturation)
  - Grafana dashboards
  - Already in Kubernetes ecosystem
  - Short retention (15-30 days) is acceptable

Common architecture:
  Prometheus (short-term metrics) + Thanos (long-term storage)
  OR
  Prometheus (alerting/dashboards) + TimescaleDB (long-term analytics)
</code></pre>
<p>TimescaleDB is the pragmatic choice for most teams: if you already have PostgreSQL, it's zero additional infrastructure, the SQL compatibility means no new query language to learn, and the performance improvements over vanilla PostgreSQL are substantial. For high-velocity IoT data or when you need a purpose-built TSDB, InfluxDB shines. For monitoring and alerting in a Kubernetes environment, Prometheus is the de facto standard.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">üìö</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Designing Data-Intensive Applications</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Essential</span></div><p class="text-xs text-gray-600">The go-to book for understanding databases, consistency, and distributed data.</p></div><a href="https://amzn.to/3RyKzOA" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">MongoDB ‚Äî The Complete Developer&#x27;s Guide ‚Äî Udemy</span></div><p class="text-xs text-gray-600">Comprehensive MongoDB course from basics to advanced aggregations.</p></div><a href="https://www.udemy.com/course/mongodb-the-complete-developers-guide/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> ‚Üí</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Time-Series%20Databases%3A%20InfluxDB%20vs%20TimescaleDB%20vs%20Prometheus&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Ftime-series-databases%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Ftime-series-databases%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#why-regular-databases-fail-for-time-series" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Why Regular Databases Fail for Time-Series</a></li><li class=""><a href="#influxdb-purpose-built-tsdb" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">InfluxDB: Purpose-Built TSDB</a></li><li class=""><a href="#timescaledb-postgresql-for-time-series" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">TimescaleDB: PostgreSQL for Time-Series</a></li><li class=""><a href="#prometheus-metrics-first" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Prometheus: Metrics-First</a></li><li class=""><a href="#comparison-and-decision-guide" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Comparison and Decision Guide</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/cassandra-data-modeling/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-green-100 text-green-700">Databases</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Cassandra Data Modeling: Design for Queries, Not Entities</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring ‚Äî every node is equal, there&#x27;s no primary, and data placement is determined by partit‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 18, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>9 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->cassandra</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->nosql</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->data modeling</span></div></article></a><a href="/blog/dynamodb-advanced-patterns/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-green-100 text-green-700">Databases</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">DynamoDB Advanced Patterns: Single-Table Design and Beyond</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 13, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>9 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->dynamodb</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->aws</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->nosql</span></div></article></a><a href="/blog/zero-downtime-database-migrations/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-green-100 text-green-700">Databases</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Zero-Downtime Database Migrations: Patterns for Production</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible ‚Äî rollback a bad deploy and your code is back to the previous version. Database schema changes are st‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 8, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>8 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->database</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->migrations</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->postgresql</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">‚Üê Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS ‚Äî by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">¬© <!-- -->2026<!-- --> CodeSprintPro ¬∑ Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js ¬∑ TailwindCSS ¬∑ Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Time-Series Databases: InfluxDB vs TimescaleDB vs Prometheus","description":"Choose the right time-series database for metrics, IoT, and observability workloads. Deep comparison of InfluxDB, TimescaleDB, and Prometheus with retention policies, downsampling, and query patterns.","date":"2025-03-29","category":"Databases","tags":["time-series","influxdb","timescaledb","prometheus","observability","iot","databases"],"featured":false,"affiliateSection":"database-resources","slug":"time-series-databases","readingTime":"11 min read","excerpt":"Time-series data is fundamentally different from general-purpose data: it arrives in time order, is queried by time ranges, has predictable decay in value, and has write patterns that overwhelm traditional relational dat‚Ä¶","contentHtml":"\u003cp\u003eTime-series data is fundamentally different from general-purpose data: it arrives in time order, is queried by time ranges, has predictable decay in value, and has write patterns that overwhelm traditional relational databases. InfluxDB, TimescaleDB, and Prometheus each solve this problem differently. Picking the wrong one means data loss, query timeouts, or a rewrite 6 months later.\u003c/p\u003e\n\u003ch2\u003eWhy Regular Databases Fail for Time-Series\u003c/h2\u003e\n\u003cp\u003eBefore looking at the solutions, it is worth understanding exactly why a general-purpose database like PostgreSQL struggles here. The problem is not just volume ‚Äî it is the combination of write velocity, append-only patterns, and the fact that most data becomes cold and rarely queried after a short window.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eIoT sensor data: 10,000 sensors √ó 1 reading/second = 10,000 inserts/second\n\nPostgreSQL (without time-series extension):\n  - B-tree index insertion: O(log n) per row\n  - At 10K/sec: 1 billion rows/day = 864GB raw data\n  - Index grows unbounded: 200GB+ index for 864GB data\n  - Query: \"average temperature last hour\" ‚Üí full index scan of 36M rows\n  - Write amplification: each insert touches multiple B-tree pages\n\nVacuum and bloat:\n  - Sensor readings are append-only ‚Üí vacuum can't reclaim space well\n  - Table bloat after months = 3√ó actual data size\n\nTimescaleDB solves this by:\n  - Chunking data by time window (1-day chunks by default)\n  - Old chunks become immutable ‚Üí no vacuum overhead\n  - Query planner prunes chunks by time ‚Üí only scan relevant chunks\n  - Chunk-level compression: 90-95% size reduction\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe core insight is that time-series data has a natural expiry: you care deeply about the last hour, somewhat about the last week, and almost never about data from 18 months ago. Time-series databases exploit this by automatically tiering, compressing, and eventually dropping old data ‚Äî something a general-purpose database makes you implement yourself.\u003c/p\u003e\n\u003ch2\u003eInfluxDB: Purpose-Built TSDB\u003c/h2\u003e\n\u003cp\u003eInfluxDB is designed exclusively for time-series data with its own query language (Flux) and data model.\u003c/p\u003e\n\u003cp\u003eInfluxDB's data model separates metadata (tags) from measurements (fields), which is a deliberate design choice: tags are indexed and meant for filtering and grouping, while fields are just stored values. If you mistakenly put a high-cardinality value (like a user ID) in a tag, InfluxDB's index grows unbounded and performance degrades dramatically. Get this distinction right and InfluxDB is extremely fast.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eData model:\n  Measurement: cpu_usage          (like a table)\n  Tags: host=web-1, region=us-east (indexed metadata ‚Äî filtering)\n  Fields: cpu_percent=89.2, load=1.23 (values ‚Äî not indexed)\n  Timestamp: 2025-03-29T10:00:00Z (nanosecond precision)\n\nWritten as line protocol:\n  cpu_usage,host=web-1,region=us-east cpu_percent=89.2,load=1.23 1711699200000000000\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe Python client below shows the two most important write patterns: single-point writes for low-frequency data, and batch writes for high-frequency sensors. Always prefer batch writes for anything above a few points per second ‚Äî sending one HTTP request per data point at high volume is the fastest way to overwhelm both the client and the server.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom influxdb_client import InfluxDBClient, Point\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nfrom datetime import datetime, timedelta\n\nclient = InfluxDBClient(\n    url=\"http://localhost:8086\",\n    token=\"your-admin-token\",\n    org=\"your-org\"\n)\n\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\n\n# Write a point\npoint = (\n    Point(\"cpu_usage\")\n    .tag(\"host\", \"web-1\")\n    .tag(\"region\", \"us-east\")\n    .field(\"cpu_percent\", 89.2)\n    .field(\"load_1m\", 1.23)\n    .time(datetime.utcnow())\n)\nwrite_api.write(bucket=\"metrics\", record=point)\n\n# Batch write (efficient for high-frequency data)\npoints = [\n    Point(\"cpu_usage\")\n        .tag(\"host\", f\"web-{i}\")\n        .field(\"cpu_percent\", 40 + i * 2.5)\n        .time(datetime.utcnow())\n    for i in range(100)\n]\nwrite_api.write(bucket=\"metrics\", record=points)\n\n# Query with Flux\nquery_api = client.query_api()\n\n# Average CPU per host, last 1 hour\nquery = '''\nfrom(bucket: \"metrics\")\n  |\u003e range(start: -1h)\n  |\u003e filter(fn: (r) =\u003e r._measurement == \"cpu_usage\")\n  |\u003e filter(fn: (r) =\u003e r._field == \"cpu_percent\")\n  |\u003e group(columns: [\"host\"])\n  |\u003e mean()\n  |\u003e sort(columns: [\"_value\"], desc: true)\n'''\n\nresult = query_api.query(query=query, org=\"your-org\")\nfor table in result:\n    for record in table.records:\n        print(f\"Host: {record['host']}, Avg CPU: {record['_value']:.1f}%\")\n\n# Downsampled aggregation: 5-minute averages over last 7 days\nquery = '''\nfrom(bucket: \"metrics\")\n  |\u003e range(start: -7d)\n  |\u003e filter(fn: (r) =\u003e r._measurement == \"cpu_usage\")\n  |\u003e filter(fn: (r) =\u003e r._field == \"cpu_percent\")\n  |\u003e aggregateWindow(every: 5m, fn: mean, createEmpty: false)\n'''\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRetention policies and automatic downsampling are where InfluxDB really shines for IoT workloads. The tier structure below keeps raw data for just 7 days, then retains progressively coarser aggregates for months or years. This gives you fast recent queries on raw data and affordable long-term trend analysis on pre-aggregated data ‚Äî without any manual data management.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eInfluxDB Retention Policies:\n  Bucket: metrics_raw        ‚Üí retention: 7 days   (high resolution)\n  Bucket: metrics_hourly     ‚Üí retention: 90 days  (1-hour aggregates)\n  Bucket: metrics_daily      ‚Üí retention: 2 years  (daily aggregates)\n\nInfluxDB Task (continuous downsampling):\n  option task = {name: \"Downsample to hourly\", every: 1h}\n\n  from(bucket: \"metrics_raw\")\n    |\u003e range(start: -1h)\n    |\u003e filter(fn: (r) =\u003e r._measurement == \"cpu_usage\")\n    |\u003e aggregateWindow(every: 1h, fn: mean)\n    |\u003e to(bucket: \"metrics_hourly\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eTimescaleDB: PostgreSQL for Time-Series\u003c/h2\u003e\n\u003cp\u003eTimescaleDB extends PostgreSQL with time-series superpowers while remaining fully PostgreSQL-compatible.\u003c/p\u003e\n\u003cp\u003eThe biggest advantage of TimescaleDB is not performance ‚Äî it is familiarity. If your team already knows SQL, already operates PostgreSQL, and already has tooling around it, TimescaleDB adds time-series capability without introducing a new database engine to learn, operate, and monitor.\u003c/p\u003e\n\u003cp\u003eThe setup process below is intentionally familiar: you create a normal PostgreSQL table, then call \u003ccode\u003ecreate_hypertable\u003c/code\u003e to activate TimescaleDB's automatic partitioning. Existing applications that query this table via standard SQL continue to work unchanged ‚Äî TimescaleDB is transparent to the query layer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003e-- Enable extension\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Create a regular table first\nCREATE TABLE sensor_data (\n    time        TIMESTAMPTZ NOT NULL,\n    sensor_id   VARCHAR(50) NOT NULL,\n    location    VARCHAR(100),\n    temperature DOUBLE PRECISION,\n    humidity    DOUBLE PRECISION,\n    pressure    DOUBLE PRECISION\n);\n\n-- Convert to hypertable (TimescaleDB magic)\n-- Creates automatic partitioning by time (7-day chunks by default)\nSELECT create_hypertable('sensor_data', 'time');\n\n-- Optional: partition by space dimension too (for IoT: partition by sensor_id)\nSELECT create_hypertable('sensor_data', 'time',\n    partitioning_column =\u003e 'sensor_id',\n    number_partitions =\u003e 8\n);\n\n-- Indexes (TimescaleDB creates them per chunk ‚Üí much more efficient)\nCREATE INDEX idx_sensor_data_sensor_time ON sensor_data (sensor_id, time DESC);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTimescaleDB's \u003ccode\u003etime_bucket\u003c/code\u003e function is the key abstraction for time-series aggregation. It divides the time axis into equal windows (5 minutes, 1 hour, 1 day) and lets you aggregate within each window using standard SQL aggregates. Gap filling is equally valuable ‚Äî real sensor networks have missing data, and \u003ccode\u003eLOCF\u003c/code\u003e (Last Observation Carry Forward) lets you produce clean, uniform time series for dashboards without preprocessing the data in your application.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003e-- Queries: full PostgreSQL SQL + time-series functions\n-- Last hour of readings for a sensor\nSELECT time, temperature, humidity\nFROM sensor_data\nWHERE sensor_id = 'sensor-42'\n  AND time \u003e NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n\n-- TimescaleDB time_bucket: aggregate by time window\nSELECT\n    time_bucket('5 minutes', time) AS bucket,\n    sensor_id,\n    AVG(temperature)   AS avg_temp,\n    MIN(temperature)   AS min_temp,\n    MAX(temperature)   AS max_temp,\n    COUNT(*)           AS readings\nFROM sensor_data\nWHERE time \u003e NOW() - INTERVAL '24 hours'\nGROUP BY bucket, sensor_id\nORDER BY bucket DESC, sensor_id;\n\n-- Gap filling: fill missing intervals with NULL or forward-fill\nSELECT\n    time_bucket_gapfill('5 minutes', time) AS bucket,\n    sensor_id,\n    LOCF(AVG(temperature)) AS temperature  -- Last observation carry forward\nFROM sensor_data\nWHERE time BETWEEN NOW() - INTERVAL '24h' AND NOW()\nGROUP BY bucket, sensor_id\nORDER BY bucket;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCompression and continuous aggregates are the features that make TimescaleDB viable for long-running IoT deployments. The SQL below enables columnar compression on chunks older than 7 days ‚Äî yielding 90%+ storage reduction for typical sensor data ‚Äî and creates a materialized view that is automatically refreshed. Queries against \u003ccode\u003esensor_hourly\u003c/code\u003e return pre-aggregated data instantly, rather than scanning millions of raw readings.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003e-- Compression (90%+ reduction for IoT data)\n-- Enable compression with 7-day delay (keep last 7 days uncompressed for fast inserts)\nALTER TABLE sensor_data SET (\n    timescaledb.compress,\n    timescaledb.compress_orderby = 'time DESC',\n    timescaledb.compress_segmentby = 'sensor_id'\n);\n\nSELECT add_compression_policy('sensor_data', INTERVAL '7 days');\n\n-- Automatic retention policy\nSELECT add_retention_policy('sensor_data', INTERVAL '1 year');\n\n-- Continuous aggregates (materialized, automatically updated)\nCREATE MATERIALIZED VIEW sensor_hourly\nWITH (timescaledb.continuous) AS\nSELECT\n    time_bucket('1 hour', time) AS hour,\n    sensor_id,\n    AVG(temperature) AS avg_temp,\n    MIN(temperature) AS min_temp,\n    MAX(temperature) AS max_temp\nFROM sensor_data\nGROUP BY hour, sensor_id\nWITH NO DATA;\n\n-- Policy: refresh aggregate every hour for last 3 hours\nSELECT add_continuous_aggregate_policy('sensor_hourly',\n    start_offset =\u003e INTERVAL '3 hours',\n    end_offset   =\u003e INTERVAL '1 hour',\n    schedule_interval =\u003e INTERVAL '1 hour'\n);\n\n-- Query the materialized aggregate (fast, pre-computed)\nSELECT * FROM sensor_hourly\nWHERE sensor_id = 'sensor-42'\n  AND hour \u003e NOW() - INTERVAL '7 days'\nORDER BY hour DESC;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003ePrometheus: Metrics-First\u003c/h2\u003e\n\u003cp\u003ePrometheus is a pull-based metrics system ‚Äî services expose metrics, Prometheus scrapes them.\u003c/p\u003e\n\u003cp\u003ePrometheus works on an inverted model from the other two databases: instead of your application pushing data to Prometheus, Prometheus reaches out and pulls from your services on a schedule. This pull model makes it easy to see when a service is down (it stops being scrapeable), and it keeps the metrics pipeline decoupled from application code ‚Äî your service just needs to expose an HTTP endpoint.\u003c/p\u003e\n\u003cp\u003eThe configuration below sets up Prometheus to scrape three Spring Boot microservices and a node exporter for server-level metrics. Services must expose a \u003ccode\u003e/actuator/prometheus\u003c/code\u003e endpoint, which Spring Boot's Micrometer integration provides automatically when you add the \u003ccode\u003emicrometer-registry-prometheus\u003c/code\u003e dependency.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003e# prometheus.yml\nglobal:\n  scrape_interval: 15s      # How often to scrape\n\nscrape_configs:\n  - job_name: spring-boot-services\n    metrics_path: /actuator/prometheus\n    static_configs:\n      - targets:\n        - order-service:8080\n        - payment-service:8080\n        - inventory-service:8080\n\n  - job_name: node-exporter    # Server metrics (CPU, memory, disk)\n    static_configs:\n      - targets: ['node-exporter:9100']\n\n# Retention\nstorage:\n  tsdb:\n    retention.time: 15d\n    retention.size: 50GB\n\n# For long-term storage: Thanos or VictoriaMetrics sidecar\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBeyond the built-in HTTP metrics that Micrometer auto-instruments (request rate, latency histograms, JVM stats), you often want custom business metrics. The three metric types below cover the most common use cases: counters for things that only go up (orders created), histograms for distributions (order value), and gauges for current state (pending order count). Defining these in a dedicated metrics class keeps instrumentation organized and testable.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Spring Boot Prometheus metrics (auto-instrumented)\n// Add dependency: micrometer-registry-prometheus\n\n// Custom business metrics\n@Service\npublic class OrderMetrics {\n\n    @Autowired\n    private MeterRegistry registry;\n\n    // Counter: total orders created\n    private Counter ordersCreated = Counter.builder(\"orders.created.total\")\n        .tag(\"payment_method\", \"credit_card\")\n        .description(\"Total orders created\")\n        .register(registry);\n\n    // Histogram: order value distribution\n    private DistributionSummary orderValue = DistributionSummary\n        .builder(\"orders.value.cents\")\n        .scale(0.01)  // Convert cents to dollars for display\n        .publishPercentiles(0.5, 0.95, 0.99)\n        .register(registry);\n\n    // Gauge: current pending orders\n    private AtomicInteger pendingOrders = registry.gauge(\n        \"orders.pending.current\", new AtomicInteger(0)\n    );\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePromQL is the query language you use to turn raw metric samples into actionable signals in Grafana dashboards and alerting rules. The three queries below cover the most important patterns: computing a rate from a counter, calculating a high-percentile latency from a histogram, and expressing an alert threshold as a ratio. The \u003ccode\u003erate()\u003c/code\u003e function is the workhorse of PromQL ‚Äî it handles counter resets (service restarts) automatically and computes a per-second rate over the specified window.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-promql\"\u003e# PromQL queries (used in Grafana dashboards)\n\n# Request rate per second (5-minute window)\nrate(http_server_requests_seconds_count[5m])\n\n# P99 latency by service\nhistogram_quantile(0.99,\n  sum by (service, le) (\n    rate(http_server_requests_seconds_bucket[5m])\n  )\n)\n\n# Alert: error rate \u003e 5%\n(\n  sum(rate(http_server_requests_seconds_count{status=~\"5..\"}[5m]))\n  /\n  sum(rate(http_server_requests_seconds_count[5m]))\n) \u003e 0.05\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eComparison and Decision Guide\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFactor\u003c/th\u003e\n\u003cth\u003eInfluxDB\u003c/th\u003e\n\u003cth\u003eTimescaleDB\u003c/th\u003e\n\u003cth\u003ePrometheus\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eQuery language\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eFlux (custom)\u003c/td\u003e\n\u003ctd\u003eSQL\u003c/td\u003e\n\u003ctd\u003ePromQL\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eWrite throughput\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eVery high (500K/sec)\u003c/td\u003e\n\u003ctd\u003eHigh (100K/sec)\u003c/td\u003e\n\u003ctd\u003eLow (pull-based)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eCardinality limit\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eMedium (millions)\u003c/td\u003e\n\u003ctd\u003eHigh (billions)\u003c/td\u003e\n\u003ctd\u003eLow (millions)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eLong-term storage\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNative buckets\u003c/td\u003e\n\u003ctd\u003eNative + compression\u003c/td\u003e\n\u003ctd\u003eNeeds Thanos/VictoriaMetrics\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eJoins/analytics\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eLimited\u003c/td\u003e\n\u003ctd\u003eFull SQL\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eUse case\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eIoT, telemetry\u003c/td\u003e\n\u003ctd\u003eGeneral time-series\u003c/td\u003e\n\u003ctd\u003eService metrics\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eOperational complexity\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eMedium\u003c/td\u003e\n\u003ctd\u003eLow (just PostgreSQL)\u003c/td\u003e\n\u003ctd\u003eLow\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThe comparison above shows that no single tool is best across all dimensions. Use the decision guide below to match your specific requirements to the right choice, keeping in mind that many production setups combine two of these tools ‚Äî typically Prometheus for short-term alerting and TimescaleDB or InfluxDB for long-term trend analysis.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eChoose InfluxDB when:\n  - IoT data at high velocity (100K+ writes/sec)\n  - Need built-in downsampling and retention policies\n  - Time-series is your only data model\n\nChoose TimescaleDB when:\n  - Already using PostgreSQL (zero new infrastructure)\n  - Need SQL joins (time-series + relational data together)\n  - Want compression + retention + SQL in one system\n  - Application data and metrics in same database\n\nChoose Prometheus when:\n  - Monitoring and alerting is the primary use case\n  - Service metrics (latency, error rate, saturation)\n  - Grafana dashboards\n  - Already in Kubernetes ecosystem\n  - Short retention (15-30 days) is acceptable\n\nCommon architecture:\n  Prometheus (short-term metrics) + Thanos (long-term storage)\n  OR\n  Prometheus (alerting/dashboards) + TimescaleDB (long-term analytics)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTimescaleDB is the pragmatic choice for most teams: if you already have PostgreSQL, it's zero additional infrastructure, the SQL compatibility means no new query language to learn, and the performance improvements over vanilla PostgreSQL are substantial. For high-velocity IoT data or when you need a purpose-built TSDB, InfluxDB shines. For monitoring and alerting in a Kubernetes environment, Prometheus is the de facto standard.\u003c/p\u003e\n","tableOfContents":[{"id":"why-regular-databases-fail-for-time-series","text":"Why Regular Databases Fail for Time-Series","level":2},{"id":"influxdb-purpose-built-tsdb","text":"InfluxDB: Purpose-Built TSDB","level":2},{"id":"timescaledb-postgresql-for-time-series","text":"TimescaleDB: PostgreSQL for Time-Series","level":2},{"id":"prometheus-metrics-first","text":"Prometheus: Metrics-First","level":2},{"id":"comparison-and-decision-guide","text":"Comparison and Decision Guide","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring ‚Äî every node is equal, there's no primary, and data placement is determined by partit‚Ä¶"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you‚Ä¶"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible ‚Äî rollback a bad deploy and your code is back to the previous version. Database schema changes are st‚Ä¶"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"time-series-databases"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>