<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">System Design: Video Streaming Platform at Netflix Scale<!-- --> | CodeSprintPro</title><meta name="description" content="Design a video streaming platform handling 250M users and 15% of global internet traffic. Covers video transcoding pipeline, CDN architecture, adaptive bitrate streaming, and recommendation systems." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/system-design-video-streaming/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="System Design: Video Streaming Platform at Netflix Scale" data-next-head=""/><meta property="og:description" content="Design a video streaming platform handling 250M users and 15% of global internet traffic. Covers video transcoding pipeline, CDN architecture, adaptive bitrate streaming, and recommendation systems." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/system-design-video-streaming/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-03-19" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="System Design" data-next-head=""/><meta property="article:tag" content="system design" data-next-head=""/><meta property="article:tag" content="video streaming" data-next-head=""/><meta property="article:tag" content="cdn" data-next-head=""/><meta property="article:tag" content="aws" data-next-head=""/><meta property="article:tag" content="distributed systems" data-next-head=""/><meta property="article:tag" content="hls" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="System Design: Video Streaming Platform at Netflix Scale" data-next-head=""/><meta name="twitter:description" content="Design a video streaming platform handling 250M users and 15% of global internet traffic. Covers video transcoding pipeline, CDN architecture, adaptive bitrate streaming, and recommendation systems." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"System Design: Video Streaming Platform at Netflix Scale","description":"Design a video streaming platform handling 250M users and 15% of global internet traffic. Covers video transcoding pipeline, CDN architecture, adaptive bitrate streaming, and recommendation systems.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-03-19","dateModified":"2025-03-19","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/system-design-video-streaming/"},"keywords":"system design, video streaming, cdn, aws, distributed systems, hls","articleSection":"System Design"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">System Design: Video Streaming Platform at Netflix Scale</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">System Design</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">System Design: Video Streaming Platform at Netflix Scale</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Design a video streaming platform handling 250M users and 15% of global internet traffic. Covers video transcoding pipeline, CDN architecture, adaptive bitrate streaming, and recommendation systems.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>¬∑</span><span>March 19, 2025</span><span>¬∑</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->system design</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->video streaming</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->cdn</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->aws</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->distributed systems</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->hls</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Netflix accounts for 15% of global internet traffic. At peak, it serves 250 million concurrent streams ‚Äî each stream adapting in real-time to network conditions, each segment served from servers 20ms away from the viewer. The architecture behind this solves problems that span encoding theory, distributed systems, network optimization, and machine learning.</p>
<h2>Requirements</h2>
<p><strong>Functional:</strong></p>
<ul>
<li>Users upload videos (creators) or watch licensed content</li>
<li>Video plays within 2 seconds of click (fast start)</li>
<li>Adaptive quality (auto-adjusts 360p ‚Üí 1080p ‚Üí 4K based on bandwidth)</li>
<li>Resume playback across devices</li>
<li>Offline download</li>
<li>Personalized recommendations</li>
</ul>
<p><strong>Non-Functional:</strong></p>
<ul>
<li>250M DAU, 500M total subscribers</li>
<li>15B streaming hours per month</li>
<li>P99 start time &#x3C; 2 seconds</li>
<li>Rebuffering rate &#x3C; 0.5% (buffer = video pauses to load = terrible UX)</li>
<li>Upload: encode 1 hour of 4K video in &#x3C; 30 minutes</li>
</ul>
<h2>The Video Pipeline: Upload to Playback</h2>
<p>Before going deep on any individual component, it helps to see the entire pipeline end to end. Video streaming has two distinct data flows that operate independently: the upload and transcoding path (happens once per video), and the playback path (happens billions of times per video). Understanding this asymmetry ‚Äî encode once, serve forever ‚Äî is the core architectural principle that makes the economics work.</p>
<pre><code>Creator Upload                    Viewer Playback
      ‚îÇ                                  ‚îÇ
      ‚ñº                                  ‚ñº
Raw video file              ‚îå‚îÄ‚îÄ‚îÄ CDN Edge (&#x3C; 20ms away)
 (H.264 4K 50GB)            ‚îÇ         ‚îÇ
      ‚îÇ                     ‚îÇ         ‚ñº
      ‚ñº                     ‚îÇ    Adaptive Bitrate
 Ingest Service ‚îÄ‚îÄ‚ñ∫ S3      ‚îÇ    Streaming Player
      ‚îÇ           (raw)     ‚îÇ    (requests appropriate
      ‚ñº                     ‚îÇ     quality segment)
Transcoding Farm ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
 (parallel workers)         ‚îÇ
  - 360p @ 500kbps          ‚îî‚îÄ‚îÄ‚îÄ Origin servers (S3 + CDN origin)
  - 720p @ 2.5Mbps
  - 1080p @ 5Mbps
  - 4K @ 25Mbps
  - Audio: AAC, Dolby Atmos
      ‚îÇ
      ‚ñº
 HLS/DASH manifest + segments
 stored in S3 ‚Üí CDN
</code></pre>
<p>The right side of this diagram ‚Äî CDN edge serving segments directly to viewers ‚Äî is where nearly all viewer requests land. The origin servers (S3) exist primarily to fill the CDN cache on first access, not to serve ongoing traffic. Getting to a 98% CDN cache hit rate is what makes serving 250 million concurrent streams economically feasible.</p>
<h2>Video Transcoding Pipeline</h2>
<p>Transcoding is the most compute-intensive operation in the system, and it must be done before a video can be watched by anyone. A one-hour 4K video encodes into five quality levels plus multiple audio tracks, and each encoding job takes significant CPU time. The solution is to parallelize across many workers simultaneously rather than processing each quality level sequentially on one machine.</p>
<pre><code class="language-java">// Distributed transcoding: break 1 video into many parallel jobs
@Service
public class VideoTranscodingOrchestrator {

    @Autowired
    private SqsClient sqs;

    @Autowired
    private S3Client s3;

    public void transcodeVideo(VideoUploadedEvent event) {
        String videoId = event.getVideoId();
        String rawS3Key = event.getRawS3Key();

        // Define all required output formats
        List&#x3C;TranscodeJob> jobs = List.of(
            new TranscodeJob(videoId, "360p",  500_000,  640,  360),
            new TranscodeJob(videoId, "480p",  1_000_000, 854, 480),
            new TranscodeJob(videoId, "720p",  2_500_000, 1280, 720),
            new TranscodeJob(videoId, "1080p", 5_000_000, 1920, 1080),
            new TranscodeJob(videoId, "4k",   25_000_000, 3840, 2160),
            // Audio tracks
            new TranscodeJob(videoId, "audio_aac",   128_000, 0, 0),
            new TranscodeJob(videoId, "audio_dolby", 640_000, 0, 0)
        );

        // Send all jobs to SQS ‚Äî transcoding workers pick up in parallel
        jobs.forEach(job -> {
            sqs.sendMessage(SendMessageRequest.builder()
                .queueUrl(TRANSCODING_QUEUE_URL)
                .messageBody(serialize(job))
                .messageGroupId(videoId)  // FIFO queue: group by video
                .build());
        });

        // Track completion ‚Äî send HLS manifest when all renditions done
        jobTracker.initializeVideoTracking(videoId, jobs.size());
    }
}

// FFmpeg-based transcoding worker (runs on EC2 Spot instances ‚Äî 90% cheaper)
@Component
public class TranscodingWorker {

    @SqsListener(queueNames = "${transcoding.queue.url}")
    public void processJob(TranscodeJob job) {
        // Download raw video chunk from S3
        File rawFile = downloadFromS3(job.getRawS3Key());

        // FFmpeg transcoding
        ProcessBuilder pb = new ProcessBuilder(
            "ffmpeg",
            "-i", rawFile.getAbsolutePath(),
            "-vf", "scale=" + job.getWidth() + ":" + job.getHeight(),
            "-c:v", "libx264",
            "-b:v", job.getBitrate() + "k",
            "-preset", "slow",          // Better compression, slower
            "-crf", "23",               // Quality level
            "-c:a", "aac",
            "-b:a", "128k",
            // HLS segmentation: 6-second segments
            "-f", "hls",
            "-hls_time", "6",
            "-hls_playlist_type", "vod",
            "-hls_segment_filename", outputDir + "/%d.ts",
            outputDir + "/playlist.m3u8"
        );

        executeAndWait(pb);

        // Upload segments and playlist to S3
        uploadSegmentsToS3(outputDir, job.getVideoId(), job.getRendition());

        // Notify orchestrator this rendition is complete
        jobTracker.markComplete(job.getVideoId(), job.getRendition());
    }
}
</code></pre>
<p>Running transcoding workers on EC2 Spot instances rather than on-demand instances is a 90% cost reduction ‚Äî and transcoding is a perfect fit for Spot because jobs are interruptible: if a Spot instance is reclaimed, SQS retains the unacknowledged job and another worker picks it up. The <code>-preset slow</code> flag in FFmpeg deliberately trades encoding speed for file size: a slower preset produces a smaller output file at the same quality level, which reduces S3 storage costs and CDN bandwidth for every view of the video.</p>
<h2>HLS Manifest: Adaptive Bitrate Streaming</h2>
<p>The output of transcoding is not just video files ‚Äî it is a manifest structure that the player uses to navigate between quality levels on the fly. HLS (HTTP Live Streaming) organizes this into a two-level hierarchy: a master playlist that lists all available quality levels, and per-rendition playlists that list individual 6-second segments.</p>
<pre><code># Master playlist (index.m3u8) ‚Äî served to player
#EXTM3U
#EXT-X-VERSION:3

# Renditions ordered by bandwidth
#EXT-X-STREAM-INF:BANDWIDTH=500000,RESOLUTION=640x360
https://cdn.example.com/videos/abc123/360p/playlist.m3u8

#EXT-X-STREAM-INF:BANDWIDTH=2500000,RESOLUTION=1280x720
https://cdn.example.com/videos/abc123/720p/playlist.m3u8

#EXT-X-STREAM-INF:BANDWIDTH=5000000,RESOLUTION=1920x1080
https://cdn.example.com/videos/abc123/1080p/playlist.m3u8

#EXT-X-STREAM-INF:BANDWIDTH=25000000,RESOLUTION=3840x2160
https://cdn.example.com/videos/abc123/4k/playlist.m3u8

# 360p playlist (360p/playlist.m3u8)
#EXTM3U
#EXT-X-VERSION:3
#EXT-X-TARGETDURATION:6
#EXT-X-PLAYLIST-TYPE:VOD

#EXTINF:6.0,
0.ts
#EXTINF:6.0,
1.ts
...
#EXTINF:4.2,
600.ts
#EXT-X-ENDLIST
</code></pre>
<p>The HLS player measures download speed. If 1080p segment downloads in 2 seconds but should play in 6 seconds ‚Üí ample buffer ‚Üí stay at 1080p. If 1080p segment downloads in 7 seconds but should play in 6 seconds ‚Üí falling behind ‚Üí switch to 720p. This adaptation happens every 6 seconds.</p>
<p>The 6-second segment duration is a deliberate engineering choice that balances two competing concerns: shorter segments (2-3 seconds) make quality switching more responsive but increase the number of HTTP requests and add overhead; longer segments (10-15 seconds) reduce HTTP overhead but make quality adaptation sluggish. Six seconds is the sweet spot that most major streaming platforms have converged on.</p>
<h2>CDN Architecture</h2>
<p>The CDN is where the "encode once, serve forever" principle pays off. A video segment is a fixed bytes-on-disk file that never changes after encoding, which means it can be cached indefinitely at every CDN edge node worldwide. Once a segment is cached at an edge location, every subsequent request for that segment from that region costs nothing in compute and delivers in under 5ms.</p>
<pre><code>Global CDN (CloudFront / Akamai):
  - 400+ edge locations worldwide
  - Video segments cached at edge: 95% cache hit rate
  - Cache hit: 5ms latency
  - Cache miss ‚Üí origin S3: 50-200ms (once)

Cache key: /videos/{videoId}/{rendition}/{segment}.ts

Cache strategy:
  - Segments: immutable (never change), cache forever (Cache-Control: max-age=31536000)
  - Manifests: short TTL during encoding (5 seconds), long TTL after (1 hour)
  - Playlists: short TTL during live streams, long after VOD is complete
  - Thumbnails: 1 day TTL

CDN hit rate target: 98%
  - Popular videos: 100% (100M+ viewers = every edge has it cached)
  - Long-tail (indie content): 60-80% (less popular = cache misses)
  - First 1% views: always miss (fill the cache)
</code></pre>
<p>The distinction between <code>max-age=31536000</code> (one year) for segments versus a 1-hour TTL for manifests reflects their different mutability: segments are truly immutable and can be cached forever, but manifests may need to be updated if metadata changes or if the encoding pipeline re-processes the video. If you served stale segments it would not matter, but a stale manifest pointing to old segments could break playback.</p>
<h2>Video Start Time Optimization</h2>
<p>Start time is the user experience metric that matters most before playback begins. Think of it like launching a rocket: every millisecond of delay before the first frame is felt more acutely than buffering that happens 10 minutes into a show. The optimizations below stack on top of each other, each shaving latency from a different part of the startup sequence.</p>
<pre><code>P99 &#x3C; 2 seconds requires:

1. Fast DNS resolution: Anycast DNS ‚Üí nearest CDN PoP
   Impact: 50ms ‚Üí 5ms

2. Video manifest prefetch: when user hovers on thumbnail,
   fetch and cache the master playlist
   Impact: 200ms ‚Üí 0ms (already cached)

3. Pre-buffered thumbnails: before user clicks play,
   stream still frames (video preview on hover)
   Impact: perceived start time ‚Üí "instant"

4. Start with lowest quality: request 360p first segment immediately,
   then upgrade quality
   Impact: first frame in 200ms (360p = 375KB), then upgrade

5. ABR algorithm: start low, ramp up quickly
   - First segment: lowest quality (fast start)
   - Next 3 segments: upgrade if bandwidth allows
   - After 30 seconds: stable quality

Start time breakdown:
  DNS lookup: 5ms
  TCP + TLS handshake: 30ms
  Manifest download: 20ms
  First segment download: 100ms (360p, 375KB, 30Mbps)
  Total: 155ms ‚Üí well under 2 second target
</code></pre>
<p>The manifest prefetch on hover is worth highlighting as one of the highest-leverage optimizations available: users typically hover over a thumbnail for 200-500ms before clicking, and during that window you can silently fetch the manifest and cache it locally. By the time the user clicks play, the manifest round-trip is already done and the player can immediately request the first segment.</p>
<h2>Recommendation System Architecture</h2>
<p>Once a user finishes watching, the next challenge is showing them something they will want to watch next. The recommendation system is a two-stage pipeline designed around a fundamental tradeoff: generating the best possible 5 recommendations from 200 million titles is too slow to do in real time, but doing it entirely in batch misses real-time signals like what the user just watched 10 minutes ago.</p>
<pre><code>Data pipeline:
  User events (play, pause, skip, complete, rate) ‚Üí Kafka ‚Üí Feature store

Two-stage recommendation:
  Stage 1: Candidate generation (fast, broad)
    - Collaborative filtering: "users like you watched X"
    - Content-based: "you watched thrillers ‚Üí more thrillers"
    - Trending: popular content in your region
    - Output: 500-1000 candidates per user

  Stage 2: Ranking (slower, precise)
    - Neural network ranking model
    - Features: user history, content metadata, time of day, device
    - Output: top 20 ranked recommendations

Serving:
  - Pre-compute recommendations nightly (batch job)
  - Store in Redis: user_id ‚Üí [ranked video IDs]
  - Real-time adjustments: boost content just watched by friends
  - A/B test: 20% users get new ranking model vs baseline
</code></pre>
<p>The two-stage architecture is a pattern you will see throughout large-scale ML systems: a fast, approximate retrieval stage narrows millions of candidates to hundreds, then a slow, precise ranking stage applies a sophisticated model to that smaller set. Trying to run the precise ranking model against all 200 million titles for every user every second would require thousands of GPUs; running it against 500-1000 pre-filtered candidates is economical.</p>
<h2>Storage Cost Optimization</h2>
<p>Video storage is where the economics of streaming become genuinely challenging. The numbers below reveal why a naive approach of storing all quality levels for all content is financially unsustainable, and why tiered storage and newer codecs are not just nice-to-haves but existential requirements for the business model.</p>
<pre><code>Content stored at multiple quality levels:
  720p (avg 2.5 Mbps, 2hr movie = 2.25GB) √ó 200M titles = 450 PB
  + 4 more quality levels √ó 200M titles = 2.25 EB

That's impossibly expensive. Netflix's actual strategy:

1. S3 Intelligent-Tiering:
   - Hot tier (frequently accessed): $0.023/GB/month
   - Cold tier (infrequently accessed, 90+ days): $0.0025/GB/month
   - Long tail content automatically moves to cold tier

2. Per-title quality decision:
   - High-demand titles: store all quality levels at edge
   - Low-demand titles: store only in origin (380p, 720p)
   - Zero-demand titles: transcode on-demand

3. HEVC (H.265) encoding for new content:
   - 40-50% smaller files vs H.264 at same quality
   - Requires hardware decoder (all modern devices support it)

4. AV1 codec (open, royalty-free):
   - 30% smaller than HEVC
   - Google/Netflix co-developed
   - Rolling deployment on supported devices
</code></pre>
<p>The combination of S3 Intelligent-Tiering and per-title quality decisions means the system is essentially self-optimizing on cost: popular titles migrate to hot-tier storage and get all quality levels at edge CDN, while obscure titles sit in cold storage and are only fetched on the rare occasion someone watches them. A 90% reduction in storage price for cold-tier content changes the math on keeping long-tail content in the catalog at all.</p>
<p>The lessons from video streaming architecture apply broadly: pre-computation beats real-time computation (transcode once, serve forever), geographic distribution beats raw speed (CDN edge beats fast origin), and partial results beat waiting (low quality first, upgrade while playing). These principles show up in recommendation systems, image delivery, web performance ‚Äî anywhere latency matters more than perfection.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">üìö</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">System Design Interview ‚Äî Alex Xu</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Best Seller</span></div><p class="text-xs text-gray-600">Step-by-step guide to ace system design interviews with real-world examples.</p></div><a href="https://amzn.to/3TqsPRp" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Grokking System Design on Educative</span></div><p class="text-xs text-gray-600">Interactive course teaching system design with visual diagrams and practice problems.</p></div><a href="https://www.educative.io/courses/grokking-the-system-design-interview" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Designing Data-Intensive Applications</span></div><p class="text-xs text-gray-600">Martin Kleppmann&#x27;s book is essential reading for any system design role.</p></div><a href="https://amzn.to/3RyKzOA" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=System%20Design%3A%20Video%20Streaming%20Platform%20at%20Netflix%20Scale&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fsystem-design-video-streaming%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fsystem-design-video-streaming%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#requirements" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Requirements</a></li><li class=""><a href="#the-video-pipeline-upload-to-playback" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">The Video Pipeline: Upload to Playback</a></li><li class=""><a href="#video-transcoding-pipeline" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Video Transcoding Pipeline</a></li><li class=""><a href="#hls-manifest-adaptive-bitrate-streaming" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">HLS Manifest: Adaptive Bitrate Streaming</a></li><li class=""><a href="#cdn-architecture" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">CDN Architecture</a></li><li class=""><a href="#video-start-time-optimization" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Video Start Time Optimization</a></li><li class=""><a href="#recommendation-system-architecture" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Recommendation System Architecture</a></li><li class=""><a href="#storage-cost-optimization" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Storage Cost Optimization</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/observability-opentelemetry-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Building Production Observability with OpenTelemetry and Grafana Stack</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why ‚Äî by exploring system state through metrics, traces, and logs without needing to know in advance‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jul 3, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>6 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->observability</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->opentelemetry</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->prometheus</span></div></article></a><a href="/blog/event-sourcing-cqrs-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Event Sourcing and CQRS in Production: Beyond the Theory</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory ‚Äî store events instead of state, derive state by replaying events ‚Äî is sou‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 23, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->event sourcing</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->cqrs</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->system design</span></div></article></a><a href="/blog/grpc-vs-rest-vs-graphql/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">gRPC vs REST vs GraphQL: Choosing the Right API Protocol</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 18, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->grpc</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->rest</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->graphql</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">‚Üê Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS ‚Äî by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">¬© <!-- -->2026<!-- --> CodeSprintPro ¬∑ Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js ¬∑ TailwindCSS ¬∑ Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"System Design: Video Streaming Platform at Netflix Scale","description":"Design a video streaming platform handling 250M users and 15% of global internet traffic. Covers video transcoding pipeline, CDN architecture, adaptive bitrate streaming, and recommendation systems.","date":"2025-03-19","category":"System Design","tags":["system design","video streaming","cdn","aws","distributed systems","hls"],"featured":false,"affiliateSection":"system-design-courses","slug":"system-design-video-streaming","readingTime":"11 min read","excerpt":"Netflix accounts for 15% of global internet traffic. At peak, it serves 250 million concurrent streams ‚Äî each stream adapting in real-time to network conditions, each segment served from servers 20ms away from the viewer‚Ä¶","contentHtml":"\u003cp\u003eNetflix accounts for 15% of global internet traffic. At peak, it serves 250 million concurrent streams ‚Äî each stream adapting in real-time to network conditions, each segment served from servers 20ms away from the viewer. The architecture behind this solves problems that span encoding theory, distributed systems, network optimization, and machine learning.\u003c/p\u003e\n\u003ch2\u003eRequirements\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eFunctional:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUsers upload videos (creators) or watch licensed content\u003c/li\u003e\n\u003cli\u003eVideo plays within 2 seconds of click (fast start)\u003c/li\u003e\n\u003cli\u003eAdaptive quality (auto-adjusts 360p ‚Üí 1080p ‚Üí 4K based on bandwidth)\u003c/li\u003e\n\u003cli\u003eResume playback across devices\u003c/li\u003e\n\u003cli\u003eOffline download\u003c/li\u003e\n\u003cli\u003ePersonalized recommendations\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNon-Functional:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e250M DAU, 500M total subscribers\u003c/li\u003e\n\u003cli\u003e15B streaming hours per month\u003c/li\u003e\n\u003cli\u003eP99 start time \u0026#x3C; 2 seconds\u003c/li\u003e\n\u003cli\u003eRebuffering rate \u0026#x3C; 0.5% (buffer = video pauses to load = terrible UX)\u003c/li\u003e\n\u003cli\u003eUpload: encode 1 hour of 4K video in \u0026#x3C; 30 minutes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Video Pipeline: Upload to Playback\u003c/h2\u003e\n\u003cp\u003eBefore going deep on any individual component, it helps to see the entire pipeline end to end. Video streaming has two distinct data flows that operate independently: the upload and transcoding path (happens once per video), and the playback path (happens billions of times per video). Understanding this asymmetry ‚Äî encode once, serve forever ‚Äî is the core architectural principle that makes the economics work.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCreator Upload                    Viewer Playback\n      ‚îÇ                                  ‚îÇ\n      ‚ñº                                  ‚ñº\nRaw video file              ‚îå‚îÄ‚îÄ‚îÄ CDN Edge (\u0026#x3C; 20ms away)\n (H.264 4K 50GB)            ‚îÇ         ‚îÇ\n      ‚îÇ                     ‚îÇ         ‚ñº\n      ‚ñº                     ‚îÇ    Adaptive Bitrate\n Ingest Service ‚îÄ‚îÄ‚ñ∫ S3      ‚îÇ    Streaming Player\n      ‚îÇ           (raw)     ‚îÇ    (requests appropriate\n      ‚ñº                     ‚îÇ     quality segment)\nTranscoding Farm ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ\n (parallel workers)         ‚îÇ\n  - 360p @ 500kbps          ‚îî‚îÄ‚îÄ‚îÄ Origin servers (S3 + CDN origin)\n  - 720p @ 2.5Mbps\n  - 1080p @ 5Mbps\n  - 4K @ 25Mbps\n  - Audio: AAC, Dolby Atmos\n      ‚îÇ\n      ‚ñº\n HLS/DASH manifest + segments\n stored in S3 ‚Üí CDN\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe right side of this diagram ‚Äî CDN edge serving segments directly to viewers ‚Äî is where nearly all viewer requests land. The origin servers (S3) exist primarily to fill the CDN cache on first access, not to serve ongoing traffic. Getting to a 98% CDN cache hit rate is what makes serving 250 million concurrent streams economically feasible.\u003c/p\u003e\n\u003ch2\u003eVideo Transcoding Pipeline\u003c/h2\u003e\n\u003cp\u003eTranscoding is the most compute-intensive operation in the system, and it must be done before a video can be watched by anyone. A one-hour 4K video encodes into five quality levels plus multiple audio tracks, and each encoding job takes significant CPU time. The solution is to parallelize across many workers simultaneously rather than processing each quality level sequentially on one machine.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Distributed transcoding: break 1 video into many parallel jobs\n@Service\npublic class VideoTranscodingOrchestrator {\n\n    @Autowired\n    private SqsClient sqs;\n\n    @Autowired\n    private S3Client s3;\n\n    public void transcodeVideo(VideoUploadedEvent event) {\n        String videoId = event.getVideoId();\n        String rawS3Key = event.getRawS3Key();\n\n        // Define all required output formats\n        List\u0026#x3C;TranscodeJob\u003e jobs = List.of(\n            new TranscodeJob(videoId, \"360p\",  500_000,  640,  360),\n            new TranscodeJob(videoId, \"480p\",  1_000_000, 854, 480),\n            new TranscodeJob(videoId, \"720p\",  2_500_000, 1280, 720),\n            new TranscodeJob(videoId, \"1080p\", 5_000_000, 1920, 1080),\n            new TranscodeJob(videoId, \"4k\",   25_000_000, 3840, 2160),\n            // Audio tracks\n            new TranscodeJob(videoId, \"audio_aac\",   128_000, 0, 0),\n            new TranscodeJob(videoId, \"audio_dolby\", 640_000, 0, 0)\n        );\n\n        // Send all jobs to SQS ‚Äî transcoding workers pick up in parallel\n        jobs.forEach(job -\u003e {\n            sqs.sendMessage(SendMessageRequest.builder()\n                .queueUrl(TRANSCODING_QUEUE_URL)\n                .messageBody(serialize(job))\n                .messageGroupId(videoId)  // FIFO queue: group by video\n                .build());\n        });\n\n        // Track completion ‚Äî send HLS manifest when all renditions done\n        jobTracker.initializeVideoTracking(videoId, jobs.size());\n    }\n}\n\n// FFmpeg-based transcoding worker (runs on EC2 Spot instances ‚Äî 90% cheaper)\n@Component\npublic class TranscodingWorker {\n\n    @SqsListener(queueNames = \"${transcoding.queue.url}\")\n    public void processJob(TranscodeJob job) {\n        // Download raw video chunk from S3\n        File rawFile = downloadFromS3(job.getRawS3Key());\n\n        // FFmpeg transcoding\n        ProcessBuilder pb = new ProcessBuilder(\n            \"ffmpeg\",\n            \"-i\", rawFile.getAbsolutePath(),\n            \"-vf\", \"scale=\" + job.getWidth() + \":\" + job.getHeight(),\n            \"-c:v\", \"libx264\",\n            \"-b:v\", job.getBitrate() + \"k\",\n            \"-preset\", \"slow\",          // Better compression, slower\n            \"-crf\", \"23\",               // Quality level\n            \"-c:a\", \"aac\",\n            \"-b:a\", \"128k\",\n            // HLS segmentation: 6-second segments\n            \"-f\", \"hls\",\n            \"-hls_time\", \"6\",\n            \"-hls_playlist_type\", \"vod\",\n            \"-hls_segment_filename\", outputDir + \"/%d.ts\",\n            outputDir + \"/playlist.m3u8\"\n        );\n\n        executeAndWait(pb);\n\n        // Upload segments and playlist to S3\n        uploadSegmentsToS3(outputDir, job.getVideoId(), job.getRendition());\n\n        // Notify orchestrator this rendition is complete\n        jobTracker.markComplete(job.getVideoId(), job.getRendition());\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRunning transcoding workers on EC2 Spot instances rather than on-demand instances is a 90% cost reduction ‚Äî and transcoding is a perfect fit for Spot because jobs are interruptible: if a Spot instance is reclaimed, SQS retains the unacknowledged job and another worker picks it up. The \u003ccode\u003e-preset slow\u003c/code\u003e flag in FFmpeg deliberately trades encoding speed for file size: a slower preset produces a smaller output file at the same quality level, which reduces S3 storage costs and CDN bandwidth for every view of the video.\u003c/p\u003e\n\u003ch2\u003eHLS Manifest: Adaptive Bitrate Streaming\u003c/h2\u003e\n\u003cp\u003eThe output of transcoding is not just video files ‚Äî it is a manifest structure that the player uses to navigate between quality levels on the fly. HLS (HTTP Live Streaming) organizes this into a two-level hierarchy: a master playlist that lists all available quality levels, and per-rendition playlists that list individual 6-second segments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Master playlist (index.m3u8) ‚Äî served to player\n#EXTM3U\n#EXT-X-VERSION:3\n\n# Renditions ordered by bandwidth\n#EXT-X-STREAM-INF:BANDWIDTH=500000,RESOLUTION=640x360\nhttps://cdn.example.com/videos/abc123/360p/playlist.m3u8\n\n#EXT-X-STREAM-INF:BANDWIDTH=2500000,RESOLUTION=1280x720\nhttps://cdn.example.com/videos/abc123/720p/playlist.m3u8\n\n#EXT-X-STREAM-INF:BANDWIDTH=5000000,RESOLUTION=1920x1080\nhttps://cdn.example.com/videos/abc123/1080p/playlist.m3u8\n\n#EXT-X-STREAM-INF:BANDWIDTH=25000000,RESOLUTION=3840x2160\nhttps://cdn.example.com/videos/abc123/4k/playlist.m3u8\n\n# 360p playlist (360p/playlist.m3u8)\n#EXTM3U\n#EXT-X-VERSION:3\n#EXT-X-TARGETDURATION:6\n#EXT-X-PLAYLIST-TYPE:VOD\n\n#EXTINF:6.0,\n0.ts\n#EXTINF:6.0,\n1.ts\n...\n#EXTINF:4.2,\n600.ts\n#EXT-X-ENDLIST\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe HLS player measures download speed. If 1080p segment downloads in 2 seconds but should play in 6 seconds ‚Üí ample buffer ‚Üí stay at 1080p. If 1080p segment downloads in 7 seconds but should play in 6 seconds ‚Üí falling behind ‚Üí switch to 720p. This adaptation happens every 6 seconds.\u003c/p\u003e\n\u003cp\u003eThe 6-second segment duration is a deliberate engineering choice that balances two competing concerns: shorter segments (2-3 seconds) make quality switching more responsive but increase the number of HTTP requests and add overhead; longer segments (10-15 seconds) reduce HTTP overhead but make quality adaptation sluggish. Six seconds is the sweet spot that most major streaming platforms have converged on.\u003c/p\u003e\n\u003ch2\u003eCDN Architecture\u003c/h2\u003e\n\u003cp\u003eThe CDN is where the \"encode once, serve forever\" principle pays off. A video segment is a fixed bytes-on-disk file that never changes after encoding, which means it can be cached indefinitely at every CDN edge node worldwide. Once a segment is cached at an edge location, every subsequent request for that segment from that region costs nothing in compute and delivers in under 5ms.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eGlobal CDN (CloudFront / Akamai):\n  - 400+ edge locations worldwide\n  - Video segments cached at edge: 95% cache hit rate\n  - Cache hit: 5ms latency\n  - Cache miss ‚Üí origin S3: 50-200ms (once)\n\nCache key: /videos/{videoId}/{rendition}/{segment}.ts\n\nCache strategy:\n  - Segments: immutable (never change), cache forever (Cache-Control: max-age=31536000)\n  - Manifests: short TTL during encoding (5 seconds), long TTL after (1 hour)\n  - Playlists: short TTL during live streams, long after VOD is complete\n  - Thumbnails: 1 day TTL\n\nCDN hit rate target: 98%\n  - Popular videos: 100% (100M+ viewers = every edge has it cached)\n  - Long-tail (indie content): 60-80% (less popular = cache misses)\n  - First 1% views: always miss (fill the cache)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe distinction between \u003ccode\u003emax-age=31536000\u003c/code\u003e (one year) for segments versus a 1-hour TTL for manifests reflects their different mutability: segments are truly immutable and can be cached forever, but manifests may need to be updated if metadata changes or if the encoding pipeline re-processes the video. If you served stale segments it would not matter, but a stale manifest pointing to old segments could break playback.\u003c/p\u003e\n\u003ch2\u003eVideo Start Time Optimization\u003c/h2\u003e\n\u003cp\u003eStart time is the user experience metric that matters most before playback begins. Think of it like launching a rocket: every millisecond of delay before the first frame is felt more acutely than buffering that happens 10 minutes into a show. The optimizations below stack on top of each other, each shaving latency from a different part of the startup sequence.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eP99 \u0026#x3C; 2 seconds requires:\n\n1. Fast DNS resolution: Anycast DNS ‚Üí nearest CDN PoP\n   Impact: 50ms ‚Üí 5ms\n\n2. Video manifest prefetch: when user hovers on thumbnail,\n   fetch and cache the master playlist\n   Impact: 200ms ‚Üí 0ms (already cached)\n\n3. Pre-buffered thumbnails: before user clicks play,\n   stream still frames (video preview on hover)\n   Impact: perceived start time ‚Üí \"instant\"\n\n4. Start with lowest quality: request 360p first segment immediately,\n   then upgrade quality\n   Impact: first frame in 200ms (360p = 375KB), then upgrade\n\n5. ABR algorithm: start low, ramp up quickly\n   - First segment: lowest quality (fast start)\n   - Next 3 segments: upgrade if bandwidth allows\n   - After 30 seconds: stable quality\n\nStart time breakdown:\n  DNS lookup: 5ms\n  TCP + TLS handshake: 30ms\n  Manifest download: 20ms\n  First segment download: 100ms (360p, 375KB, 30Mbps)\n  Total: 155ms ‚Üí well under 2 second target\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe manifest prefetch on hover is worth highlighting as one of the highest-leverage optimizations available: users typically hover over a thumbnail for 200-500ms before clicking, and during that window you can silently fetch the manifest and cache it locally. By the time the user clicks play, the manifest round-trip is already done and the player can immediately request the first segment.\u003c/p\u003e\n\u003ch2\u003eRecommendation System Architecture\u003c/h2\u003e\n\u003cp\u003eOnce a user finishes watching, the next challenge is showing them something they will want to watch next. The recommendation system is a two-stage pipeline designed around a fundamental tradeoff: generating the best possible 5 recommendations from 200 million titles is too slow to do in real time, but doing it entirely in batch misses real-time signals like what the user just watched 10 minutes ago.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eData pipeline:\n  User events (play, pause, skip, complete, rate) ‚Üí Kafka ‚Üí Feature store\n\nTwo-stage recommendation:\n  Stage 1: Candidate generation (fast, broad)\n    - Collaborative filtering: \"users like you watched X\"\n    - Content-based: \"you watched thrillers ‚Üí more thrillers\"\n    - Trending: popular content in your region\n    - Output: 500-1000 candidates per user\n\n  Stage 2: Ranking (slower, precise)\n    - Neural network ranking model\n    - Features: user history, content metadata, time of day, device\n    - Output: top 20 ranked recommendations\n\nServing:\n  - Pre-compute recommendations nightly (batch job)\n  - Store in Redis: user_id ‚Üí [ranked video IDs]\n  - Real-time adjustments: boost content just watched by friends\n  - A/B test: 20% users get new ranking model vs baseline\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe two-stage architecture is a pattern you will see throughout large-scale ML systems: a fast, approximate retrieval stage narrows millions of candidates to hundreds, then a slow, precise ranking stage applies a sophisticated model to that smaller set. Trying to run the precise ranking model against all 200 million titles for every user every second would require thousands of GPUs; running it against 500-1000 pre-filtered candidates is economical.\u003c/p\u003e\n\u003ch2\u003eStorage Cost Optimization\u003c/h2\u003e\n\u003cp\u003eVideo storage is where the economics of streaming become genuinely challenging. The numbers below reveal why a naive approach of storing all quality levels for all content is financially unsustainable, and why tiered storage and newer codecs are not just nice-to-haves but existential requirements for the business model.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eContent stored at multiple quality levels:\n  720p (avg 2.5 Mbps, 2hr movie = 2.25GB) √ó 200M titles = 450 PB\n  + 4 more quality levels √ó 200M titles = 2.25 EB\n\nThat's impossibly expensive. Netflix's actual strategy:\n\n1. S3 Intelligent-Tiering:\n   - Hot tier (frequently accessed): $0.023/GB/month\n   - Cold tier (infrequently accessed, 90+ days): $0.0025/GB/month\n   - Long tail content automatically moves to cold tier\n\n2. Per-title quality decision:\n   - High-demand titles: store all quality levels at edge\n   - Low-demand titles: store only in origin (380p, 720p)\n   - Zero-demand titles: transcode on-demand\n\n3. HEVC (H.265) encoding for new content:\n   - 40-50% smaller files vs H.264 at same quality\n   - Requires hardware decoder (all modern devices support it)\n\n4. AV1 codec (open, royalty-free):\n   - 30% smaller than HEVC\n   - Google/Netflix co-developed\n   - Rolling deployment on supported devices\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe combination of S3 Intelligent-Tiering and per-title quality decisions means the system is essentially self-optimizing on cost: popular titles migrate to hot-tier storage and get all quality levels at edge CDN, while obscure titles sit in cold storage and are only fetched on the rare occasion someone watches them. A 90% reduction in storage price for cold-tier content changes the math on keeping long-tail content in the catalog at all.\u003c/p\u003e\n\u003cp\u003eThe lessons from video streaming architecture apply broadly: pre-computation beats real-time computation (transcode once, serve forever), geographic distribution beats raw speed (CDN edge beats fast origin), and partial results beat waiting (low quality first, upgrade while playing). These principles show up in recommendation systems, image delivery, web performance ‚Äî anywhere latency matters more than perfection.\u003c/p\u003e\n","tableOfContents":[{"id":"requirements","text":"Requirements","level":2},{"id":"the-video-pipeline-upload-to-playback","text":"The Video Pipeline: Upload to Playback","level":2},{"id":"video-transcoding-pipeline","text":"Video Transcoding Pipeline","level":2},{"id":"hls-manifest-adaptive-bitrate-streaming","text":"HLS Manifest: Adaptive Bitrate Streaming","level":2},{"id":"cdn-architecture","text":"CDN Architecture","level":2},{"id":"video-start-time-optimization","text":"Video Start Time Optimization","level":2},{"id":"recommendation-system-architecture","text":"Recommendation System Architecture","level":2},{"id":"storage-cost-optimization","text":"Storage Cost Optimization","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why ‚Äî by exploring system state through metrics, traces, and logs without needing to know in advance‚Ä¶"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory ‚Äî store events instead of state, derive state by replaying events ‚Äî is sou‚Ä¶"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to t‚Ä¶"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"system-design-video-streaming"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>