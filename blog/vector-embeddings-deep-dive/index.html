<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Vector Embeddings: The Foundation of Modern AI Applications<!-- --> | CodeSprintPro</title><meta name="description" content="Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/vector-embeddings-deep-dive/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Vector Embeddings: The Foundation of Modern AI Applications" data-next-head=""/><meta property="og:description" content="Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/vector-embeddings-deep-dive/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-03-11" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="AI/ML" data-next-head=""/><meta property="article:tag" content="ai" data-next-head=""/><meta property="article:tag" content="embeddings" data-next-head=""/><meta property="article:tag" content="vector database" data-next-head=""/><meta property="article:tag" content="semantic search" data-next-head=""/><meta property="article:tag" content="rag" data-next-head=""/><meta property="article:tag" content="pgvector" data-next-head=""/><meta property="article:tag" content="pinecone" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Vector Embeddings: The Foundation of Modern AI Applications" data-next-head=""/><meta name="twitter:description" content="Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Vector Embeddings: The Foundation of Modern AI Applications","description":"Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-03-11","dateModified":"2025-03-11","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/vector-embeddings-deep-dive/"},"keywords":"ai, embeddings, vector database, semantic search, rag, pgvector, pinecone","articleSection":"AI/ML"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Vector Embeddings: The Foundation of Modern AI Applications</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">AI/ML</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Vector Embeddings: The Foundation of Modern AI Applications</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>¬∑</span><span>March 11, 2025</span><span>¬∑</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->ai</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->embeddings</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->vector database</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->semantic search</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->rag</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->pgvector</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->pinecone</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Every modern AI application ‚Äî semantic search, RAG, recommendations, duplicate detection ‚Äî is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantically similar items are geometrically close. This geometric property is what powers "find me articles about machine learning" returning results that match the concept, not the exact words.</p>
<h2>What Are Embeddings?</h2>
<p>Before diving into code, here is the intuition: imagine placing every piece of text you have ever written onto a map, where texts that mean similar things end up in the same neighborhood. "Machine learning" and "neural networks" would be a few blocks apart; "machine learning" and "cooking recipes" would be in different cities. An embedding model learns to draw this map by training on billions of examples of which texts are semantically related. The diagram below shows concretely what changes between traditional keyword search and this map-based approach.</p>
<pre><code>Traditional keyword search:
  Query: "machine learning"
  Matches: "machine learning", "Machine Learning", "MACHINE LEARNING"
  Does NOT match: "neural networks", "deep learning", "AI algorithms"

Embedding-based semantic search:
  Query: "machine learning"
  Matches: "machine learning", "neural networks", "deep learning",
           "AI algorithms", "gradient descent", "model training"
  Based on: meaning, not string matching

How:
  "machine learning" ‚Üí [0.23, -0.45, 0.12, 0.89, ...] (1536 dimensions)
  "neural networks"  ‚Üí [0.21, -0.43, 0.15, 0.87, ...] (similar vector!)
  "cooking recipes"  ‚Üí [-0.67, 0.34, -0.89, 0.12, ...] (very different vector)

  Cosine similarity("machine learning", "neural networks") = 0.94 (very similar)
  Cosine similarity("machine learning", "cooking recipes") = 0.12 (unrelated)
</code></pre>
<p>Each dimension captures some aspect of meaning ‚Äî not interpretable individually, but the ensemble encodes semantic relationships learned from billions of text examples.</p>
<h2>Generating Embeddings</h2>
<p>Now you can see how to generate these vectors in practice. The <code>cosine_similarity</code> function at the bottom is the mathematical equivalent of measuring how close two points are on your semantic map ‚Äî a score near 1.0 means the texts are neighbors, near 0 means they are unrelated.</p>
<pre><code class="language-python"># OpenAI text-embedding-3-small (best cost/performance ratio)
from openai import OpenAI
import numpy as np

client = OpenAI()

def embed(text: str) -> list[float]:
    """Generate embedding for a single text."""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text,
        encoding_format="float"
    )
    return response.data[0].embedding

def embed_batch(texts: list[str]) -> list[list[float]]:
    """Generate embeddings for multiple texts in one API call."""
    # Batch up to 2048 texts per request
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return [item.embedding for item in sorted(response.data, key=lambda x: x.index)]

# Dimensions: text-embedding-3-small = 1536, text-embedding-3-large = 3072
# Cost: $0.02 per million tokens (very cheap)

# Cosine similarity
def cosine_similarity(a: list[float], b: list[float]) -> float:
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# Example:
e1 = embed("Java virtual threads")
e2 = embed("Java lightweight concurrency")
e3 = embed("Python web scraping")

print(cosine_similarity(e1, e2))  # ~0.92 (highly similar)
print(cosine_similarity(e1, e3))  # ~0.45 (unrelated)
</code></pre>
<p>The results show exactly why embeddings are useful: "Java virtual threads" and "Java lightweight concurrency" score 0.92 even though they share only the word "Java" ‚Äî the model understood they describe the same concept. A score of 0.45 for the Python web scraping comparison confirms they are semantically unrelated despite being in the same programming domain.</p>
<h2>Vector Databases: Efficient Similarity Search</h2>
<p>Brute-force cosine similarity over 1M vectors takes seconds. Vector databases use ANN (Approximate Nearest Neighbor) algorithms to answer "find 10 most similar vectors" in milliseconds.</p>
<p>The tradeoff here is precision for speed: an ANN index might miss the single most-similar vector 1-2% of the time, but it answers in under 10ms instead of several seconds. For search applications, that tradeoff is almost always worth it ‚Äî your users will not notice the occasional near-miss, but they will absolutely notice a slow response.</p>
<h3>Option 1: pgvector (PostgreSQL Extension)</h3>
<p>Best for: existing PostgreSQL users, &#x3C; 10M vectors, want SQL joins with similarity search.</p>
<p>If you are already running PostgreSQL, pgvector is a compelling choice because it lets you combine similarity search with all the power of SQL ‚Äî you can filter by category, join against user tables, or run aggregations alongside your vector queries without managing a separate service.</p>
<pre><code class="language-sql">-- Install pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Table: blog articles with embeddings
CREATE TABLE articles (
    id          BIGSERIAL PRIMARY KEY,
    slug        VARCHAR(200) UNIQUE NOT NULL,
    title       TEXT NOT NULL,
    content     TEXT NOT NULL,
    category    VARCHAR(50),
    embedding   vector(1536),    -- OpenAI text-embedding-3-small dimensions
    created_at  TIMESTAMPTZ DEFAULT NOW()
);

-- IVFFlat index: fast approximate search
-- lists = sqrt(total_rows) is a good starting point
CREATE INDEX idx_articles_embedding ON articles
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

-- HNSW index (PostgreSQL 15+): better recall, more memory
CREATE INDEX idx_articles_embedding_hnsw ON articles
  USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);

-- Semantic search query
SELECT
    id,
    title,
    category,
    1 - (embedding &#x3C;=> '[0.23, -0.45, 0.12, ...]'::vector) AS similarity
FROM articles
WHERE category = 'System Design'           -- Filter BEFORE similarity (important!)
ORDER BY embedding &#x3C;=> '[0.23, ...]'::vector  -- &#x3C;=> = cosine distance
LIMIT 10;

-- Operators:
-- &#x3C;=>  cosine distance (use for text ‚Äî normalized vectors)
-- &#x3C;->  Euclidean distance
-- &#x3C;#>  negative inner product (use if vectors are normalized)
</code></pre>
<p>Notice the comment "Filter BEFORE similarity (important!)". Applying your <code>WHERE category = 'System Design'</code> filter before the vector search dramatically reduces the number of vectors the index needs to scan, giving you both faster queries and more relevant results.</p>
<pre><code class="language-python"># Python + pgvector
import psycopg2
import numpy as np

def search_similar_articles(
    query: str,
    category: str | None = None,
    limit: int = 5
) -> list[dict]:
    query_embedding = embed(query)

    sql = """
        SELECT id, title, category, slug,
               1 - (embedding &#x3C;=> %s::vector) AS similarity
        FROM articles
        WHERE 1=1
    """
    params = [str(query_embedding)]

    if category:
        sql += " AND category = %s"
        params.append(category)

    sql += " ORDER BY embedding &#x3C;=> %s::vector LIMIT %s"
    params.extend([str(query_embedding), limit])

    with psycopg2.connect(DATABASE_URL) as conn:
        with conn.cursor() as cur:
            cur.execute(sql, params)
            return [
                {"id": r[0], "title": r[1], "category": r[2],
                 "slug": r[3], "similarity": r[4]}
                for r in cur.fetchall()
            ]
</code></pre>
<h3>Option 2: Pinecone (Managed Vector Database)</h3>
<p>Best for: > 10M vectors, need managed scaling, serverless pricing.</p>
<p>When your vector count grows into the tens of millions or you need zero-ops infrastructure, Pinecone removes the burden of tuning index parameters, managing shards, and handling node failures. The code below shows the same semantic search capability as pgvector, but backed by a fully managed service.</p>
<pre><code class="language-python">from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key="your-api-key")

# Create index
pc.create_index(
    name="articles",
    dimension=1536,
    metric="cosine",
    spec=ServerlessSpec(cloud="aws", region="us-east-1")
)

index = pc.Index("articles")

# Upsert vectors with metadata
def index_article(article: dict):
    embedding = embed(article["title"] + "\n\n" + article["content"])
    index.upsert(vectors=[{
        "id": article["slug"],
        "values": embedding,
        "metadata": {
            "title": article["title"],
            "category": article["category"],
            "created_at": article["created_at"]
        }
    }])

# Query with metadata filtering
def search(query: str, category: str | None = None, top_k: int = 5):
    query_embedding = embed(query)

    filter_dict = {}
    if category:
        filter_dict["category"] = {"$eq": category}

    results = index.query(
        vector=query_embedding,
        top_k=top_k,
        filter=filter_dict if filter_dict else None,
        include_metadata=True
    )

    return [
        {
            "id": match.id,
            "score": match.score,
            **match.metadata
        }
        for match in results.matches
    ]
</code></pre>
<h2>Hybrid Search: Combining Keyword and Semantic</h2>
<p>Pure semantic search misses exact keyword matches. Pure keyword search misses semantic matches. Hybrid search combines both.</p>
<p>Consider a user searching for a specific API endpoint like <code>POST /api/v2/users/reset-password</code>. Pure semantic search might return conceptually related content about authentication but miss this exact path. Pure keyword search finds the path but misses related documentation. Reciprocal Rank Fusion (RRF) below solves this by merging the two ranked lists into a single score that rewards items that rank well in both.</p>
<pre><code class="language-python"># Reciprocal Rank Fusion (RRF) ‚Äî combine two result lists
def hybrid_search(query: str, k: int = 5) -> list[dict]:
    # 1. Semantic search via vector similarity
    semantic_results = search_similar_articles(query, limit=20)

    # 2. Keyword search via PostgreSQL full-text search
    keyword_results = keyword_search(query, limit=20)

    # 3. Merge using RRF: score = sum(1 / (rank + 60))
    scores: dict[str, float] = {}
    RRF_K = 60

    for rank, result in enumerate(semantic_results):
        scores[result["slug"]] = scores.get(result["slug"], 0) + 1 / (rank + RRF_K)

    for rank, result in enumerate(keyword_results):
        scores[result["slug"]] = scores.get(result["slug"], 0) + 1 / (rank + RRF_K)

    # Sort by combined score
    all_slugs = {r["slug"]: r for r in semantic_results + keyword_results}
    return sorted(
        [all_slugs[slug] for slug in scores],
        key=lambda x: scores[x["slug"]],
        reverse=True
    )[:k]
</code></pre>
<p>The constant <code>RRF_K = 60</code> dampens the influence of rank position ‚Äî an item ranked 1st gets score <code>1/61 ‚âà 0.016</code>, while an item ranked 61st gets <code>1/121 ‚âà 0.008</code>. This prevents a very high-ranked result in one list from dominating the combined score and means results that appear in both lists consistently float to the top.</p>
<h2>Embeddings for RAG (Retrieval-Augmented Generation)</h2>
<p>With embeddings and similarity search in place, you have all the pieces needed to build a complete RAG pipeline. The four steps below ‚Äî embed the question, retrieve matching chunks, assemble context, and answer with an LLM ‚Äî are the core loop that powers every document Q&#x26;A system. Notice that the LLM is explicitly instructed to only use the retrieved context and to cite sources, which is what prevents hallucination.</p>
<pre><code class="language-python"># Complete RAG pipeline: question ‚Üí retrieve context ‚Üí answer with LLM
from anthropic import Anthropic

anthropic_client = Anthropic()

def rag_answer(question: str) -> str:
    # Step 1: Embed the question
    question_embedding = embed(question)

    # Step 2: Retrieve relevant chunks from knowledge base
    relevant_chunks = search_similar_articles(
        question,
        limit=5  # Top 5 most relevant articles/chunks
    )

    # Step 3: Build context from retrieved chunks
    context = "\n\n---\n\n".join([
        f"Title: {chunk['title']}\n{chunk['content']}"
        for chunk in relevant_chunks
    ])

    # Step 4: Answer with LLM using retrieved context
    response = anthropic_client.messages.create(
        model="claude-opus-4-6",
        max_tokens=1024,
        system="""You are a technical assistant. Answer questions using ONLY
        the provided context. If the answer isn't in the context, say so.
        Cite the specific articles you're drawing from.""",
        messages=[{
            "role": "user",
            "content": f"Context:\n{context}\n\nQuestion: {question}"
        }]
    )

    return response.content[0].text
</code></pre>
<h2>Chunking Strategy: Critical for RAG Quality</h2>
<p>How you split documents before embedding them is arguably more important than which embedding model you choose. A chunk that cuts a sentence in half, or that lumps together five unrelated paragraphs, produces an embedding that represents nothing clearly ‚Äî and no similarity search can recover useful signal from a bad embedding.</p>
<pre><code class="language-python"># Bad: chunk by fixed character count (breaks mid-sentence)
def bad_chunking(text: str, chunk_size: int = 1000) -> list[str]:
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

# Good: chunk by semantic units (paragraphs, sections)
def good_chunking(markdown_text: str, max_chunk_size: int = 800) -> list[str]:
    chunks = []
    current_chunk = []
    current_size = 0

    for paragraph in markdown_text.split("\n\n"):
        paragraph = paragraph.strip()
        if not paragraph:
            continue

        if current_size + len(paragraph) > max_chunk_size and current_chunk:
            chunks.append("\n\n".join(current_chunk))
            current_chunk = [paragraph]
            current_size = len(paragraph)
        else:
            current_chunk.append(paragraph)
            current_size += len(paragraph)

    if current_chunk:
        chunks.append("\n\n".join(current_chunk))

    return chunks

# Best: sliding window with overlap (maintains context at boundaries)
def sliding_window_chunks(text: str, chunk_size: int = 800, overlap: int = 200) -> list[str]:
    words = text.split()
    chunks = []
    i = 0
    while i &#x3C; len(words):
        chunk_words = words[i:i + chunk_size]
        chunks.append(" ".join(chunk_words))
        i += chunk_size - overlap  # Overlap keeps context between chunks
    return chunks
</code></pre>
<p>The <code>overlap</code> parameter in the sliding window approach is doing critical work: it ensures that a sentence split across the boundary between chunk N and chunk N+1 appears fully in at least one of them. Without overlap, every boundary is a potential context loss point that will silently degrade your retrieval quality.</p>
<h2>Dimensionality Reduction for Visualization</h2>
<p>Once you have a collection of embeddings, visualizing them is one of the fastest ways to build intuition about your data and validate that your embedding model is working correctly. If the visualization shows random scatter with no clustering by category, that is a signal your embeddings are not capturing the semantic distinctions you care about.</p>
<pre><code class="language-python"># Visualize your embedding space to understand clustering
import plotly.express as px
from sklearn.manifold import TSNE

# Generate embeddings for a set of articles
articles = fetch_articles()
embeddings = embed_batch([a["title"] for a in articles])

# Reduce 1536D ‚Üí 2D for visualization (t-SNE)
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
coords_2d = tsne.fit_transform(np.array(embeddings))

# Plot
df = pd.DataFrame({
    "x": coords_2d[:, 0],
    "y": coords_2d[:, 1],
    "title": [a["title"] for a in articles],
    "category": [a["category"] for a in articles]
})

fig = px.scatter(df, x="x", y="y", color="category",
                 hover_data=["title"], title="Article Embedding Space")
fig.show()
# You'll see: System Design articles cluster together, Java articles cluster,
# AI/ML articles cluster ‚Äî semantic proximity is visible
</code></pre>
<p>The insight that unlocks vector embeddings: you're not searching text anymore ‚Äî you're searching a semantic space where proximity equals meaning. Once you build intuition for what's close vs far in embedding space, you'll see applications everywhere: deduplication, content recommendations, anomaly detection, and anything that requires "find similar things." The math is straightforward; the power comes from applying it to the right problems.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">üìö</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Building LLM Apps with LangChain ‚Äî Udemy</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Hot</span></div><p class="text-xs text-gray-600">Build RAG systems, agents, and LLM-powered apps with Python and LangChain.</p></div><a href="https://www.udemy.com/course/langchain/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Hands-On Large Language Models</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">New</span></div><p class="text-xs text-gray-600">Practical guide to training, fine-tuning, and deploying LLMs.</p></div><a href="https://amzn.to/3Vpd8h5" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">AI Engineering by Chip Huyen</span></div><p class="text-xs text-gray-600">Building intelligent systems with foundation models ‚Äî from retrieval to agents.</p></div><a href="https://amzn.to/3Vrd1Rd" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Vector%20Embeddings%3A%20The%20Foundation%20of%20Modern%20AI%20Applications&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fvector-embeddings-deep-dive%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fvector-embeddings-deep-dive%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#what-are-embeddings" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">What Are Embeddings?</a></li><li class=""><a href="#generating-embeddings" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Generating Embeddings</a></li><li class=""><a href="#vector-databases-efficient-similarity-search" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Vector Databases: Efficient Similarity Search</a></li><li class="ml-4"><a href="#option-1-pgvector-postgresql-extension" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Option 1: pgvector (PostgreSQL Extension)</a></li><li class="ml-4"><a href="#option-2-pinecone-managed-vector-database" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Option 2: Pinecone (Managed Vector Database)</a></li><li class=""><a href="#hybrid-search-combining-keyword-and-semantic" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Hybrid Search: Combining Keyword and Semantic</a></li><li class=""><a href="#embeddings-for-rag-retrieval-augmented-generation" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Embeddings for RAG (Retrieval-Augmented Generation)</a></li><li class=""><a href="#chunking-strategy-critical-for-rag-quality" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Chunking Strategy: Critical for RAG Quality</a></li><li class=""><a href="#dimensionality-reduction-for-visualization" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Dimensionality Reduction for Visualization</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/fine-tuning-llms/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Fine-Tuning LLMs: When to Fine-Tune, When to Prompt</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning ‚Äî fo‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 27, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->llm</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->fine-tuning</span></div></article></a><a href="/blog/llm-agents-tool-use/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Building AI Agents with Tool Use: From Chatbot to Autonomous Agent</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 23, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->agents</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->claude</span></div></article></a><a href="/blog/prompt-engineering-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Prompt Engineering: Advanced Techniques for Production LLMs</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Most prompt engineering tutorials stop at &quot;be specific and provide context.&quot; That&#x27;s necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-gra‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Feb 26, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->llm</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->prompt engineering</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">‚Üê Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS ‚Äî by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">¬© <!-- -->2026<!-- --> CodeSprintPro ¬∑ Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js ¬∑ TailwindCSS ¬∑ Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Vector Embeddings: The Foundation of Modern AI Applications","description":"Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.","date":"2025-03-11","category":"AI/ML","tags":["ai","embeddings","vector database","semantic search","rag","pgvector","pinecone"],"featured":false,"affiliateSection":"ai-ml-books","slug":"vector-embeddings-deep-dive","readingTime":"11 min read","excerpt":"Every modern AI application ‚Äî semantic search, RAG, recommendations, duplicate detection ‚Äî is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantic‚Ä¶","contentHtml":"\u003cp\u003eEvery modern AI application ‚Äî semantic search, RAG, recommendations, duplicate detection ‚Äî is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semantically similar items are geometrically close. This geometric property is what powers \"find me articles about machine learning\" returning results that match the concept, not the exact words.\u003c/p\u003e\n\u003ch2\u003eWhat Are Embeddings?\u003c/h2\u003e\n\u003cp\u003eBefore diving into code, here is the intuition: imagine placing every piece of text you have ever written onto a map, where texts that mean similar things end up in the same neighborhood. \"Machine learning\" and \"neural networks\" would be a few blocks apart; \"machine learning\" and \"cooking recipes\" would be in different cities. An embedding model learns to draw this map by training on billions of examples of which texts are semantically related. The diagram below shows concretely what changes between traditional keyword search and this map-based approach.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTraditional keyword search:\n  Query: \"machine learning\"\n  Matches: \"machine learning\", \"Machine Learning\", \"MACHINE LEARNING\"\n  Does NOT match: \"neural networks\", \"deep learning\", \"AI algorithms\"\n\nEmbedding-based semantic search:\n  Query: \"machine learning\"\n  Matches: \"machine learning\", \"neural networks\", \"deep learning\",\n           \"AI algorithms\", \"gradient descent\", \"model training\"\n  Based on: meaning, not string matching\n\nHow:\n  \"machine learning\" ‚Üí [0.23, -0.45, 0.12, 0.89, ...] (1536 dimensions)\n  \"neural networks\"  ‚Üí [0.21, -0.43, 0.15, 0.87, ...] (similar vector!)\n  \"cooking recipes\"  ‚Üí [-0.67, 0.34, -0.89, 0.12, ...] (very different vector)\n\n  Cosine similarity(\"machine learning\", \"neural networks\") = 0.94 (very similar)\n  Cosine similarity(\"machine learning\", \"cooking recipes\") = 0.12 (unrelated)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEach dimension captures some aspect of meaning ‚Äî not interpretable individually, but the ensemble encodes semantic relationships learned from billions of text examples.\u003c/p\u003e\n\u003ch2\u003eGenerating Embeddings\u003c/h2\u003e\n\u003cp\u003eNow you can see how to generate these vectors in practice. The \u003ccode\u003ecosine_similarity\u003c/code\u003e function at the bottom is the mathematical equivalent of measuring how close two points are on your semantic map ‚Äî a score near 1.0 means the texts are neighbors, near 0 means they are unrelated.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# OpenAI text-embedding-3-small (best cost/performance ratio)\nfrom openai import OpenAI\nimport numpy as np\n\nclient = OpenAI()\n\ndef embed(text: str) -\u003e list[float]:\n    \"\"\"Generate embedding for a single text.\"\"\"\n    response = client.embeddings.create(\n        model=\"text-embedding-3-small\",\n        input=text,\n        encoding_format=\"float\"\n    )\n    return response.data[0].embedding\n\ndef embed_batch(texts: list[str]) -\u003e list[list[float]]:\n    \"\"\"Generate embeddings for multiple texts in one API call.\"\"\"\n    # Batch up to 2048 texts per request\n    response = client.embeddings.create(\n        model=\"text-embedding-3-small\",\n        input=texts\n    )\n    return [item.embedding for item in sorted(response.data, key=lambda x: x.index)]\n\n# Dimensions: text-embedding-3-small = 1536, text-embedding-3-large = 3072\n# Cost: $0.02 per million tokens (very cheap)\n\n# Cosine similarity\ndef cosine_similarity(a: list[float], b: list[float]) -\u003e float:\n    a, b = np.array(a), np.array(b)\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Example:\ne1 = embed(\"Java virtual threads\")\ne2 = embed(\"Java lightweight concurrency\")\ne3 = embed(\"Python web scraping\")\n\nprint(cosine_similarity(e1, e2))  # ~0.92 (highly similar)\nprint(cosine_similarity(e1, e3))  # ~0.45 (unrelated)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe results show exactly why embeddings are useful: \"Java virtual threads\" and \"Java lightweight concurrency\" score 0.92 even though they share only the word \"Java\" ‚Äî the model understood they describe the same concept. A score of 0.45 for the Python web scraping comparison confirms they are semantically unrelated despite being in the same programming domain.\u003c/p\u003e\n\u003ch2\u003eVector Databases: Efficient Similarity Search\u003c/h2\u003e\n\u003cp\u003eBrute-force cosine similarity over 1M vectors takes seconds. Vector databases use ANN (Approximate Nearest Neighbor) algorithms to answer \"find 10 most similar vectors\" in milliseconds.\u003c/p\u003e\n\u003cp\u003eThe tradeoff here is precision for speed: an ANN index might miss the single most-similar vector 1-2% of the time, but it answers in under 10ms instead of several seconds. For search applications, that tradeoff is almost always worth it ‚Äî your users will not notice the occasional near-miss, but they will absolutely notice a slow response.\u003c/p\u003e\n\u003ch3\u003eOption 1: pgvector (PostgreSQL Extension)\u003c/h3\u003e\n\u003cp\u003eBest for: existing PostgreSQL users, \u0026#x3C; 10M vectors, want SQL joins with similarity search.\u003c/p\u003e\n\u003cp\u003eIf you are already running PostgreSQL, pgvector is a compelling choice because it lets you combine similarity search with all the power of SQL ‚Äî you can filter by category, join against user tables, or run aggregations alongside your vector queries without managing a separate service.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003e-- Install pgvector extension\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Table: blog articles with embeddings\nCREATE TABLE articles (\n    id          BIGSERIAL PRIMARY KEY,\n    slug        VARCHAR(200) UNIQUE NOT NULL,\n    title       TEXT NOT NULL,\n    content     TEXT NOT NULL,\n    category    VARCHAR(50),\n    embedding   vector(1536),    -- OpenAI text-embedding-3-small dimensions\n    created_at  TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- IVFFlat index: fast approximate search\n-- lists = sqrt(total_rows) is a good starting point\nCREATE INDEX idx_articles_embedding ON articles\n  USING ivfflat (embedding vector_cosine_ops)\n  WITH (lists = 100);\n\n-- HNSW index (PostgreSQL 15+): better recall, more memory\nCREATE INDEX idx_articles_embedding_hnsw ON articles\n  USING hnsw (embedding vector_cosine_ops)\n  WITH (m = 16, ef_construction = 64);\n\n-- Semantic search query\nSELECT\n    id,\n    title,\n    category,\n    1 - (embedding \u0026#x3C;=\u003e '[0.23, -0.45, 0.12, ...]'::vector) AS similarity\nFROM articles\nWHERE category = 'System Design'           -- Filter BEFORE similarity (important!)\nORDER BY embedding \u0026#x3C;=\u003e '[0.23, ...]'::vector  -- \u0026#x3C;=\u003e = cosine distance\nLIMIT 10;\n\n-- Operators:\n-- \u0026#x3C;=\u003e  cosine distance (use for text ‚Äî normalized vectors)\n-- \u0026#x3C;-\u003e  Euclidean distance\n-- \u0026#x3C;#\u003e  negative inner product (use if vectors are normalized)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNotice the comment \"Filter BEFORE similarity (important!)\". Applying your \u003ccode\u003eWHERE category = 'System Design'\u003c/code\u003e filter before the vector search dramatically reduces the number of vectors the index needs to scan, giving you both faster queries and more relevant results.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Python + pgvector\nimport psycopg2\nimport numpy as np\n\ndef search_similar_articles(\n    query: str,\n    category: str | None = None,\n    limit: int = 5\n) -\u003e list[dict]:\n    query_embedding = embed(query)\n\n    sql = \"\"\"\n        SELECT id, title, category, slug,\n               1 - (embedding \u0026#x3C;=\u003e %s::vector) AS similarity\n        FROM articles\n        WHERE 1=1\n    \"\"\"\n    params = [str(query_embedding)]\n\n    if category:\n        sql += \" AND category = %s\"\n        params.append(category)\n\n    sql += \" ORDER BY embedding \u0026#x3C;=\u003e %s::vector LIMIT %s\"\n    params.extend([str(query_embedding), limit])\n\n    with psycopg2.connect(DATABASE_URL) as conn:\n        with conn.cursor() as cur:\n            cur.execute(sql, params)\n            return [\n                {\"id\": r[0], \"title\": r[1], \"category\": r[2],\n                 \"slug\": r[3], \"similarity\": r[4]}\n                for r in cur.fetchall()\n            ]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eOption 2: Pinecone (Managed Vector Database)\u003c/h3\u003e\n\u003cp\u003eBest for: \u003e 10M vectors, need managed scaling, serverless pricing.\u003c/p\u003e\n\u003cp\u003eWhen your vector count grows into the tens of millions or you need zero-ops infrastructure, Pinecone removes the burden of tuning index parameters, managing shards, and handling node failures. The code below shows the same semantic search capability as pgvector, but backed by a fully managed service.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom pinecone import Pinecone, ServerlessSpec\n\npc = Pinecone(api_key=\"your-api-key\")\n\n# Create index\npc.create_index(\n    name=\"articles\",\n    dimension=1536,\n    metric=\"cosine\",\n    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n)\n\nindex = pc.Index(\"articles\")\n\n# Upsert vectors with metadata\ndef index_article(article: dict):\n    embedding = embed(article[\"title\"] + \"\\n\\n\" + article[\"content\"])\n    index.upsert(vectors=[{\n        \"id\": article[\"slug\"],\n        \"values\": embedding,\n        \"metadata\": {\n            \"title\": article[\"title\"],\n            \"category\": article[\"category\"],\n            \"created_at\": article[\"created_at\"]\n        }\n    }])\n\n# Query with metadata filtering\ndef search(query: str, category: str | None = None, top_k: int = 5):\n    query_embedding = embed(query)\n\n    filter_dict = {}\n    if category:\n        filter_dict[\"category\"] = {\"$eq\": category}\n\n    results = index.query(\n        vector=query_embedding,\n        top_k=top_k,\n        filter=filter_dict if filter_dict else None,\n        include_metadata=True\n    )\n\n    return [\n        {\n            \"id\": match.id,\n            \"score\": match.score,\n            **match.metadata\n        }\n        for match in results.matches\n    ]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eHybrid Search: Combining Keyword and Semantic\u003c/h2\u003e\n\u003cp\u003ePure semantic search misses exact keyword matches. Pure keyword search misses semantic matches. Hybrid search combines both.\u003c/p\u003e\n\u003cp\u003eConsider a user searching for a specific API endpoint like \u003ccode\u003ePOST /api/v2/users/reset-password\u003c/code\u003e. Pure semantic search might return conceptually related content about authentication but miss this exact path. Pure keyword search finds the path but misses related documentation. Reciprocal Rank Fusion (RRF) below solves this by merging the two ranked lists into a single score that rewards items that rank well in both.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Reciprocal Rank Fusion (RRF) ‚Äî combine two result lists\ndef hybrid_search(query: str, k: int = 5) -\u003e list[dict]:\n    # 1. Semantic search via vector similarity\n    semantic_results = search_similar_articles(query, limit=20)\n\n    # 2. Keyword search via PostgreSQL full-text search\n    keyword_results = keyword_search(query, limit=20)\n\n    # 3. Merge using RRF: score = sum(1 / (rank + 60))\n    scores: dict[str, float] = {}\n    RRF_K = 60\n\n    for rank, result in enumerate(semantic_results):\n        scores[result[\"slug\"]] = scores.get(result[\"slug\"], 0) + 1 / (rank + RRF_K)\n\n    for rank, result in enumerate(keyword_results):\n        scores[result[\"slug\"]] = scores.get(result[\"slug\"], 0) + 1 / (rank + RRF_K)\n\n    # Sort by combined score\n    all_slugs = {r[\"slug\"]: r for r in semantic_results + keyword_results}\n    return sorted(\n        [all_slugs[slug] for slug in scores],\n        key=lambda x: scores[x[\"slug\"]],\n        reverse=True\n    )[:k]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe constant \u003ccode\u003eRRF_K = 60\u003c/code\u003e dampens the influence of rank position ‚Äî an item ranked 1st gets score \u003ccode\u003e1/61 ‚âà 0.016\u003c/code\u003e, while an item ranked 61st gets \u003ccode\u003e1/121 ‚âà 0.008\u003c/code\u003e. This prevents a very high-ranked result in one list from dominating the combined score and means results that appear in both lists consistently float to the top.\u003c/p\u003e\n\u003ch2\u003eEmbeddings for RAG (Retrieval-Augmented Generation)\u003c/h2\u003e\n\u003cp\u003eWith embeddings and similarity search in place, you have all the pieces needed to build a complete RAG pipeline. The four steps below ‚Äî embed the question, retrieve matching chunks, assemble context, and answer with an LLM ‚Äî are the core loop that powers every document Q\u0026#x26;A system. Notice that the LLM is explicitly instructed to only use the retrieved context and to cite sources, which is what prevents hallucination.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Complete RAG pipeline: question ‚Üí retrieve context ‚Üí answer with LLM\nfrom anthropic import Anthropic\n\nanthropic_client = Anthropic()\n\ndef rag_answer(question: str) -\u003e str:\n    # Step 1: Embed the question\n    question_embedding = embed(question)\n\n    # Step 2: Retrieve relevant chunks from knowledge base\n    relevant_chunks = search_similar_articles(\n        question,\n        limit=5  # Top 5 most relevant articles/chunks\n    )\n\n    # Step 3: Build context from retrieved chunks\n    context = \"\\n\\n---\\n\\n\".join([\n        f\"Title: {chunk['title']}\\n{chunk['content']}\"\n        for chunk in relevant_chunks\n    ])\n\n    # Step 4: Answer with LLM using retrieved context\n    response = anthropic_client.messages.create(\n        model=\"claude-opus-4-6\",\n        max_tokens=1024,\n        system=\"\"\"You are a technical assistant. Answer questions using ONLY\n        the provided context. If the answer isn't in the context, say so.\n        Cite the specific articles you're drawing from.\"\"\",\n        messages=[{\n            \"role\": \"user\",\n            \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n        }]\n    )\n\n    return response.content[0].text\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eChunking Strategy: Critical for RAG Quality\u003c/h2\u003e\n\u003cp\u003eHow you split documents before embedding them is arguably more important than which embedding model you choose. A chunk that cuts a sentence in half, or that lumps together five unrelated paragraphs, produces an embedding that represents nothing clearly ‚Äî and no similarity search can recover useful signal from a bad embedding.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Bad: chunk by fixed character count (breaks mid-sentence)\ndef bad_chunking(text: str, chunk_size: int = 1000) -\u003e list[str]:\n    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n\n# Good: chunk by semantic units (paragraphs, sections)\ndef good_chunking(markdown_text: str, max_chunk_size: int = 800) -\u003e list[str]:\n    chunks = []\n    current_chunk = []\n    current_size = 0\n\n    for paragraph in markdown_text.split(\"\\n\\n\"):\n        paragraph = paragraph.strip()\n        if not paragraph:\n            continue\n\n        if current_size + len(paragraph) \u003e max_chunk_size and current_chunk:\n            chunks.append(\"\\n\\n\".join(current_chunk))\n            current_chunk = [paragraph]\n            current_size = len(paragraph)\n        else:\n            current_chunk.append(paragraph)\n            current_size += len(paragraph)\n\n    if current_chunk:\n        chunks.append(\"\\n\\n\".join(current_chunk))\n\n    return chunks\n\n# Best: sliding window with overlap (maintains context at boundaries)\ndef sliding_window_chunks(text: str, chunk_size: int = 800, overlap: int = 200) -\u003e list[str]:\n    words = text.split()\n    chunks = []\n    i = 0\n    while i \u0026#x3C; len(words):\n        chunk_words = words[i:i + chunk_size]\n        chunks.append(\" \".join(chunk_words))\n        i += chunk_size - overlap  # Overlap keeps context between chunks\n    return chunks\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eoverlap\u003c/code\u003e parameter in the sliding window approach is doing critical work: it ensures that a sentence split across the boundary between chunk N and chunk N+1 appears fully in at least one of them. Without overlap, every boundary is a potential context loss point that will silently degrade your retrieval quality.\u003c/p\u003e\n\u003ch2\u003eDimensionality Reduction for Visualization\u003c/h2\u003e\n\u003cp\u003eOnce you have a collection of embeddings, visualizing them is one of the fastest ways to build intuition about your data and validate that your embedding model is working correctly. If the visualization shows random scatter with no clustering by category, that is a signal your embeddings are not capturing the semantic distinctions you care about.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Visualize your embedding space to understand clustering\nimport plotly.express as px\nfrom sklearn.manifold import TSNE\n\n# Generate embeddings for a set of articles\narticles = fetch_articles()\nembeddings = embed_batch([a[\"title\"] for a in articles])\n\n# Reduce 1536D ‚Üí 2D for visualization (t-SNE)\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\ncoords_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot\ndf = pd.DataFrame({\n    \"x\": coords_2d[:, 0],\n    \"y\": coords_2d[:, 1],\n    \"title\": [a[\"title\"] for a in articles],\n    \"category\": [a[\"category\"] for a in articles]\n})\n\nfig = px.scatter(df, x=\"x\", y=\"y\", color=\"category\",\n                 hover_data=[\"title\"], title=\"Article Embedding Space\")\nfig.show()\n# You'll see: System Design articles cluster together, Java articles cluster,\n# AI/ML articles cluster ‚Äî semantic proximity is visible\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe insight that unlocks vector embeddings: you're not searching text anymore ‚Äî you're searching a semantic space where proximity equals meaning. Once you build intuition for what's close vs far in embedding space, you'll see applications everywhere: deduplication, content recommendations, anomaly detection, and anything that requires \"find similar things.\" The math is straightforward; the power comes from applying it to the right problems.\u003c/p\u003e\n","tableOfContents":[{"id":"what-are-embeddings","text":"What Are Embeddings?","level":2},{"id":"generating-embeddings","text":"Generating Embeddings","level":2},{"id":"vector-databases-efficient-similarity-search","text":"Vector Databases: Efficient Similarity Search","level":2},{"id":"option-1-pgvector-postgresql-extension","text":"Option 1: pgvector (PostgreSQL Extension)","level":3},{"id":"option-2-pinecone-managed-vector-database","text":"Option 2: Pinecone (Managed Vector Database)","level":3},{"id":"hybrid-search-combining-keyword-and-semantic","text":"Hybrid Search: Combining Keyword and Semantic","level":2},{"id":"embeddings-for-rag-retrieval-augmented-generation","text":"Embeddings for RAG (Retrieval-Augmented Generation)","level":2},{"id":"chunking-strategy-critical-for-rag-quality","text":"Chunking Strategy: Critical for RAG Quality","level":2},{"id":"dimensionality-reduction-for-visualization","text":"Dimensionality Reduction for Visualization","level":2}]},"relatedPosts":[{"title":"Fine-Tuning LLMs: When to Fine-Tune, When to Prompt","description":"Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.","date":"2025-03-27","category":"AI/ML","tags":["ai","llm","fine-tuning","lora","hugging face","openai","machine learning"],"featured":false,"affiliateSection":"ai-ml-books","slug":"fine-tuning-llms","readingTime":"10 min read","excerpt":"Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning ‚Äî fo‚Ä¶"},{"title":"Building AI Agents with Tool Use: From Chatbot to Autonomous Agent","description":"Build production AI agents using Claude's tool use API. Learn the agentic loop, error handling, multi-step reasoning, human-in-the-loop patterns, and how to build reliable autonomous systems.","date":"2025-03-23","category":"AI/ML","tags":["ai","agents","claude","tool use","llm","autonomous systems","python"],"featured":false,"affiliateSection":"ai-ml-books","slug":"llm-agents-tool-use","readingTime":"10 min read","excerpt":"A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatio‚Ä¶"},{"title":"Prompt Engineering: Advanced Techniques for Production LLMs","description":"Go beyond basic prompting. Learn chain-of-thought reasoning, few-shot examples, structured output, self-consistency, ReAct agents, and evaluation techniques for production LLM applications.","date":"2025-02-26","category":"AI/ML","tags":["ai","llm","prompt engineering","gpt","claude","production"],"featured":false,"affiliateSection":"ai-ml-books","slug":"prompt-engineering-production","readingTime":"11 min read","excerpt":"Most prompt engineering tutorials stop at \"be specific and provide context.\" That's necessary but not sufficient for production systems. This article covers the advanced techniques that separate demos from production-gra‚Ä¶"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"vector-embeddings-deep-dive"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>