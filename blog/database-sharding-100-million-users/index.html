<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Designing a Database Sharding Strategy for 100 Million Users<!-- --> | CodeSprintPro</title><meta name="description" content="A practical guide to horizontal sharding at scale: shard key selection, hot shard prevention, consistent hashing, cross-shard queries, and zero-downtime data migration with real fintech architecture examples." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/database-sharding-100-million-users/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Designing a Database Sharding Strategy for 100 Million Users" data-next-head=""/><meta property="og:description" content="A practical guide to horizontal sharding at scale: shard key selection, hot shard prevention, consistent hashing, cross-shard queries, and zero-downtime data migration with real fintech architecture examples." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/database-sharding-100-million-users/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-04-14" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="Databases" data-next-head=""/><meta property="article:tag" content="databases" data-next-head=""/><meta property="article:tag" content="sharding" data-next-head=""/><meta property="article:tag" content="postgresql" data-next-head=""/><meta property="article:tag" content="system design" data-next-head=""/><meta property="article:tag" content="distributed systems" data-next-head=""/><meta property="article:tag" content="scaling" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Designing a Database Sharding Strategy for 100 Million Users" data-next-head=""/><meta name="twitter:description" content="A practical guide to horizontal sharding at scale: shard key selection, hot shard prevention, consistent hashing, cross-shard queries, and zero-downtime data migration with real fintech architecture examples." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Designing a Database Sharding Strategy for 100 Million Users","description":"A practical guide to horizontal sharding at scale: shard key selection, hot shard prevention, consistent hashing, cross-shard queries, and zero-downtime data migration with real fintech architecture examples.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-04-14","dateModified":"2025-04-14","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/database-sharding-100-million-users/"},"keywords":"databases, sharding, postgresql, system design, distributed systems, scaling","articleSection":"Databases"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Designing a Database Sharding Strategy for 100 Million Users</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">Databases</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Designing a Database Sharding Strategy for 100 Million Users</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">A practical guide to horizontal sharding at scale: shard key selection, hot shard prevention, consistent hashing, cross-shard queries, and zero-downtime data migration with real fintech architecture examples.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>Â·</span><span>April 14, 2025</span><span>Â·</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>9 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->databases</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->sharding</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->postgresql</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->system design</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->distributed systems</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->scaling</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Vertical scaling has a ceiling. For most applications, that ceiling arrives somewhere between 1 million and 10 million users, depending on write patterns and data size. At 100 million users, the question is not whether to shard â€” it's how to shard without destroying query capabilities, operational sanity, and transactional guarantees.</p>
<p>This article is a complete playbook for database sharding at fintech scale.</p>
<h2>Horizontal vs Vertical Sharding</h2>
<p><strong>Vertical sharding</strong> (functional partitioning) splits tables across databases by domain: users database, orders database, payments database. Each service owns its database. This is what microservices architecture gives you naturally.</p>
<pre><code>Vertical Sharding:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Users DB          â”‚  â”‚   Orders DB          â”‚  â”‚   Payments DB       â”‚
â”‚   users table       â”‚  â”‚   orders table       â”‚  â”‚   payments table    â”‚
â”‚   profiles table    â”‚  â”‚   order_items table  â”‚  â”‚   ledger table      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p><strong>Horizontal sharding</strong> splits a single large table across multiple database instances by row. Each shard holds a subset of rows.</p>
<pre><code>Horizontal Sharding:
users table â†’ split by user_id range

Shard 0: user_id 0â€“24,999,999         (Shard DB 0)
Shard 1: user_id 25,000,000â€“49,999,999 (Shard DB 1)
Shard 2: user_id 50,000,000â€“74,999,999 (Shard DB 2)
Shard 3: user_id 75,000,000â€“99,999,999 (Shard DB 3)
</code></pre>
<p>Vertical sharding should always come first. It's operationally simpler, enables independent scaling per domain, and avoids distributed transactions within a service. Horizontal sharding is the next step when a single domain's write volume exceeds what one machine can handle.</p>
<h2>Shard Key Selection Strategy</h2>
<p>The shard key is the most consequential decision in your sharding design. Getting it wrong means data hotspots, expensive cross-shard joins, or re-sharding after launch.</p>
<p><strong>Rule 1: High cardinality.</strong> The shard key must have enough distinct values to distribute data evenly. <code>user_id</code> (UUID or integer) works. <code>country_code</code> does not â€” if 40% of your users are in the US, one shard gets 40% of the load.</p>
<p><strong>Rule 2: Even access distribution.</strong> The key should distribute both read and write load evenly. Timestamp-based keys (<code>created_at</code>) often create write hotspots â€” all new records hit the latest shard.</p>
<p><strong>Rule 3: Co-locate related data.</strong> Queries that need to be fast should touch one shard. For a payment system, sharding payments by <code>user_id</code> means all of a user's payment history is on one shard, enabling efficient account statements without cross-shard queries.</p>
<p><strong>Rule 4: Immutable.</strong> Changing the shard key value means moving the row to a different shard â€” an expensive operation. Use IDs that never change.</p>
<p>For a fintech platform at 100M users:</p>
<pre><code class="language-sql">-- Schema design: payments table
CREATE TABLE payments (
    payment_id      UUID DEFAULT gen_random_uuid(),
    user_id         BIGINT NOT NULL,          -- Shard key
    merchant_id     BIGINT NOT NULL,
    amount          DECIMAL(19,4) NOT NULL,
    currency        CHAR(3) NOT NULL,
    status          VARCHAR(20) NOT NULL,
    idempotency_key VARCHAR(255) UNIQUE,
    created_at      TIMESTAMPTZ DEFAULT NOW(),
    updated_at      TIMESTAMPTZ DEFAULT NOW(),
    PRIMARY KEY (user_id, payment_id)         -- Shard key first in PK
);

-- Shard key determines physical location
-- All rows for user_id 12345678 are on Shard (12345678 % 64)
</code></pre>
<h2>Consistent Hashing</h2>
<p>Naive hash-based sharding uses <code>shard_id = hash(user_id) % num_shards</code>. The problem: adding a shard changes the modulus, requiring almost all data to move.</p>
<p>Consistent hashing solves this with a ring:</p>
<pre><code>Consistent Hashing Ring (0 to 2^32):

              0 / 2^32
                  â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  Shard 0               Shard 1
  (0 - 2^30)        (2^30 - 2^31)
                â”‚
           Shard 2
        (2^31 - 3Â·2^30)
                â”‚
           Shard 3
      (3Â·2^30 - 2^32)
</code></pre>
<p>Each shard owns a range on the ring. A user's shard is determined by where <code>hash(user_id)</code> lands. When you add a fourth shard, only the users whose hash falls between the new shard's range boundaries need to move â€” roughly 1/N of data, not all of it.</p>
<p>Virtual nodes (vnodes) improve distribution: each physical shard has multiple positions on the ring (typically 150â€“300 vnodes). This smooths uneven distributions caused by non-uniform hash outputs.</p>
<h2>Hot Shard Problem</h2>
<p>Even with good key selection, certain shards can become disproportionately busy:</p>
<ul>
<li>A viral merchant has 10M transactions, all landing on Shard 4</li>
<li>A batch job processes all users in <code>user_id</code> range 0â€“1M sequentially</li>
<li>A celebrity user account is read millions of times per day</li>
</ul>
<p><strong>Detection:</strong> Monitor per-shard QPS, CPU, and I/O independently. A shard running at 80% CPU while others run at 20% is a hot shard.</p>
<p><strong>Mitigation strategies:</strong></p>
<ol>
<li>
<p><strong>Key-based hot shard splitting:</strong> Split the hot shard into two, re-hashing the subset. Requires migration.</p>
</li>
<li>
<p><strong>Read replicas for read-heavy hot shards:</strong> Add read replicas to the hot shard. Route reads there, writes to the primary.</p>
</li>
<li>
<p><strong>Application-layer caching for celebrity objects:</strong> Cache the hot user/merchant data in Redis. This solves read hotspots without re-sharding.</p>
</li>
<li>
<p><strong>Secondary shard key for compound hotness:</strong> If merchant_id causes hotspots, shard the <code>merchant_payments</code> aggregate table by <code>merchant_id</code> separately from the main <code>payments</code> table sharded by <code>user_id</code>.</p>
</li>
</ol>
<h2>Cross-Shard Query Challenges</h2>
<p>The most painful limitation of horizontal sharding: queries spanning multiple shards require scatter-gather.</p>
<pre><code>SELECT amount, currency, merchant_id
FROM payments
WHERE created_at BETWEEN '2025-01-01' AND '2025-01-31'
  AND status = 'completed'
ORDER BY created_at DESC
LIMIT 100;
</code></pre>
<p>This query has no <code>user_id</code> predicate, so it must run on all 64 shards and results must be merged. Approaches:</p>
<p><strong>1. Application-layer scatter-gather:</strong></p>
<pre><code class="language-java">List&#x3C;CompletableFuture&#x3C;List&#x3C;Payment>>> futures = shards.stream()
    .map(shard -> CompletableFuture.supplyAsync(
        () -> shard.query(sql, startDate, endDate), executor))
    .collect(toList());

List&#x3C;Payment> allResults = futures.stream()
    .flatMap(f -> f.join().stream())
    .sorted(Comparator.comparing(Payment::getCreatedAt).reversed())
    .limit(100)
    .collect(toList());
</code></pre>
<p>For N shards, you retrieve <code>N Ã— 100</code> rows and discard <code>(N-1) Ã— 100</code>. At 64 shards, you're fetching 6,400 rows to return 100.</p>
<p><strong>2. Denormalized query tables in a separate unsharded database:</strong>
For analytics and reporting queries, maintain a denormalized table in a single reporting database (or data warehouse) that aggregates across shards. ETL runs periodically (or via CDC) to populate it.</p>
<p><strong>3. Elasticsearch or ClickHouse as query layer:</strong>
Index payment data into Elasticsearch or ClickHouse for flexible querying without shard boundaries. The source of truth stays in sharded PostgreSQL; the query engine handles aggregation.</p>
<h2>Transaction Management Across Shards</h2>
<p>Distributed transactions across shards require either 2-Phase Commit (2PC) or Saga pattern. 2PC is slow and blocking; Saga is complex but resilient.</p>
<p>For a payment that debits <code>user_id=A</code> (Shard 12) and credits <code>user_id=B</code> (Shard 47), the Saga pattern:</p>
<pre><code>Saga: Cross-Shard Payment Transfer

Step 1: Debit user A on Shard 12
        â†’ Write debit record, set status=PENDING
        â†’ Publish event: MoneyDebited(txn_id, user_A, amount)

Step 2: Credit user B on Shard 47 (on event receipt)
        â†’ Write credit record
        â†’ Publish event: MoneyCredited(txn_id, user_B, amount)

Step 3: Confirm debit on Shard 12 (on event receipt)
        â†’ Set debit status=COMPLETED

Compensating transactions (on failure):
Step 2 fails â†’ Publish event: CreditFailed(txn_id)
Step 1 compensation â†’ Reverse debit on Shard 12, set status=REVERSED
</code></pre>
<p>The transaction coordinator is event-driven. Each step is locally atomic on its shard. The saga state machine tracks overall progress.</p>
<h2>Rebalancing Shards</h2>
<p>When you add shards, data must be re-distributed. The naive approach (stop world, migrate, restart) is unacceptable at scale. Use live migration:</p>
<pre><code>Zero-Downtime Rebalancing (Double-Write Pattern):

Phase 1: Add new shard. Start double-writing to old and new shard.
         Read from old shard only.

Phase 2: Backfill historical data from old shard to new shard.
         Verify row counts and checksums.

Phase 3: Switch reads to new shard. Continue double-writing.
         Verify reads are correct on new shard.

Phase 4: Stop writing to old shard. New shard is authoritative.

Phase 5: After validation window, decommission old shard data.
</code></pre>
<pre><code>Architecture During Migration:

Application Server
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard Router     â”‚  (reads routing table from config store)
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â–º Old Shard (reads + writes during phase 1-3)
    â”‚
    â””â”€â”€â–º New Shard (writes only in phase 1, reads+writes in phase 3+)
</code></pre>
<h2>Failure Recovery Strategy</h2>
<p>Each shard should be a primary-replica pair:</p>
<pre><code>Shard 12 Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard 12 Primary   â”‚  RDS PostgreSQL, Multi-AZ
â”‚  (us-east-1a)       â”‚â—„â”€â”€â”€ Writes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Synchronous replication (&#x3C; 5ms lag)
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard 12 Replica   â”‚
â”‚  (us-east-1b)       â”‚â—„â”€â”€â”€ Reads (optional)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>Shard failure handling: The shard router maintains a health map. When a shard's health check fails, the router returns a 503 for requests targeting that shard rather than routing to a degraded node. Partial service availability (63/64 shards healthy) is better than full outage.</p>
<h2>Monitoring Shard Health</h2>
<pre><code class="language-sql">-- Per-shard monitoring query (run on each shard):
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
    n_live_tup AS row_count,
    n_dead_tup AS dead_rows,
    last_autovacuum,
    last_autoanalyze
FROM pg_stat_user_tables
WHERE tablename = 'payments'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
</code></pre>
<p>Prometheus metrics to expose per shard:</p>
<ul>
<li><code>db_shard_connections_active</code> â€” active connections</li>
<li><code>db_shard_query_latency_p99</code> â€” per-shard P99 query latency</li>
<li><code>db_shard_row_count</code> â€” total rows (detects uneven distribution)</li>
<li><code>db_shard_replication_lag_seconds</code> â€” replica lag</li>
<li><code>db_shard_disk_usage_bytes</code> â€” storage growth rate</li>
</ul>
<p>Alert when any shard's P99 latency is 2Ã— the median shard latency â€” early indicator of a hot shard.</p>
<h2>Real Fintech-Scale Example</h2>
<p>A payment processor handling 100M registered users, 5M daily active, 2M payments per day (23 payments/second average, 200 peak):</p>
<p><strong>Schema design:</strong></p>
<pre><code class="language-sql">-- 64 shards, keyed by user_id % 64
-- Each shard: ~1.5M users, ~31K payments/day

CREATE TABLE payments (
    payment_id      UUID DEFAULT gen_random_uuid(),
    user_id         BIGINT NOT NULL,
    merchant_id     BIGINT NOT NULL,
    amount          DECIMAL(19,4) NOT NULL,
    currency        CHAR(3) NOT NULL,
    payment_method  JSONB NOT NULL,
    status          VARCHAR(20) NOT NULL,
    failure_code    VARCHAR(50),
    idempotency_key VARCHAR(255) NOT NULL,
    metadata        JSONB,
    created_at      TIMESTAMPTZ DEFAULT NOW(),
    PRIMARY KEY (user_id, payment_id),
    UNIQUE (idempotency_key)
);

CREATE INDEX idx_payments_user_created ON payments (user_id, created_at DESC);
CREATE INDEX idx_payments_merchant ON payments (merchant_id, created_at DESC);
CREATE INDEX idx_payments_status ON payments (status) WHERE status IN ('pending', 'processing');
</code></pre>
<p><strong>Infrastructure:</strong> 64 RDS PostgreSQL Multi-AZ instances (<code>db.r6g.xlarge</code>), plus 64 read replicas for reporting queries. A separate ClickHouse cluster for analytics.</p>
<p><strong>Shard router:</strong> A thin Spring Boot service with routing table in Redis. Routing table maps <code>shard_id â†’ jdbc_url</code>. Changing routing table in Redis propagates to all router instances within 5 seconds.</p>
<h2>Anti-Patterns</h2>
<p><strong>Anti-pattern 1: Using a monotonically increasing integer as shard key.</strong> New users always go to the latest shard. Use UUID or hash-based IDs.</p>
<p><strong>Anti-pattern 2: Sharding too early.</strong> Sharding adds enormous operational complexity. Shard at 10M users, not 10K.</p>
<p><strong>Anti-pattern 3: Cross-shard foreign keys.</strong> They don't exist in a sharded system. Denormalize aggressively; join at the application layer.</p>
<p><strong>Anti-pattern 4: Shard count that's not a power of 2.</strong> Start with 16 or 32 shards. Re-sharding from 16 to 32 means each shard splits cleanly in two. Re-sharding from 15 to 30 requires moving data across almost every shard boundary.</p>
<p><strong>Anti-pattern 5: Global auto-increment IDs.</strong> Auto-increment across shards requires a centralized sequence, which becomes a bottleneck. Use UUIDs or distributed ID generation (Snowflake-style).</p>
<p>Sharding is not a technology problem â€” it's a data modeling problem. The shard key shapes every query pattern, every operational procedure, and every failure mode for the life of the system. Get it right upfront.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">ğŸ“š</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Designing Data-Intensive Applications</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Essential</span></div><p class="text-xs text-gray-600">The go-to book for understanding databases, consistency, and distributed data.</p></div><a href="https://amzn.to/3RyKzOA" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> â†’</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">MongoDB â€” The Complete Developer&#x27;s Guide â€” Udemy</span></div><p class="text-xs text-gray-600">Comprehensive MongoDB course from basics to advanced aggregations.</p></div><a href="https://www.udemy.com/course/mongodb-the-complete-developers-guide/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> â†’</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Designing%20a%20Database%20Sharding%20Strategy%20for%20100%20Million%20Users&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fdatabase-sharding-100-million-users%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fdatabase-sharding-100-million-users%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#horizontal-vs-vertical-sharding" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Horizontal vs Vertical Sharding</a></li><li class=""><a href="#shard-key-selection-strategy" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Shard Key Selection Strategy</a></li><li class=""><a href="#consistent-hashing" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Consistent Hashing</a></li><li class=""><a href="#hot-shard-problem" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Hot Shard Problem</a></li><li class=""><a href="#cross-shard-query-challenges" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Cross-Shard Query Challenges</a></li><li class=""><a href="#transaction-management-across-shards" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Transaction Management Across Shards</a></li><li class=""><a href="#rebalancing-shards" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Rebalancing Shards</a></li><li class=""><a href="#failure-recovery-strategy" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Failure Recovery Strategy</a></li><li class=""><a href="#monitoring-shard-health" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Monitoring Shard Health</a></li><li class=""><a href="#real-fintech-scale-example" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Real Fintech-Scale Example</a></li><li class=""><a href="#anti-patterns" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Anti-Patterns</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/cassandra-data-modeling/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-green-100 text-green-700">Databases</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Cassandra Data Modeling: Design for Queries, Not Entities</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring â€” every node is equal, there&#x27;s no primary, and data placement is determined by partitâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 18, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>9 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->cassandra</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->nosql</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->data modeling</span></div></article></a><a href="/blog/dynamodb-advanced-patterns/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-green-100 text-green-700">Databases</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">DynamoDB Advanced Patterns: Single-Table Design and Beyond</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before youâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 13, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>9 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->dynamodb</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->aws</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->nosql</span></div></article></a><a href="/blog/zero-downtime-database-migrations/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-green-100 text-green-700">Databases</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Zero-Downtime Database Migrations: Patterns for Production</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible â€” rollback a bad deploy and your code is back to the previous version. Database schema changes are stâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 8, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>8 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->database</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->migrations</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->postgresql</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">â† Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS â€” by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">Â© <!-- -->2026<!-- --> CodeSprintPro Â· Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js Â· TailwindCSS Â· Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Designing a Database Sharding Strategy for 100 Million Users","description":"A practical guide to horizontal sharding at scale: shard key selection, hot shard prevention, consistent hashing, cross-shard queries, and zero-downtime data migration with real fintech architecture examples.","date":"2025-04-14","category":"Databases","tags":["databases","sharding","postgresql","system design","distributed systems","scaling"],"featured":false,"affiliateSection":"database-resources","slug":"database-sharding-100-million-users","readingTime":"9 min read","excerpt":"Vertical scaling has a ceiling. For most applications, that ceiling arrives somewhere between 1 million and 10 million users, depending on write patterns and data size. At 100 million users, the question is not whether tâ€¦","contentHtml":"\u003cp\u003eVertical scaling has a ceiling. For most applications, that ceiling arrives somewhere between 1 million and 10 million users, depending on write patterns and data size. At 100 million users, the question is not whether to shard â€” it's how to shard without destroying query capabilities, operational sanity, and transactional guarantees.\u003c/p\u003e\n\u003cp\u003eThis article is a complete playbook for database sharding at fintech scale.\u003c/p\u003e\n\u003ch2\u003eHorizontal vs Vertical Sharding\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eVertical sharding\u003c/strong\u003e (functional partitioning) splits tables across databases by domain: users database, orders database, payments database. Each service owns its database. This is what microservices architecture gives you naturally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eVertical Sharding:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Users DB          â”‚  â”‚   Orders DB          â”‚  â”‚   Payments DB       â”‚\nâ”‚   users table       â”‚  â”‚   orders table       â”‚  â”‚   payments table    â”‚\nâ”‚   profiles table    â”‚  â”‚   order_items table  â”‚  â”‚   ledger table      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eHorizontal sharding\u003c/strong\u003e splits a single large table across multiple database instances by row. Each shard holds a subset of rows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eHorizontal Sharding:\nusers table â†’ split by user_id range\n\nShard 0: user_id 0â€“24,999,999         (Shard DB 0)\nShard 1: user_id 25,000,000â€“49,999,999 (Shard DB 1)\nShard 2: user_id 50,000,000â€“74,999,999 (Shard DB 2)\nShard 3: user_id 75,000,000â€“99,999,999 (Shard DB 3)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eVertical sharding should always come first. It's operationally simpler, enables independent scaling per domain, and avoids distributed transactions within a service. Horizontal sharding is the next step when a single domain's write volume exceeds what one machine can handle.\u003c/p\u003e\n\u003ch2\u003eShard Key Selection Strategy\u003c/h2\u003e\n\u003cp\u003eThe shard key is the most consequential decision in your sharding design. Getting it wrong means data hotspots, expensive cross-shard joins, or re-sharding after launch.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRule 1: High cardinality.\u003c/strong\u003e The shard key must have enough distinct values to distribute data evenly. \u003ccode\u003euser_id\u003c/code\u003e (UUID or integer) works. \u003ccode\u003ecountry_code\u003c/code\u003e does not â€” if 40% of your users are in the US, one shard gets 40% of the load.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRule 2: Even access distribution.\u003c/strong\u003e The key should distribute both read and write load evenly. Timestamp-based keys (\u003ccode\u003ecreated_at\u003c/code\u003e) often create write hotspots â€” all new records hit the latest shard.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRule 3: Co-locate related data.\u003c/strong\u003e Queries that need to be fast should touch one shard. For a payment system, sharding payments by \u003ccode\u003euser_id\u003c/code\u003e means all of a user's payment history is on one shard, enabling efficient account statements without cross-shard queries.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRule 4: Immutable.\u003c/strong\u003e Changing the shard key value means moving the row to a different shard â€” an expensive operation. Use IDs that never change.\u003c/p\u003e\n\u003cp\u003eFor a fintech platform at 100M users:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003e-- Schema design: payments table\nCREATE TABLE payments (\n    payment_id      UUID DEFAULT gen_random_uuid(),\n    user_id         BIGINT NOT NULL,          -- Shard key\n    merchant_id     BIGINT NOT NULL,\n    amount          DECIMAL(19,4) NOT NULL,\n    currency        CHAR(3) NOT NULL,\n    status          VARCHAR(20) NOT NULL,\n    idempotency_key VARCHAR(255) UNIQUE,\n    created_at      TIMESTAMPTZ DEFAULT NOW(),\n    updated_at      TIMESTAMPTZ DEFAULT NOW(),\n    PRIMARY KEY (user_id, payment_id)         -- Shard key first in PK\n);\n\n-- Shard key determines physical location\n-- All rows for user_id 12345678 are on Shard (12345678 % 64)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eConsistent Hashing\u003c/h2\u003e\n\u003cp\u003eNaive hash-based sharding uses \u003ccode\u003eshard_id = hash(user_id) % num_shards\u003c/code\u003e. The problem: adding a shard changes the modulus, requiring almost all data to move.\u003c/p\u003e\n\u003cp\u003eConsistent hashing solves this with a ring:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eConsistent Hashing Ring (0 to 2^32):\n\n              0 / 2^32\n                  â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  Shard 0               Shard 1\n  (0 - 2^30)        (2^30 - 2^31)\n                â”‚\n           Shard 2\n        (2^31 - 3Â·2^30)\n                â”‚\n           Shard 3\n      (3Â·2^30 - 2^32)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEach shard owns a range on the ring. A user's shard is determined by where \u003ccode\u003ehash(user_id)\u003c/code\u003e lands. When you add a fourth shard, only the users whose hash falls between the new shard's range boundaries need to move â€” roughly 1/N of data, not all of it.\u003c/p\u003e\n\u003cp\u003eVirtual nodes (vnodes) improve distribution: each physical shard has multiple positions on the ring (typically 150â€“300 vnodes). This smooths uneven distributions caused by non-uniform hash outputs.\u003c/p\u003e\n\u003ch2\u003eHot Shard Problem\u003c/h2\u003e\n\u003cp\u003eEven with good key selection, certain shards can become disproportionately busy:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA viral merchant has 10M transactions, all landing on Shard 4\u003c/li\u003e\n\u003cli\u003eA batch job processes all users in \u003ccode\u003euser_id\u003c/code\u003e range 0â€“1M sequentially\u003c/li\u003e\n\u003cli\u003eA celebrity user account is read millions of times per day\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDetection:\u003c/strong\u003e Monitor per-shard QPS, CPU, and I/O independently. A shard running at 80% CPU while others run at 20% is a hot shard.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMitigation strategies:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eKey-based hot shard splitting:\u003c/strong\u003e Split the hot shard into two, re-hashing the subset. Requires migration.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eRead replicas for read-heavy hot shards:\u003c/strong\u003e Add read replicas to the hot shard. Route reads there, writes to the primary.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eApplication-layer caching for celebrity objects:\u003c/strong\u003e Cache the hot user/merchant data in Redis. This solves read hotspots without re-sharding.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSecondary shard key for compound hotness:\u003c/strong\u003e If merchant_id causes hotspots, shard the \u003ccode\u003emerchant_payments\u003c/code\u003e aggregate table by \u003ccode\u003emerchant_id\u003c/code\u003e separately from the main \u003ccode\u003epayments\u003c/code\u003e table sharded by \u003ccode\u003euser_id\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eCross-Shard Query Challenges\u003c/h2\u003e\n\u003cp\u003eThe most painful limitation of horizontal sharding: queries spanning multiple shards require scatter-gather.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSELECT amount, currency, merchant_id\nFROM payments\nWHERE created_at BETWEEN '2025-01-01' AND '2025-01-31'\n  AND status = 'completed'\nORDER BY created_at DESC\nLIMIT 100;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis query has no \u003ccode\u003euser_id\u003c/code\u003e predicate, so it must run on all 64 shards and results must be merged. Approaches:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Application-layer scatter-gather:\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eList\u0026#x3C;CompletableFuture\u0026#x3C;List\u0026#x3C;Payment\u003e\u003e\u003e futures = shards.stream()\n    .map(shard -\u003e CompletableFuture.supplyAsync(\n        () -\u003e shard.query(sql, startDate, endDate), executor))\n    .collect(toList());\n\nList\u0026#x3C;Payment\u003e allResults = futures.stream()\n    .flatMap(f -\u003e f.join().stream())\n    .sorted(Comparator.comparing(Payment::getCreatedAt).reversed())\n    .limit(100)\n    .collect(toList());\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor N shards, you retrieve \u003ccode\u003eN Ã— 100\u003c/code\u003e rows and discard \u003ccode\u003e(N-1) Ã— 100\u003c/code\u003e. At 64 shards, you're fetching 6,400 rows to return 100.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Denormalized query tables in a separate unsharded database:\u003c/strong\u003e\nFor analytics and reporting queries, maintain a denormalized table in a single reporting database (or data warehouse) that aggregates across shards. ETL runs periodically (or via CDC) to populate it.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Elasticsearch or ClickHouse as query layer:\u003c/strong\u003e\nIndex payment data into Elasticsearch or ClickHouse for flexible querying without shard boundaries. The source of truth stays in sharded PostgreSQL; the query engine handles aggregation.\u003c/p\u003e\n\u003ch2\u003eTransaction Management Across Shards\u003c/h2\u003e\n\u003cp\u003eDistributed transactions across shards require either 2-Phase Commit (2PC) or Saga pattern. 2PC is slow and blocking; Saga is complex but resilient.\u003c/p\u003e\n\u003cp\u003eFor a payment that debits \u003ccode\u003euser_id=A\u003c/code\u003e (Shard 12) and credits \u003ccode\u003euser_id=B\u003c/code\u003e (Shard 47), the Saga pattern:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSaga: Cross-Shard Payment Transfer\n\nStep 1: Debit user A on Shard 12\n        â†’ Write debit record, set status=PENDING\n        â†’ Publish event: MoneyDebited(txn_id, user_A, amount)\n\nStep 2: Credit user B on Shard 47 (on event receipt)\n        â†’ Write credit record\n        â†’ Publish event: MoneyCredited(txn_id, user_B, amount)\n\nStep 3: Confirm debit on Shard 12 (on event receipt)\n        â†’ Set debit status=COMPLETED\n\nCompensating transactions (on failure):\nStep 2 fails â†’ Publish event: CreditFailed(txn_id)\nStep 1 compensation â†’ Reverse debit on Shard 12, set status=REVERSED\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe transaction coordinator is event-driven. Each step is locally atomic on its shard. The saga state machine tracks overall progress.\u003c/p\u003e\n\u003ch2\u003eRebalancing Shards\u003c/h2\u003e\n\u003cp\u003eWhen you add shards, data must be re-distributed. The naive approach (stop world, migrate, restart) is unacceptable at scale. Use live migration:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eZero-Downtime Rebalancing (Double-Write Pattern):\n\nPhase 1: Add new shard. Start double-writing to old and new shard.\n         Read from old shard only.\n\nPhase 2: Backfill historical data from old shard to new shard.\n         Verify row counts and checksums.\n\nPhase 3: Switch reads to new shard. Continue double-writing.\n         Verify reads are correct on new shard.\n\nPhase 4: Stop writing to old shard. New shard is authoritative.\n\nPhase 5: After validation window, decommission old shard data.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eArchitecture During Migration:\n\nApplication Server\n        â”‚\n        â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Shard Router     â”‚  (reads routing table from config store)\nâ””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    â”‚\n    â”œâ”€â”€â–º Old Shard (reads + writes during phase 1-3)\n    â”‚\n    â””â”€â”€â–º New Shard (writes only in phase 1, reads+writes in phase 3+)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eFailure Recovery Strategy\u003c/h2\u003e\n\u003cp\u003eEach shard should be a primary-replica pair:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eShard 12 Architecture:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Shard 12 Primary   â”‚  RDS PostgreSQL, Multi-AZ\nâ”‚  (us-east-1a)       â”‚â—„â”€â”€â”€ Writes\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚ Synchronous replication (\u0026#x3C; 5ms lag)\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Shard 12 Replica   â”‚\nâ”‚  (us-east-1b)       â”‚â—„â”€â”€â”€ Reads (optional)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eShard failure handling: The shard router maintains a health map. When a shard's health check fails, the router returns a 503 for requests targeting that shard rather than routing to a degraded node. Partial service availability (63/64 shards healthy) is better than full outage.\u003c/p\u003e\n\u003ch2\u003eMonitoring Shard Health\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003e-- Per-shard monitoring query (run on each shard):\nSELECT\n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,\n    n_live_tup AS row_count,\n    n_dead_tup AS dead_rows,\n    last_autovacuum,\n    last_autoanalyze\nFROM pg_stat_user_tables\nWHERE tablename = 'payments'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePrometheus metrics to expose per shard:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003edb_shard_connections_active\u003c/code\u003e â€” active connections\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edb_shard_query_latency_p99\u003c/code\u003e â€” per-shard P99 query latency\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edb_shard_row_count\u003c/code\u003e â€” total rows (detects uneven distribution)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edb_shard_replication_lag_seconds\u003c/code\u003e â€” replica lag\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edb_shard_disk_usage_bytes\u003c/code\u003e â€” storage growth rate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAlert when any shard's P99 latency is 2Ã— the median shard latency â€” early indicator of a hot shard.\u003c/p\u003e\n\u003ch2\u003eReal Fintech-Scale Example\u003c/h2\u003e\n\u003cp\u003eA payment processor handling 100M registered users, 5M daily active, 2M payments per day (23 payments/second average, 200 peak):\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSchema design:\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003e-- 64 shards, keyed by user_id % 64\n-- Each shard: ~1.5M users, ~31K payments/day\n\nCREATE TABLE payments (\n    payment_id      UUID DEFAULT gen_random_uuid(),\n    user_id         BIGINT NOT NULL,\n    merchant_id     BIGINT NOT NULL,\n    amount          DECIMAL(19,4) NOT NULL,\n    currency        CHAR(3) NOT NULL,\n    payment_method  JSONB NOT NULL,\n    status          VARCHAR(20) NOT NULL,\n    failure_code    VARCHAR(50),\n    idempotency_key VARCHAR(255) NOT NULL,\n    metadata        JSONB,\n    created_at      TIMESTAMPTZ DEFAULT NOW(),\n    PRIMARY KEY (user_id, payment_id),\n    UNIQUE (idempotency_key)\n);\n\nCREATE INDEX idx_payments_user_created ON payments (user_id, created_at DESC);\nCREATE INDEX idx_payments_merchant ON payments (merchant_id, created_at DESC);\nCREATE INDEX idx_payments_status ON payments (status) WHERE status IN ('pending', 'processing');\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eInfrastructure:\u003c/strong\u003e 64 RDS PostgreSQL Multi-AZ instances (\u003ccode\u003edb.r6g.xlarge\u003c/code\u003e), plus 64 read replicas for reporting queries. A separate ClickHouse cluster for analytics.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eShard router:\u003c/strong\u003e A thin Spring Boot service with routing table in Redis. Routing table maps \u003ccode\u003eshard_id â†’ jdbc_url\u003c/code\u003e. Changing routing table in Redis propagates to all router instances within 5 seconds.\u003c/p\u003e\n\u003ch2\u003eAnti-Patterns\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eAnti-pattern 1: Using a monotonically increasing integer as shard key.\u003c/strong\u003e New users always go to the latest shard. Use UUID or hash-based IDs.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnti-pattern 2: Sharding too early.\u003c/strong\u003e Sharding adds enormous operational complexity. Shard at 10M users, not 10K.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnti-pattern 3: Cross-shard foreign keys.\u003c/strong\u003e They don't exist in a sharded system. Denormalize aggressively; join at the application layer.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnti-pattern 4: Shard count that's not a power of 2.\u003c/strong\u003e Start with 16 or 32 shards. Re-sharding from 16 to 32 means each shard splits cleanly in two. Re-sharding from 15 to 30 requires moving data across almost every shard boundary.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnti-pattern 5: Global auto-increment IDs.\u003c/strong\u003e Auto-increment across shards requires a centralized sequence, which becomes a bottleneck. Use UUIDs or distributed ID generation (Snowflake-style).\u003c/p\u003e\n\u003cp\u003eSharding is not a technology problem â€” it's a data modeling problem. The shard key shapes every query pattern, every operational procedure, and every failure mode for the life of the system. Get it right upfront.\u003c/p\u003e\n","tableOfContents":[{"id":"horizontal-vs-vertical-sharding","text":"Horizontal vs Vertical Sharding","level":2},{"id":"shard-key-selection-strategy","text":"Shard Key Selection Strategy","level":2},{"id":"consistent-hashing","text":"Consistent Hashing","level":2},{"id":"hot-shard-problem","text":"Hot Shard Problem","level":2},{"id":"cross-shard-query-challenges","text":"Cross-Shard Query Challenges","level":2},{"id":"transaction-management-across-shards","text":"Transaction Management Across Shards","level":2},{"id":"rebalancing-shards","text":"Rebalancing Shards","level":2},{"id":"failure-recovery-strategy","text":"Failure Recovery Strategy","level":2},{"id":"monitoring-shard-health","text":"Monitoring Shard Health","level":2},{"id":"real-fintech-scale-example","text":"Real Fintech-Scale Example","level":2},{"id":"anti-patterns","text":"Anti-Patterns","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring â€” every node is equal, there's no primary, and data placement is determined by partitâ€¦"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before youâ€¦"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible â€” rollback a bad deploy and your code is back to the previous version. Database schema changes are stâ€¦"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"database-sharding-100-million-users"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>