<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Building a Production RAG System: Embeddings, Vector DBs, and Retrieval<!-- --> | CodeSprintPro</title><meta name="description" content="A practical guide to building a Retrieval-Augmented Generation system â€” from chunking strategies and embedding models to vector databases, retrieval optimization, and avoiding hallucinations." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/building-rag-system-langchain/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Building a Production RAG System: Embeddings, Vector DBs, and Retrieval" data-next-head=""/><meta property="og:description" content="A practical guide to building a Retrieval-Augmented Generation system â€” from chunking strategies and embedding models to vector databases, retrieval optimization, and avoiding hallucinations." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/building-rag-system-langchain/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-02-12" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="AI/ML" data-next-head=""/><meta property="article:tag" content="ai" data-next-head=""/><meta property="article:tag" content="llm" data-next-head=""/><meta property="article:tag" content="rag" data-next-head=""/><meta property="article:tag" content="langchain" data-next-head=""/><meta property="article:tag" content="vector database" data-next-head=""/><meta property="article:tag" content="embeddings" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Building a Production RAG System: Embeddings, Vector DBs, and Retrieval" data-next-head=""/><meta name="twitter:description" content="A practical guide to building a Retrieval-Augmented Generation system â€” from chunking strategies and embedding models to vector databases, retrieval optimization, and avoiding hallucinations." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building a Production RAG System: Embeddings, Vector DBs, and Retrieval","description":"A practical guide to building a Retrieval-Augmented Generation system â€” from chunking strategies and embedding models to vector databases, retrieval optimization, and avoiding hallucinations.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-02-12","dateModified":"2025-02-12","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/building-rag-system-langchain/"},"keywords":"ai, llm, rag, langchain, vector database, embeddings","articleSection":"AI/ML"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Building a Production RAG System: Embeddings, Vector DBs, and Retrieval</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">AI/ML</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Building a Production RAG System: Embeddings, Vector DBs, and Retrieval</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">A practical guide to building a Retrieval-Augmented Generation system â€” from chunking strategies and embedding models to vector databases, retrieval optimization, and avoiding hallucinations.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>Â·</span><span>February 12, 2025</span><span>Â·</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>12 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->ai</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->llm</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->rag</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->langchain</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->vector database</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->embeddings</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Retrieval-Augmented Generation (RAG) is the most practical technique for making LLMs useful on your private data. Instead of hoping the model memorizes your documents during training (it doesn't), RAG retrieves relevant context at query time and injects it into the prompt. The model reasons over retrieved facts rather than hallucinated ones.</p>
<p>Getting RAG to work in a notebook demo is easy. Getting it to work reliably in production â€” with accurate retrieval, consistent quality, and measurable performance â€” requires understanding every component in the pipeline.</p>
<h2>Why RAG, Not Fine-Tuning?</h2>
<table>
<thead>
<tr>
<th>Approach</th>
<th>When to Use</th>
<th>Cost</th>
<th>Freshness</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RAG</strong></td>
<td>Dynamic data, factual Q&#x26;A, large corpora</td>
<td>Low compute, storage cost</td>
<td>Real-time updates</td>
</tr>
<tr>
<td><strong>Fine-tuning</strong></td>
<td>Style/tone transfer, format adherence, domain jargon</td>
<td>High GPU cost, retraining</td>
<td>Snapshot in time</td>
</tr>
<tr>
<td><strong>Context stuffing</strong></td>
<td>&#x3C;128K tokens, structured data</td>
<td>API cost (tokens)</td>
<td>Real-time</td>
</tr>
<tr>
<td><strong>RAG + Fine-tuning</strong></td>
<td>Best factual recall + domain style</td>
<td>High</td>
<td>Real-time</td>
</tr>
</tbody>
</table>
<p>For most enterprise use cases â€” internal knowledge bases, documentation Q&#x26;A, customer support â€” RAG is the right tool.</p>
<h2>The RAG Pipeline</h2>
<p>Think of the RAG pipeline as two separate workflows: an offline ingestion phase where you prepare and index your documents, and an online query phase where you look up relevant information to answer each user question. Understanding this separation helps you optimize each phase independently.</p>
<pre><code>Ingestion Pipeline (offline):
  Documents â†’ Chunker â†’ Embedder â†’ Vector DB

Query Pipeline (online):
  Question â†’ Embedder â†’ Vector DB â†’ [Top-K chunks] â†’ LLM â†’ Answer

Full flow:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Document â”‚â”€â”€â–ºâ”‚  Chunk   â”‚â”€â”€â–ºâ”‚  Embed     â”‚â”€â”€â–ºâ”‚  Vector DB   â”‚
  â”‚ (PDF,    â”‚   â”‚  Split   â”‚   â”‚  (OpenAI,  â”‚   â”‚  (Pinecone,  â”‚
  â”‚  DOCX,   â”‚   â”‚          â”‚   â”‚  Cohere)   â”‚   â”‚  ChromaDB,   â”‚
  â”‚  HTML)   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  pgvector)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚ similarity
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ search
  â”‚  Answer  â”‚â—„â”€â”€â”‚   LLM    â”‚â—„â”€â”€â”‚  Prompt    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚          â”‚   â”‚ (GPT-4,  â”‚   â”‚  Template  â”‚  Top-K chunks
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  Claude) â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2>Step 1: Document Loading and Chunking</h2>
<p>Chunking is the most underappreciated step. Bad chunking breaks context across meaningful boundaries, and no retrieval algorithm can recover from that.</p>
<p>Your goal with chunking is to create pieces of text that are self-contained enough to answer a question on their own, while staying small enough to be precise when retrieved. The code below demonstrates three strategies â€” from the simplest recursive split to the most sophisticated semantic approach â€” so you can pick the right tool for your document type.</p>
<pre><code class="language-python">from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter,
)

# Load documents
loader = DirectoryLoader("./docs", glob="**/*.pdf", loader_cls=PyPDFLoader)
raw_docs = loader.load()

# Strategy 1: Recursive character splitting (most robust for mixed content)
# Tries to split on: paragraphs â†’ sentences â†’ words â†’ characters
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Characters per chunk
    chunk_overlap=200,    # Overlap to preserve context across boundaries
    separators=["\n\n", "\n", ". ", " ", ""],
)
chunks = text_splitter.split_documents(raw_docs)

# Strategy 2: Markdown-aware splitting (for structured docs)
# Respects heading hierarchy â€” chunks stay within sections
md_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[
        ("#", "h1"),
        ("##", "h2"),
        ("###", "h3"),
    ],
    strip_headers=False,  # Keep headers in chunk for context
)

# Strategy 3: Semantic chunking (most accurate, slower)
# Groups sentences by embedding similarity â€” no arbitrary character limits
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

semantic_splitter = SemanticChunker(
    OpenAIEmbeddings(),
    breakpoint_threshold_type="percentile",  # Split at 95th percentile of similarity drop
    breakpoint_threshold_amount=95,
)
</code></pre>
<p>Notice that the <code>chunk_overlap=200</code> parameter is doing important work here: it ensures that a sentence split across two chunks appears in both, so neither chunk loses critical context at its boundary.</p>
<p><strong>Chunking guidelines from production experience:</strong></p>
<ul>
<li><strong>500-1000 characters</strong> for Q&#x26;A over prose documents</li>
<li><strong>1500-2000 characters</strong> for code documentation (preserve function context)</li>
<li><strong>200 character overlap</strong> to prevent context loss at boundaries</li>
<li><strong>Metadata preservation</strong> is critical: always keep source URL, page number, section header</li>
</ul>
<h2>Step 2: Embedding Models</h2>
<p>Embeddings convert text to vectors â€” numerical representations where semantically similar text clusters together in high-dimensional space.</p>
<p>Think of an embedding as a sophisticated "fingerprint" for meaning: two sentences that say the same thing in different words will have nearly identical fingerprints, while two sentences about completely different topics will have fingerprints that bear no resemblance to each other. The code below shows how to generate these fingerprints using three different providers, each with different cost and performance tradeoffs.</p>
<pre><code class="language-python">from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings

# Option 1: OpenAI text-embedding-3-small (recommended for most use cases)
# Dimensions: 1536, Cost: $0.02/1M tokens
# Strong multilingual support, easy integration
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Option 2: Open-source via HuggingFace (no API cost, self-hosted)
# BAAI/bge-large-en-v1.5: Strong English performance, competitive with OpenAI
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)

# Option 3: Cohere embed-english-v3.0
# Better at distinguishing search queries from documents (trained with input_type)
from langchain_cohere import CohereEmbeddings
embeddings = CohereEmbeddings(
    model="embed-english-v3.0",
    input_type="search_query",  # Or "search_document" for indexing
)
</code></pre>
<p>Pay attention to the <code>input_type</code> parameter in the Cohere example â€” this is a subtle but powerful feature. Cohere trains the model to understand that a short user question and a long document passage are answering the same semantic question from different sides, which improves retrieval quality compared to treating both identically.</p>
<p><strong>Embedding model comparison (MTEB benchmark, 2024):</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>MTEB Score</th>
<th>Dimensions</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenAI text-embedding-3-large</td>
<td>64.6</td>
<td>3072</td>
<td>$0.13/1M tokens</td>
</tr>
<tr>
<td>Cohere embed-english-v3.0</td>
<td>64.5</td>
<td>1024</td>
<td>$0.10/1M tokens</td>
</tr>
<tr>
<td>BAAI/bge-large-en-v1.5</td>
<td>63.5</td>
<td>1024</td>
<td>Free (self-hosted)</td>
</tr>
<tr>
<td>OpenAI text-embedding-3-small</td>
<td>62.3</td>
<td>1536</td>
<td>$0.02/1M tokens</td>
</tr>
</tbody>
</table>
<p>For most production RAG systems, <code>text-embedding-3-small</code> or <code>bge-large-en-v1.5</code> provides the best cost/performance tradeoff.</p>
<h2>Step 3: Vector Databases</h2>
<p>Once your documents are embedded, you need somewhere to store and efficiently search those vectors. A vector database is purpose-built for one operation: given a query vector, find the N most similar stored vectors as fast as possible. The choice of database mainly comes down to your scale and infrastructure constraints â€” the code patterns look nearly identical across all three options shown here.</p>
<pre><code class="language-python"># Option 1: ChromaDB (local development, small-medium scale)
import chromadb
from langchain_chroma import Chroma

chroma_client = chromadb.PersistentClient(path="./chroma_db")
vectorstore = Chroma(
    collection_name="docs",
    embedding_function=embeddings,
    client=chroma_client,
)

# Ingest documents
vectorstore.add_documents(chunks)

# Option 2: pgvector (PostgreSQL extension â€” great for existing PG users)
from langchain_postgres import PGVector

vectorstore = PGVector(
    embeddings=embeddings,
    collection_name="docs",
    connection="postgresql+psycopg://user:pass@localhost:5432/mydb",
)

# Option 3: Pinecone (managed, production-grade, serverless)
from langchain_pinecone import PineconeVectorStore
import pinecone

pc = pinecone.Pinecone(api_key="YOUR_API_KEY")
index = pc.Index("docs-index")

vectorstore = PineconeVectorStore(
    index=index,
    embedding=embeddings,
    text_key="text",
)
</code></pre>
<p><strong>Vector DB selection criteria:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>ChromaDB</th>
<th>pgvector</th>
<th>Pinecone</th>
<th>Weaviate</th>
</tr>
</thead>
<tbody>
<tr>
<td>Setup</td>
<td>Trivial</td>
<td>Easy (extension)</td>
<td>Managed</td>
<td>Self-hosted/managed</td>
</tr>
<tr>
<td>Scale</td>
<td>&#x3C;1M vectors</td>
<td>&#x3C;100M vectors</td>
<td>Hundreds of millions</td>
<td>Billions</td>
</tr>
<tr>
<td>Filtering</td>
<td>Basic</td>
<td>Full SQL</td>
<td>Metadata filters</td>
<td>GraphQL</td>
</tr>
<tr>
<td>Cost</td>
<td>Free</td>
<td>PG cost</td>
<td>Pay per vector</td>
<td>Free/managed</td>
</tr>
<tr>
<td>Best for</td>
<td>Dev/prototype</td>
<td>Existing PG</td>
<td>Production SaaS</td>
<td>Complex filtering</td>
</tr>
</tbody>
</table>
<h2>Step 4: Building the Retrieval Chain</h2>
<p>Now that you have indexed documents, you can wire together the full question-answering chain. The prompt template here is doing a critical job: it instructs the model to stay within the retrieved context and explicitly say when it doesn't know something, which is what prevents hallucination.</p>
<p>Basic retrieval is just similarity search. Production retrieval combines dense search, sparse (BM25) search, and reranking.</p>
<pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Basic RAG chain
llm = ChatOpenAI(model="gpt-4o", temperature=0)

prompt_template = """Use the following context to answer the question.
If the answer is not in the context, say "I don't have enough information to answer this question."
Do not make up information.

Context:
{context}

Question: {question}

Answer:"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"],
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",                       # stuff: concat all chunks into one prompt
    retriever=vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}                # Retrieve top 5 chunks
    ),
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True,             # For citation support
)

result = qa_chain.invoke({"query": "What is the refund policy?"})
print(result["result"])
print("Sources:", [doc.metadata for doc in result["source_documents"]])
</code></pre>
<p>The <code>return_source_documents=True</code> flag is important for production use: it lets you display citations alongside the answer, so users can verify the information and you can debug retrieval failures.</p>
<h3>Advanced: Hybrid Search with Reranking</h3>
<p>Pure vector similarity misses keyword-heavy queries. Hybrid search combines dense (embedding) and sparse (BM25/TF-IDF) retrieval. Imagine a user searching for a product code like "SKU-7829" â€” pure semantic search will struggle with this exact string, but BM25 keyword search handles it perfectly. By blending both approaches, you get the best of both worlds.</p>
<pre><code class="language-python">from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

# BM25: keyword-based retrieval (great for exact terms, product codes, names)
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 10

# Dense: semantic retrieval
dense_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# Ensemble: weighted hybrid (60% dense, 40% sparse)
hybrid_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, bm25_retriever],
    weights=[0.6, 0.4],
)

# Reranker: re-scores top-20 results using a cross-encoder model
# Cross-encoders compare query+document jointly â€” much more accurate than bi-encoder similarity
reranker_model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-large")
reranker = CrossEncoderReranker(model=reranker_model, top_n=5)

# Final chain: retrieve 20, rerank to top 5
from langchain.retrievers import ContextualCompressionRetriever

final_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=hybrid_retriever,
)
</code></pre>
<p>The two-stage retrieve-then-rerank pattern is the key insight here: you cast a wide net with fast approximate search (retrieving 20 candidates), then apply an expensive but highly accurate cross-encoder to reorder just those 20 into the final top 5. This gives you accuracy close to brute-force search at a fraction of the cost.</p>
<h2>Step 5: Evaluating RAG Quality</h2>
<p>RAG quality is hard to measure without a systematic evaluation framework. The three key metrics:</p>
<p>Without measurement, you cannot tell the difference between a retrieval bug and a generation bug â€” they both produce wrong answers. The RAGAS library gives you automated scores across four dimensions, letting you pinpoint exactly where your pipeline is failing.</p>
<pre><code class="language-python"># Using RAGAS (open-source RAG evaluation library)
from ragas import evaluate
from ragas.metrics import (
    faithfulness,       # Does the answer contain only information from context?
    answer_relevancy,   # Is the answer relevant to the question?
    context_precision,  # Are retrieved chunks relevant to the question?
    context_recall,     # Were all relevant chunks retrieved?
)
from datasets import Dataset

# Build evaluation dataset
eval_data = {
    "question": ["What is the return policy?", "How do I reset my password?"],
    "answer": [answer1, answer2],              # Model's generated answers
    "contexts": [[doc1, doc2], [doc3]],        # Retrieved chunks
    "ground_truth": ["30-day returns...", "Click Forgot Password..."],  # Expected answers
}

dataset = Dataset.from_dict(eval_data)
result = evaluate(dataset, metrics=[faithfulness, answer_relevancy, context_precision, context_recall])

print(result)
# faithfulness: 0.92 (high = model stays within retrieved context)
# answer_relevancy: 0.87
# context_precision: 0.78 (room to improve retrieval)
# context_recall: 0.83
</code></pre>
<p>A low <code>context_precision</code> score (like 0.78 above) tells you that your retriever is returning irrelevant chunks â€” the problem is in retrieval, not generation. A low <code>faithfulness</code> score tells you the LLM is going off-script and adding information not in the retrieved context â€” the fix is a stricter prompt. These metrics let you diagnose and fix the right component.</p>
<h2>Common Production Pitfalls</h2>
<p><strong>1. Chunks too large:</strong>
The LLM sees the full chunk even if only 2 sentences are relevant. Information density drops. Solution: smaller chunks (500-800 chars) with reranking to surface the best ones.</p>
<p><strong>2. No metadata filtering:</strong>
Searching all documents when the user's question is clearly about product X. Solution: extract entities from the query and filter by metadata before vector search.</p>
<p>Before performing similarity search, you can narrow the candidate pool using structured metadata filters â€” this is far cheaper than relying on the vector index alone to find the right product or time range.</p>
<pre><code class="language-python"># Filter by document source before vector search
results = vectorstore.similarity_search(
    query=user_question,
    k=5,
    filter={"product": "product-x", "version": "2.0"},
)
</code></pre>
<p><strong>3. Missing query rewriting:</strong>
User questions are often terse or ambiguous. Rewrite queries before retrieval. A user typing "refund?" has a very different retrieval surface area than the full query "What is the process for requesting a refund for a digital product?". Query rewriting bridges that gap automatically.</p>
<pre><code class="language-python">rewrite_prompt = """Rewrite this user question into a detailed search query
that will retrieve relevant documentation chunks.

User question: {question}
Search query:"""

# Multi-query: generate 3 variations, union retrieval results
from langchain.retrievers.multi_query import MultiQueryRetriever
multi_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm,
)
</code></pre>
<p><strong>4. No citation/grounding:</strong>
Users cannot verify answers if sources aren't surfaced. Always return source documents and display them alongside the answer.</p>
<p><strong>5. Embedding model mismatch:</strong>
Indexing with one model and querying with another produces garbage results. Document your embedding model version and treat it as a breaking change when upgrading.</p>
<h2>Production Architecture</h2>
<p>With all the individual components understood, here is how they fit together in a complete production system. Notice that caching and monitoring are first-class concerns â€” not afterthoughts â€” because they determine whether your system is fast and debuggable at scale.</p>
<pre><code>Query flow:
  API request â†’ Query rewriter â†’ Hybrid retriever â†’ Reranker â†’ LLM â†’ Response
                                        â†•
                               Pinecone + ElasticSearch
                               (dense + sparse search)

Caching:
  Embed query â†’ Hash â†’ Redis cache â†’ Return cached answer if hit
  Cache TTL: 1 hour (for FAQ-style queries that repeat)

Monitoring:
  - Faithfulness score per request (flag &#x3C;0.8)
  - Retrieval latency (p99 should be &#x3C;500ms)
  - LLM latency (p99 should be &#x3C;3s)
  - Thumbs up/down feedback â†’ used to improve retrieval
</code></pre>
<p>RAG is not a silver bullet, but for the right use cases â€” private knowledge bases, document Q&#x26;A, support bots â€” it's the most cost-effective and maintainable path to reliable LLM-powered applications. The difference between a demo and a production system lies in chunking quality, hybrid retrieval, reranking, and systematic evaluation.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">ğŸ“š</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Building LLM Apps with LangChain â€” Udemy</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Hot</span></div><p class="text-xs text-gray-600">Build RAG systems, agents, and LLM-powered apps with Python and LangChain.</p></div><a href="https://www.udemy.com/course/langchain/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> â†’</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Hands-On Large Language Models</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">New</span></div><p class="text-xs text-gray-600">Practical guide to training, fine-tuning, and deploying LLMs.</p></div><a href="https://amzn.to/3Vpd8h5" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> â†’</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">AI Engineering by Chip Huyen</span></div><p class="text-xs text-gray-600">Building intelligent systems with foundation models â€” from retrieval to agents.</p></div><a href="https://amzn.to/3Vrd1Rd" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> â†’</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Building%20a%20Production%20RAG%20System%3A%20Embeddings%2C%20Vector%20DBs%2C%20and%20Retrieval&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fbuilding-rag-system-langchain%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fbuilding-rag-system-langchain%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#why-rag-not-fine-tuning" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Why RAG, Not Fine-Tuning?</a></li><li class=""><a href="#the-rag-pipeline" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">The RAG Pipeline</a></li><li class=""><a href="#step-1-document-loading-and-chunking" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Step 1: Document Loading and Chunking</a></li><li class=""><a href="#step-2-embedding-models" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Step 2: Embedding Models</a></li><li class=""><a href="#step-3-vector-databases" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Step 3: Vector Databases</a></li><li class=""><a href="#step-4-building-the-retrieval-chain" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Step 4: Building the Retrieval Chain</a></li><li class="ml-4"><a href="#advanced-hybrid-search-with-reranking" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Advanced: Hybrid Search with Reranking</a></li><li class=""><a href="#step-5-evaluating-rag-quality" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Step 5: Evaluating RAG Quality</a></li><li class=""><a href="#common-production-pitfalls" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Common Production Pitfalls</a></li><li class=""><a href="#production-architecture" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Production Architecture</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/fine-tuning-llms/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Fine-Tuning LLMs: When to Fine-Tune, When to Prompt</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning â€” foâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 27, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->llm</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->fine-tuning</span></div></article></a><a href="/blog/llm-agents-tool-use/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Building AI Agents with Tool Use: From Chatbot to Autonomous Agent</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatioâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 23, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>10 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->agents</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->claude</span></div></article></a><a href="/blog/vector-embeddings-deep-dive/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-purple-100 text-purple-700">AI/ML</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Vector Embeddings: The Foundation of Modern AI Applications</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Every modern AI application â€” semantic search, RAG, recommendations, duplicate detection â€” is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semanticâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Mar 11, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ai</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->embeddings</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->vector database</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">â† Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS â€” by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">Â© <!-- -->2026<!-- --> CodeSprintPro Â· Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js Â· TailwindCSS Â· Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Building a Production RAG System: Embeddings, Vector DBs, and Retrieval","description":"A practical guide to building a Retrieval-Augmented Generation system â€” from chunking strategies and embedding models to vector databases, retrieval optimization, and avoiding hallucinations.","date":"2025-02-12","category":"AI/ML","tags":["ai","llm","rag","langchain","vector database","embeddings"],"featured":true,"affiliateSection":"ai-ml-books","slug":"building-rag-system-langchain","readingTime":"12 min read","excerpt":"Retrieval-Augmented Generation (RAG) is the most practical technique for making LLMs useful on your private data. Instead of hoping the model memorizes your documents during training (it doesn't), RAG retrieves relevant â€¦","contentHtml":"\u003cp\u003eRetrieval-Augmented Generation (RAG) is the most practical technique for making LLMs useful on your private data. Instead of hoping the model memorizes your documents during training (it doesn't), RAG retrieves relevant context at query time and injects it into the prompt. The model reasons over retrieved facts rather than hallucinated ones.\u003c/p\u003e\n\u003cp\u003eGetting RAG to work in a notebook demo is easy. Getting it to work reliably in production â€” with accurate retrieval, consistent quality, and measurable performance â€” requires understanding every component in the pipeline.\u003c/p\u003e\n\u003ch2\u003eWhy RAG, Not Fine-Tuning?\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eApproach\u003c/th\u003e\n\u003cth\u003eWhen to Use\u003c/th\u003e\n\u003cth\u003eCost\u003c/th\u003e\n\u003cth\u003eFreshness\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eRAG\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eDynamic data, factual Q\u0026#x26;A, large corpora\u003c/td\u003e\n\u003ctd\u003eLow compute, storage cost\u003c/td\u003e\n\u003ctd\u003eReal-time updates\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eFine-tuning\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eStyle/tone transfer, format adherence, domain jargon\u003c/td\u003e\n\u003ctd\u003eHigh GPU cost, retraining\u003c/td\u003e\n\u003ctd\u003eSnapshot in time\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eContext stuffing\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u0026#x3C;128K tokens, structured data\u003c/td\u003e\n\u003ctd\u003eAPI cost (tokens)\u003c/td\u003e\n\u003ctd\u003eReal-time\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eRAG + Fine-tuning\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eBest factual recall + domain style\u003c/td\u003e\n\u003ctd\u003eHigh\u003c/td\u003e\n\u003ctd\u003eReal-time\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFor most enterprise use cases â€” internal knowledge bases, documentation Q\u0026#x26;A, customer support â€” RAG is the right tool.\u003c/p\u003e\n\u003ch2\u003eThe RAG Pipeline\u003c/h2\u003e\n\u003cp\u003eThink of the RAG pipeline as two separate workflows: an offline ingestion phase where you prepare and index your documents, and an online query phase where you look up relevant information to answer each user question. Understanding this separation helps you optimize each phase independently.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eIngestion Pipeline (offline):\n  Documents â†’ Chunker â†’ Embedder â†’ Vector DB\n\nQuery Pipeline (online):\n  Question â†’ Embedder â†’ Vector DB â†’ [Top-K chunks] â†’ LLM â†’ Answer\n\nFull flow:\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚ Document â”‚â”€â”€â–ºâ”‚  Chunk   â”‚â”€â”€â–ºâ”‚  Embed     â”‚â”€â”€â–ºâ”‚  Vector DB   â”‚\n  â”‚ (PDF,    â”‚   â”‚  Split   â”‚   â”‚  (OpenAI,  â”‚   â”‚  (Pinecone,  â”‚\n  â”‚  DOCX,   â”‚   â”‚          â”‚   â”‚  Cohere)   â”‚   â”‚  ChromaDB,   â”‚\n  â”‚  HTML)   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  pgvector)   â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                         â”‚ similarity\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ search\n  â”‚  Answer  â”‚â—„â”€â”€â”‚   LLM    â”‚â—„â”€â”€â”‚  Prompt    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n  â”‚          â”‚   â”‚ (GPT-4,  â”‚   â”‚  Template  â”‚  Top-K chunks\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  Claude) â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eStep 1: Document Loading and Chunking\u003c/h2\u003e\n\u003cp\u003eChunking is the most underappreciated step. Bad chunking breaks context across meaningful boundaries, and no retrieval algorithm can recover from that.\u003c/p\u003e\n\u003cp\u003eYour goal with chunking is to create pieces of text that are self-contained enough to answer a question on their own, while staying small enough to be precise when retrieved. The code below demonstrates three strategies â€” from the simplest recursive split to the most sophisticated semantic approach â€” so you can pick the right tool for your document type.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langchain.document_loaders import PyPDFLoader, DirectoryLoader\nfrom langchain.text_splitter import (\n    RecursiveCharacterTextSplitter,\n    MarkdownHeaderTextSplitter,\n)\n\n# Load documents\nloader = DirectoryLoader(\"./docs\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\nraw_docs = loader.load()\n\n# Strategy 1: Recursive character splitting (most robust for mixed content)\n# Tries to split on: paragraphs â†’ sentences â†’ words â†’ characters\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,      # Characters per chunk\n    chunk_overlap=200,    # Overlap to preserve context across boundaries\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n)\nchunks = text_splitter.split_documents(raw_docs)\n\n# Strategy 2: Markdown-aware splitting (for structured docs)\n# Respects heading hierarchy â€” chunks stay within sections\nmd_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on=[\n        (\"#\", \"h1\"),\n        (\"##\", \"h2\"),\n        (\"###\", \"h3\"),\n    ],\n    strip_headers=False,  # Keep headers in chunk for context\n)\n\n# Strategy 3: Semantic chunking (most accurate, slower)\n# Groups sentences by embedding similarity â€” no arbitrary character limits\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai import OpenAIEmbeddings\n\nsemantic_splitter = SemanticChunker(\n    OpenAIEmbeddings(),\n    breakpoint_threshold_type=\"percentile\",  # Split at 95th percentile of similarity drop\n    breakpoint_threshold_amount=95,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNotice that the \u003ccode\u003echunk_overlap=200\u003c/code\u003e parameter is doing important work here: it ensures that a sentence split across two chunks appears in both, so neither chunk loses critical context at its boundary.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChunking guidelines from production experience:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e500-1000 characters\u003c/strong\u003e for Q\u0026#x26;A over prose documents\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e1500-2000 characters\u003c/strong\u003e for code documentation (preserve function context)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e200 character overlap\u003c/strong\u003e to prevent context loss at boundaries\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetadata preservation\u003c/strong\u003e is critical: always keep source URL, page number, section header\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eStep 2: Embedding Models\u003c/h2\u003e\n\u003cp\u003eEmbeddings convert text to vectors â€” numerical representations where semantically similar text clusters together in high-dimensional space.\u003c/p\u003e\n\u003cp\u003eThink of an embedding as a sophisticated \"fingerprint\" for meaning: two sentences that say the same thing in different words will have nearly identical fingerprints, while two sentences about completely different topics will have fingerprints that bear no resemblance to each other. The code below shows how to generate these fingerprints using three different providers, each with different cost and performance tradeoffs.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Option 1: OpenAI text-embedding-3-small (recommended for most use cases)\n# Dimensions: 1536, Cost: $0.02/1M tokens\n# Strong multilingual support, easy integration\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# Option 2: Open-source via HuggingFace (no API cost, self-hosted)\n# BAAI/bge-large-en-v1.5: Strong English performance, competitive with OpenAI\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"BAAI/bge-large-en-v1.5\",\n    model_kwargs={\"device\": \"cpu\"},\n    encode_kwargs={\"normalize_embeddings\": True},\n)\n\n# Option 3: Cohere embed-english-v3.0\n# Better at distinguishing search queries from documents (trained with input_type)\nfrom langchain_cohere import CohereEmbeddings\nembeddings = CohereEmbeddings(\n    model=\"embed-english-v3.0\",\n    input_type=\"search_query\",  # Or \"search_document\" for indexing\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePay attention to the \u003ccode\u003einput_type\u003c/code\u003e parameter in the Cohere example â€” this is a subtle but powerful feature. Cohere trains the model to understand that a short user question and a long document passage are answering the same semantic question from different sides, which improves retrieval quality compared to treating both identically.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEmbedding model comparison (MTEB benchmark, 2024):\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e\n\u003cth\u003eMTEB Score\u003c/th\u003e\n\u003cth\u003eDimensions\u003c/th\u003e\n\u003cth\u003eCost\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eOpenAI text-embedding-3-large\u003c/td\u003e\n\u003ctd\u003e64.6\u003c/td\u003e\n\u003ctd\u003e3072\u003c/td\u003e\n\u003ctd\u003e$0.13/1M tokens\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCohere embed-english-v3.0\u003c/td\u003e\n\u003ctd\u003e64.5\u003c/td\u003e\n\u003ctd\u003e1024\u003c/td\u003e\n\u003ctd\u003e$0.10/1M tokens\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBAAI/bge-large-en-v1.5\u003c/td\u003e\n\u003ctd\u003e63.5\u003c/td\u003e\n\u003ctd\u003e1024\u003c/td\u003e\n\u003ctd\u003eFree (self-hosted)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eOpenAI text-embedding-3-small\u003c/td\u003e\n\u003ctd\u003e62.3\u003c/td\u003e\n\u003ctd\u003e1536\u003c/td\u003e\n\u003ctd\u003e$0.02/1M tokens\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFor most production RAG systems, \u003ccode\u003etext-embedding-3-small\u003c/code\u003e or \u003ccode\u003ebge-large-en-v1.5\u003c/code\u003e provides the best cost/performance tradeoff.\u003c/p\u003e\n\u003ch2\u003eStep 3: Vector Databases\u003c/h2\u003e\n\u003cp\u003eOnce your documents are embedded, you need somewhere to store and efficiently search those vectors. A vector database is purpose-built for one operation: given a query vector, find the N most similar stored vectors as fast as possible. The choice of database mainly comes down to your scale and infrastructure constraints â€” the code patterns look nearly identical across all three options shown here.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Option 1: ChromaDB (local development, small-medium scale)\nimport chromadb\nfrom langchain_chroma import Chroma\n\nchroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\nvectorstore = Chroma(\n    collection_name=\"docs\",\n    embedding_function=embeddings,\n    client=chroma_client,\n)\n\n# Ingest documents\nvectorstore.add_documents(chunks)\n\n# Option 2: pgvector (PostgreSQL extension â€” great for existing PG users)\nfrom langchain_postgres import PGVector\n\nvectorstore = PGVector(\n    embeddings=embeddings,\n    collection_name=\"docs\",\n    connection=\"postgresql+psycopg://user:pass@localhost:5432/mydb\",\n)\n\n# Option 3: Pinecone (managed, production-grade, serverless)\nfrom langchain_pinecone import PineconeVectorStore\nimport pinecone\n\npc = pinecone.Pinecone(api_key=\"YOUR_API_KEY\")\nindex = pc.Index(\"docs-index\")\n\nvectorstore = PineconeVectorStore(\n    index=index,\n    embedding=embeddings,\n    text_key=\"text\",\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eVector DB selection criteria:\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eChromaDB\u003c/th\u003e\n\u003cth\u003epgvector\u003c/th\u003e\n\u003cth\u003ePinecone\u003c/th\u003e\n\u003cth\u003eWeaviate\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eSetup\u003c/td\u003e\n\u003ctd\u003eTrivial\u003c/td\u003e\n\u003ctd\u003eEasy (extension)\u003c/td\u003e\n\u003ctd\u003eManaged\u003c/td\u003e\n\u003ctd\u003eSelf-hosted/managed\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eScale\u003c/td\u003e\n\u003ctd\u003e\u0026#x3C;1M vectors\u003c/td\u003e\n\u003ctd\u003e\u0026#x3C;100M vectors\u003c/td\u003e\n\u003ctd\u003eHundreds of millions\u003c/td\u003e\n\u003ctd\u003eBillions\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eFiltering\u003c/td\u003e\n\u003ctd\u003eBasic\u003c/td\u003e\n\u003ctd\u003eFull SQL\u003c/td\u003e\n\u003ctd\u003eMetadata filters\u003c/td\u003e\n\u003ctd\u003eGraphQL\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCost\u003c/td\u003e\n\u003ctd\u003eFree\u003c/td\u003e\n\u003ctd\u003ePG cost\u003c/td\u003e\n\u003ctd\u003ePay per vector\u003c/td\u003e\n\u003ctd\u003eFree/managed\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBest for\u003c/td\u003e\n\u003ctd\u003eDev/prototype\u003c/td\u003e\n\u003ctd\u003eExisting PG\u003c/td\u003e\n\u003ctd\u003eProduction SaaS\u003c/td\u003e\n\u003ctd\u003eComplex filtering\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003eStep 4: Building the Retrieval Chain\u003c/h2\u003e\n\u003cp\u003eNow that you have indexed documents, you can wire together the full question-answering chain. The prompt template here is doing a critical job: it instructs the model to stay within the retrieved context and explicitly say when it doesn't know something, which is what prevents hallucination.\u003c/p\u003e\n\u003cp\u003eBasic retrieval is just similarity search. Production retrieval combines dense search, sparse (BM25) search, and reranking.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langchain_openai import ChatOpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\n# Basic RAG chain\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\nprompt_template = \"\"\"Use the following context to answer the question.\nIf the answer is not in the context, say \"I don't have enough information to answer this question.\"\nDo not make up information.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\nprompt = PromptTemplate(\n    template=prompt_template,\n    input_variables=[\"context\", \"question\"],\n)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",                       # stuff: concat all chunks into one prompt\n    retriever=vectorstore.as_retriever(\n        search_type=\"similarity\",\n        search_kwargs={\"k\": 5}                # Retrieve top 5 chunks\n    ),\n    chain_type_kwargs={\"prompt\": prompt},\n    return_source_documents=True,             # For citation support\n)\n\nresult = qa_chain.invoke({\"query\": \"What is the refund policy?\"})\nprint(result[\"result\"])\nprint(\"Sources:\", [doc.metadata for doc in result[\"source_documents\"]])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003ereturn_source_documents=True\u003c/code\u003e flag is important for production use: it lets you display citations alongside the answer, so users can verify the information and you can debug retrieval failures.\u003c/p\u003e\n\u003ch3\u003eAdvanced: Hybrid Search with Reranking\u003c/h3\u003e\n\u003cp\u003ePure vector similarity misses keyword-heavy queries. Hybrid search combines dense (embedding) and sparse (BM25/TF-IDF) retrieval. Imagine a user searching for a product code like \"SKU-7829\" â€” pure semantic search will struggle with this exact string, but BM25 keyword search handles it perfectly. By blending both approaches, you get the best of both worlds.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\n\n# BM25: keyword-based retrieval (great for exact terms, product codes, names)\nbm25_retriever = BM25Retriever.from_documents(chunks)\nbm25_retriever.k = 10\n\n# Dense: semantic retrieval\ndense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n\n# Ensemble: weighted hybrid (60% dense, 40% sparse)\nhybrid_retriever = EnsembleRetriever(\n    retrievers=[dense_retriever, bm25_retriever],\n    weights=[0.6, 0.4],\n)\n\n# Reranker: re-scores top-20 results using a cross-encoder model\n# Cross-encoders compare query+document jointly â€” much more accurate than bi-encoder similarity\nreranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\nreranker = CrossEncoderReranker(model=reranker_model, top_n=5)\n\n# Final chain: retrieve 20, rerank to top 5\nfrom langchain.retrievers import ContextualCompressionRetriever\n\nfinal_retriever = ContextualCompressionRetriever(\n    base_compressor=reranker,\n    base_retriever=hybrid_retriever,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe two-stage retrieve-then-rerank pattern is the key insight here: you cast a wide net with fast approximate search (retrieving 20 candidates), then apply an expensive but highly accurate cross-encoder to reorder just those 20 into the final top 5. This gives you accuracy close to brute-force search at a fraction of the cost.\u003c/p\u003e\n\u003ch2\u003eStep 5: Evaluating RAG Quality\u003c/h2\u003e\n\u003cp\u003eRAG quality is hard to measure without a systematic evaluation framework. The three key metrics:\u003c/p\u003e\n\u003cp\u003eWithout measurement, you cannot tell the difference between a retrieval bug and a generation bug â€” they both produce wrong answers. The RAGAS library gives you automated scores across four dimensions, letting you pinpoint exactly where your pipeline is failing.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Using RAGAS (open-source RAG evaluation library)\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,       # Does the answer contain only information from context?\n    answer_relevancy,   # Is the answer relevant to the question?\n    context_precision,  # Are retrieved chunks relevant to the question?\n    context_recall,     # Were all relevant chunks retrieved?\n)\nfrom datasets import Dataset\n\n# Build evaluation dataset\neval_data = {\n    \"question\": [\"What is the return policy?\", \"How do I reset my password?\"],\n    \"answer\": [answer1, answer2],              # Model's generated answers\n    \"contexts\": [[doc1, doc2], [doc3]],        # Retrieved chunks\n    \"ground_truth\": [\"30-day returns...\", \"Click Forgot Password...\"],  # Expected answers\n}\n\ndataset = Dataset.from_dict(eval_data)\nresult = evaluate(dataset, metrics=[faithfulness, answer_relevancy, context_precision, context_recall])\n\nprint(result)\n# faithfulness: 0.92 (high = model stays within retrieved context)\n# answer_relevancy: 0.87\n# context_precision: 0.78 (room to improve retrieval)\n# context_recall: 0.83\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA low \u003ccode\u003econtext_precision\u003c/code\u003e score (like 0.78 above) tells you that your retriever is returning irrelevant chunks â€” the problem is in retrieval, not generation. A low \u003ccode\u003efaithfulness\u003c/code\u003e score tells you the LLM is going off-script and adding information not in the retrieved context â€” the fix is a stricter prompt. These metrics let you diagnose and fix the right component.\u003c/p\u003e\n\u003ch2\u003eCommon Production Pitfalls\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e1. Chunks too large:\u003c/strong\u003e\nThe LLM sees the full chunk even if only 2 sentences are relevant. Information density drops. Solution: smaller chunks (500-800 chars) with reranking to surface the best ones.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. No metadata filtering:\u003c/strong\u003e\nSearching all documents when the user's question is clearly about product X. Solution: extract entities from the query and filter by metadata before vector search.\u003c/p\u003e\n\u003cp\u003eBefore performing similarity search, you can narrow the candidate pool using structured metadata filters â€” this is far cheaper than relying on the vector index alone to find the right product or time range.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Filter by document source before vector search\nresults = vectorstore.similarity_search(\n    query=user_question,\n    k=5,\n    filter={\"product\": \"product-x\", \"version\": \"2.0\"},\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e3. Missing query rewriting:\u003c/strong\u003e\nUser questions are often terse or ambiguous. Rewrite queries before retrieval. A user typing \"refund?\" has a very different retrieval surface area than the full query \"What is the process for requesting a refund for a digital product?\". Query rewriting bridges that gap automatically.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003erewrite_prompt = \"\"\"Rewrite this user question into a detailed search query\nthat will retrieve relevant documentation chunks.\n\nUser question: {question}\nSearch query:\"\"\"\n\n# Multi-query: generate 3 variations, union retrieval results\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\nmulti_retriever = MultiQueryRetriever.from_llm(\n    retriever=vectorstore.as_retriever(),\n    llm=llm,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e4. No citation/grounding:\u003c/strong\u003e\nUsers cannot verify answers if sources aren't surfaced. Always return source documents and display them alongside the answer.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e5. Embedding model mismatch:\u003c/strong\u003e\nIndexing with one model and querying with another produces garbage results. Document your embedding model version and treat it as a breaking change when upgrading.\u003c/p\u003e\n\u003ch2\u003eProduction Architecture\u003c/h2\u003e\n\u003cp\u003eWith all the individual components understood, here is how they fit together in a complete production system. Notice that caching and monitoring are first-class concerns â€” not afterthoughts â€” because they determine whether your system is fast and debuggable at scale.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eQuery flow:\n  API request â†’ Query rewriter â†’ Hybrid retriever â†’ Reranker â†’ LLM â†’ Response\n                                        â†•\n                               Pinecone + ElasticSearch\n                               (dense + sparse search)\n\nCaching:\n  Embed query â†’ Hash â†’ Redis cache â†’ Return cached answer if hit\n  Cache TTL: 1 hour (for FAQ-style queries that repeat)\n\nMonitoring:\n  - Faithfulness score per request (flag \u0026#x3C;0.8)\n  - Retrieval latency (p99 should be \u0026#x3C;500ms)\n  - LLM latency (p99 should be \u0026#x3C;3s)\n  - Thumbs up/down feedback â†’ used to improve retrieval\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRAG is not a silver bullet, but for the right use cases â€” private knowledge bases, document Q\u0026#x26;A, support bots â€” it's the most cost-effective and maintainable path to reliable LLM-powered applications. The difference between a demo and a production system lies in chunking quality, hybrid retrieval, reranking, and systematic evaluation.\u003c/p\u003e\n","tableOfContents":[{"id":"why-rag-not-fine-tuning","text":"Why RAG, Not Fine-Tuning?","level":2},{"id":"the-rag-pipeline","text":"The RAG Pipeline","level":2},{"id":"step-1-document-loading-and-chunking","text":"Step 1: Document Loading and Chunking","level":2},{"id":"step-2-embedding-models","text":"Step 2: Embedding Models","level":2},{"id":"step-3-vector-databases","text":"Step 3: Vector Databases","level":2},{"id":"step-4-building-the-retrieval-chain","text":"Step 4: Building the Retrieval Chain","level":2},{"id":"advanced-hybrid-search-with-reranking","text":"Advanced: Hybrid Search with Reranking","level":3},{"id":"step-5-evaluating-rag-quality","text":"Step 5: Evaluating RAG Quality","level":2},{"id":"common-production-pitfalls","text":"Common Production Pitfalls","level":2},{"id":"production-architecture","text":"Production Architecture","level":2}]},"relatedPosts":[{"title":"Fine-Tuning LLMs: When to Fine-Tune, When to Prompt","description":"Decide when fine-tuning beats prompt engineering, how to prepare training data, run LoRA fine-tuning efficiently, and evaluate model quality. Covers OpenAI fine-tuning and open-source with Hugging Face.","date":"2025-03-27","category":"AI/ML","tags":["ai","llm","fine-tuning","lora","hugging face","openai","machine learning"],"featured":false,"affiliateSection":"ai-ml-books","slug":"fine-tuning-llms","readingTime":"10 min read","excerpt":"Fine-tuning is often the wrong choice. Most problems that engineers reach for fine-tuning to solve are better solved with better prompt engineering, few-shot examples, or RAG. But when you genuinely need fine-tuning â€” foâ€¦"},{"title":"Building AI Agents with Tool Use: From Chatbot to Autonomous Agent","description":"Build production AI agents using Claude's tool use API. Learn the agentic loop, error handling, multi-step reasoning, human-in-the-loop patterns, and how to build reliable autonomous systems.","date":"2025-03-23","category":"AI/ML","tags":["ai","agents","claude","tool use","llm","autonomous systems","python"],"featured":false,"affiliateSection":"ai-ml-books","slug":"llm-agents-tool-use","readingTime":"10 min read","excerpt":"A chatbot answers questions. An agent takes actions. The difference is tool use: the ability to call functions, search databases, execute code, and interact with external systems. When a model can look up real informatioâ€¦"},{"title":"Vector Embeddings: The Foundation of Modern AI Applications","description":"Understand vector embeddings, similarity search, and vector databases. Build semantic search, recommendation systems, and RAG pipelines using pgvector, Pinecone, and OpenAI embeddings.","date":"2025-03-11","category":"AI/ML","tags":["ai","embeddings","vector database","semantic search","rag","pgvector","pinecone"],"featured":false,"affiliateSection":"ai-ml-books","slug":"vector-embeddings-deep-dive","readingTime":"11 min read","excerpt":"Every modern AI application â€” semantic search, RAG, recommendations, duplicate detection â€” is built on vector embeddings. An embedding converts text, images, or audio into a point in high-dimensional space where semanticâ€¦"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"building-rag-system-langchain"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>