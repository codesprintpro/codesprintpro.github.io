<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Redis Caching Strategy at Scale: Beyond Simple Key-Value<!-- --> | CodeSprintPro</title><meta name="description" content="Cache stampede, penetration, avalanche, eviction policy selection, clustering, and persistence trade-offs for production Redis deployments. With Java examples and a real production incident walkthrough." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/redis-caching-strategy-at-scale/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Redis Caching Strategy at Scale: Beyond Simple Key-Value" data-next-head=""/><meta property="og:description" content="Cache stampede, penetration, avalanche, eviction policy selection, clustering, and persistence trade-offs for production Redis deployments. With Java examples and a real production incident walkthrough." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/redis-caching-strategy-at-scale/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-05-03" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="Databases" data-next-head=""/><meta property="article:tag" content="redis" data-next-head=""/><meta property="article:tag" content="caching" data-next-head=""/><meta property="article:tag" content="java" data-next-head=""/><meta property="article:tag" content="spring boot" data-next-head=""/><meta property="article:tag" content="performance" data-next-head=""/><meta property="article:tag" content="distributed systems" data-next-head=""/><meta property="article:tag" content="cache stampede" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Redis Caching Strategy at Scale: Beyond Simple Key-Value" data-next-head=""/><meta name="twitter:description" content="Cache stampede, penetration, avalanche, eviction policy selection, clustering, and persistence trade-offs for production Redis deployments. With Java examples and a real production incident walkthrough." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Redis Caching Strategy at Scale: Beyond Simple Key-Value","description":"Cache stampede, penetration, avalanche, eviction policy selection, clustering, and persistence trade-offs for production Redis deployments. With Java examples and a real production incident walkthrough.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-05-03","dateModified":"2025-05-03","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/redis-caching-strategy-at-scale/"},"keywords":"redis, caching, java, spring boot, performance, distributed systems, cache stampede","articleSection":"Databases"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Redis Caching Strategy at Scale: Beyond Simple Key-Value</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">Databases</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Redis Caching Strategy at Scale: Beyond Simple Key-Value</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Cache stampede, penetration, avalanche, eviction policy selection, clustering, and persistence trade-offs for production Redis deployments. With Java examples and a real production incident walkthrough.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>¬∑</span><span>May 3, 2025</span><span>¬∑</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->redis</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->caching</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->java</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->spring boot</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->performance</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->distributed systems</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->cache stampede</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Every senior engineer has fought a caching bug that looked simple and turned out to be a distributed systems problem. Cache stampedes, thundering herds, avalanche failures ‚Äî these happen at scale and they are expensive. This article covers the caching patterns and anti-patterns that separate a cache that works at 100 RPS from one that holds up at 100,000 RPS.</p>
<h2>Cache-Aside vs Write-Through vs Write-Behind</h2>
<p><strong>Cache-aside (lazy loading)</strong> is the default pattern: check cache, miss ‚Üí fetch from DB, populate cache, return.</p>
<pre><code class="language-java">public Product getProduct(String productId) {
    // 1. Check cache
    Product cached = redisTemplate.opsForValue().get("product:" + productId);
    if (cached != null) return cached;

    // 2. Cache miss: fetch from DB
    Product product = productRepository.findById(productId)
        .orElseThrow(() -> new ProductNotFoundException(productId));

    // 3. Populate cache with TTL
    redisTemplate.opsForValue().set("product:" + productId, product, Duration.ofMinutes(30));
    return product;
}
</code></pre>
<p><strong>Properties:</strong> Reads are fast after warmup. Cache only holds data that's actually requested (space efficient). Inconsistency window = TTL (or until next write invalidates the key). Cache cold start causes DB load spike ‚Äî important after deploys.</p>
<p><strong>Write-through</strong> writes to cache and DB simultaneously on every update:</p>
<pre><code class="language-java">@Transactional
public Product updateProduct(String productId, ProductUpdate update) {
    Product product = productRepository.findById(productId).orElseThrow();
    product.apply(update);
    productRepository.save(product);  // Write to DB

    // Immediately update cache ‚Äî no stale reads
    redisTemplate.opsForValue().set("product:" + productId, product, Duration.ofMinutes(30));
    return product;
}
</code></pre>
<p><strong>Properties:</strong> Cache is always fresh (no inconsistency window). Every write touches both DB and cache, even for data that's never read. Cache warms up on writes, not on reads.</p>
<p><strong>Write-behind (write-back)</strong> writes to cache immediately and to the DB asynchronously:</p>
<pre><code class="language-java">public void updateProductPrice(String productId, BigDecimal newPrice) {
    // Update cache synchronously
    String key = "product:" + productId;
    Product product = (Product) redisTemplate.opsForValue().get(key);
    product.setPrice(newPrice);
    redisTemplate.opsForValue().set(key, product, Duration.ofMinutes(30));

    // Schedule async DB write
    writeQueue.submit(() -> productRepository.updatePrice(productId, newPrice));
}
</code></pre>
<p><strong>Properties:</strong> Lowest write latency. High risk ‚Äî if Redis fails before the async write, data is lost. Use only when brief data loss is acceptable (analytics counters, view counts, non-financial metrics).</p>
<p>For most production services: <strong>cache-aside for reads, write-through or cache invalidation on writes</strong>.</p>
<h2>Cache Stampede Problem</h2>
<p>The cache stampede (thundering herd) happens when a popular key expires and many concurrent requests all miss the cache simultaneously, all hitting the database at once:</p>
<pre><code>T=0: Key "hot-product-123" expires (was serving 1000 req/s)
T=0 to T=200ms: 200 requests miss cache, all query PostgreSQL simultaneously
PostgreSQL: 200 concurrent queries for the same row
Result: DB CPU spike, query queue backs up, timeouts cascade
</code></pre>
<p>At 1,000 requests/second on a key with a 30-minute TTL, when the key expires, you get ~200 simultaneous DB queries in 200ms.</p>
<h3>Solution 1: Probabilistic Early Expiry (PER)</h3>
<p>Recompute the cache before it expires, with probability proportional to how close we are to expiry. Early recompute happens in one thread while others still read the valid (slightly stale) cache:</p>
<pre><code class="language-java">public Product getProductWithPER(String productId) {
    String key = "product:" + productId;
    CachedValue&#x3C;Product> cached = getWithTTL(key);

    if (cached == null) {
        return fetchAndCache(productId);
    }

    // Probabilistic early expiry
    long remainingTtlSeconds = cached.ttlSeconds();
    double fetchTime = 0.1; // 100ms to recompute
    double beta = 1.0;

    // Recompute if: -fetchTime * beta * ln(random) >= remainingTtl
    if (-fetchTime * beta * Math.log(Math.random()) >= remainingTtlSeconds) {
        // Early refresh ‚Äî only one thread wins this race (via lock below)
        return fetchAndCacheIfLeader(productId);
    }

    return cached.value();
}
</code></pre>
<h3>Solution 2: Distributed Lock (Single Recompute)</h3>
<p>Only one thread recomputes on cache miss; others wait or serve stale:</p>
<pre><code class="language-java">private static final String LOCK_PREFIX = "lock:";
private static final long LOCK_TTL_MS = 5000;

public Product getProductLocked(String productId) {
    String valueKey = "product:" + productId;
    String lockKey = LOCK_PREFIX + productId;

    // Fast path: cache hit
    Product cached = redisTemplate.opsForValue().get(valueKey);
    if (cached != null) return cached;

    // Try to acquire lock (NX = only set if not exists)
    Boolean acquired = redisTemplate.opsForValue()
        .setIfAbsent(lockKey, "1", Duration.ofMillis(LOCK_TTL_MS));

    if (Boolean.TRUE.equals(acquired)) {
        try {
            // Double-check after acquiring lock
            cached = redisTemplate.opsForValue().get(valueKey);
            if (cached != null) return cached;

            // We are the designated recomputer
            Product product = productRepository.findById(productId).orElseThrow();
            redisTemplate.opsForValue().set(valueKey, product, Duration.ofMinutes(30));
            return product;
        } finally {
            redisTemplate.delete(lockKey);
        }
    } else {
        // Another thread is recomputing ‚Äî wait briefly and retry
        Uninterruptibles.sleepUninterruptibly(100, TimeUnit.MILLISECONDS);
        return getProductLocked(productId); // Retry ‚Äî will likely hit cache now
    }
}
</code></pre>
<h3>Solution 3: Staggered TTL</h3>
<p>Add random jitter to TTLs so keys in a set don't expire simultaneously:</p>
<pre><code class="language-java">private Duration jitteredTtl(Duration baseTtl) {
    long jitterSeconds = ThreadLocalRandom.current()
        .nextLong(0, baseTtl.toSeconds() / 10); // ¬±10% jitter
    return baseTtl.plusSeconds(jitterSeconds);
}

redisTemplate.opsForValue().set(key, value, jitteredTtl(Duration.ofMinutes(30)));
</code></pre>
<h2>Cache Penetration and Avalanche</h2>
<p><strong>Cache penetration:</strong> Requests for keys that never exist (e.g., <code>user_id=-1</code>, or IDs for deleted entities). These always miss cache and always hit the DB.</p>
<p><strong>Fix:</strong> Cache negative results with a short TTL, or use a Bloom filter to reject impossible keys upfront:</p>
<pre><code class="language-java">public Optional&#x3C;User> getUser(Long userId) {
    if (!bloomFilter.mightContain(userId)) {
        return Optional.empty(); // Definitely doesn't exist
    }

    String key = "user:" + userId;
    Object cached = redisTemplate.opsForValue().get(key);

    if (cached instanceof NullSentinel) {
        return Optional.empty(); // Cached negative result
    }

    if (cached instanceof User user) {
        return Optional.of(user);
    }

    // DB lookup
    Optional&#x3C;User> user = userRepository.findById(userId);
    if (user.isEmpty()) {
        // Cache negative result for 60 seconds
        redisTemplate.opsForValue().set(key, NullSentinel.INSTANCE, Duration.ofSeconds(60));
    } else {
        redisTemplate.opsForValue().set(key, user.get(), Duration.ofMinutes(30));
    }
    return user;
}
</code></pre>
<p><strong>Cache avalanche:</strong> Many keys expire simultaneously (same TTL set in a batch job), causing a sudden DB load spike.</p>
<p><strong>Fix:</strong> Jitter TTLs (shown above). Alternatively, warm the cache before keys expire using a background job that refreshes keys at 80% of their TTL.</p>
<h2>TTL Strategy Design</h2>
<p>TTL selection is not guesswork ‚Äî it's a trade-off between freshness and hit rate.</p>
<pre><code>TTL decision matrix:

Data type              | Update frequency  | Staleness tolerance | Recommended TTL
-----------------------|-------------------|--------------------|-----------------
Product price          | Minutes           | Low (financial)    | 5 minutes
Product catalog        | Hours             | Medium             | 1 hour + invalidation
User profile           | Daily             | Low                | 30 minutes
Static content (i18n)  | Weekly            | High               | 24 hours
Search results         | Real-time         | High               | 5 minutes
Session data           | Per request       | None               | Session timeout
Rate limit counters    | Per request       | None               | Window size (60s)
</code></pre>
<p><strong>Invalidation over TTL for write-heavy data:</strong></p>
<p>When an entity is updated, immediately delete (or update) its cache key rather than waiting for TTL expiry. This requires write operations to know which cache keys to invalidate ‚Äî a coupling that must be managed carefully.</p>
<h2>Memory Fragmentation</h2>
<p>Redis allocates memory using jemalloc. Over time, allocating and freeing keys of varying sizes causes fragmentation ‚Äî Redis reports 2GB used but actually occupies 3GB of system memory.</p>
<p>Monitor fragmentation ratio:</p>
<pre><code class="language-bash">redis-cli INFO memory | grep mem_fragmentation_ratio
# > 1.5 = high fragmentation, consider restart or activedefrag
# 1.0‚Äì1.5 = normal
# &#x3C; 1.0 = Redis is using swap (severe problem)
</code></pre>
<p>Enable active defragmentation for long-running Redis instances:</p>
<pre><code>activedefrag yes
active-defrag-ignore-bytes 100mb  # Start defrag when 100MB fragmented
active-defrag-threshold-lower 10  # Start defrag when fragmentation > 10%
active-defrag-threshold-upper 25  # Use max CPU when fragmentation > 25%
</code></pre>
<h2>Eviction Policy Comparison</h2>
<p>When Redis reaches <code>maxmemory</code>, it evicts keys according to the policy:</p>
<table>
<thead>
<tr>
<th>Policy</th>
<th>Behavior</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>noeviction</code></td>
<td>Returns error on writes when full</td>
<td>When cache full = outage (unacceptable for most)</td>
</tr>
<tr>
<td><code>allkeys-lru</code></td>
<td>Evict least recently used across all keys</td>
<td>General-purpose cache</td>
</tr>
<tr>
<td><code>volatile-lru</code></td>
<td>Evict LRU only from keys with TTL</td>
<td>Mixed cache + session store</td>
</tr>
<tr>
<td><code>allkeys-lfu</code></td>
<td>Evict least frequently used</td>
<td>Workloads with hotspot keys</td>
</tr>
<tr>
<td><code>volatile-ttl</code></td>
<td>Evict keys closest to expiry</td>
<td>When you want expiry-driven eviction</td>
</tr>
<tr>
<td><code>allkeys-random</code></td>
<td>Random eviction</td>
<td>Uniform access patterns</td>
</tr>
</tbody>
</table>
<p>For a pure cache: <code>allkeys-lru</code> or <code>allkeys-lfu</code>. <code>allkeys-lfu</code> is better for workloads where a small set of keys is accessed constantly (product catalog, configuration) ‚Äî LFU keeps hot keys in memory longer than LRU.</p>
<p>For a mixed cache + session store: <code>volatile-lru</code> ‚Äî only evicts keys with TTL, protecting session keys that have no TTL.</p>
<h2>Redis Clustering</h2>
<p>Redis Cluster shards data across nodes using 16,384 hash slots:</p>
<pre><code>Redis Cluster (3 primary + 3 replica):

Key: "product:123"
hash_slot = CRC16("product:123") % 16384 = 7483

Slot 7483 is owned by Primary-2
‚Üí Route request to Primary-2

Primary-1 (slots 0-5460)          Primary-2 (slots 5461-10922)        Primary-3 (slots 10923-16383)
     ‚îÇ                                   ‚îÇ                                    ‚îÇ
     ‚ñº                                   ‚ñº                                    ‚ñº
Replica-1                           Replica-2                            Replica-3
</code></pre>
<p><strong>Cluster limitations:</strong></p>
<ul>
<li>Multi-key operations (<code>MGET</code>, <code>MSET</code>, pipeline) only work when all keys are in the same slot</li>
<li>Use hash tags <code>{user:123}:profile</code> to force co-location: <code>{user:123}</code> is used for slot calculation, so all keys with the same tag go to the same slot</li>
<li>Transactions (<code>MULTI/EXEC</code>) only work on single nodes ‚Äî avoid cross-slot transactions</li>
</ul>
<pre><code class="language-java">// Hash tags for co-located keys:
String profileKey = "{user:" + userId + "}:profile";
String settingsKey = "{user:" + userId + "}:settings";
// Both keys route to the same slot ‚Üí MGET works
List&#x3C;Object> results = redisTemplate.opsForValue().multiGet(
    List.of(profileKey, settingsKey));
</code></pre>
<h2>Persistence: RDB vs AOF</h2>
<table>
<thead>
<tr>
<th></th>
<th>RDB (Snapshots)</th>
<th>AOF (Append-Only File)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Recovery</td>
<td>Point-in-time snapshot</td>
<td>Replay every write since last snapshot</td>
</tr>
<tr>
<td>Data loss on crash</td>
<td>Up to snapshot interval (minutes)</td>
<td>Up to 1 second (with fsync=everysec)</td>
</tr>
<tr>
<td>Performance</td>
<td>Low CPU overhead</td>
<td>fsync adds ~10% write overhead</td>
</tr>
<tr>
<td>Restart time</td>
<td>Fast (load snapshot)</td>
<td>Slow (replay log, can take minutes)</td>
</tr>
<tr>
<td>File size</td>
<td>Compact</td>
<td>Grows unbounded, requires AOF rewrite</td>
</tr>
</tbody>
</table>
<p><strong>For a cache:</strong> RDB only. Losing cache data on crash is acceptable ‚Äî the cache warms up from the DB.</p>
<p><strong>For a cache with session data:</strong> AOF with <code>appendfsync everysec</code>. Max 1 second of session data loss on crash.</p>
<p><strong>For Redis as primary data store (not cache):</strong> AOF with <code>appendfsync always</code> + RDB for backup. Maximum durability, highest write overhead.</p>
<pre><code># redis.conf for production cache:
maxmemory 12gb
maxmemory-policy allkeys-lfu
save 900 1     # RDB snapshot if 1 key changed in 900s
save 300 10    # RDB snapshot if 10 keys changed in 300s
save 60 10000  # RDB snapshot if 10000 keys changed in 60s
appendonly no  # No AOF for pure cache
</code></pre>
<h2>Distributed Locks with Redis</h2>
<p>Redis Redlock is the distributed lock algorithm. For single-node Redis or Redis Cluster, a simpler approach:</p>
<pre><code class="language-java">public &#x3C;T> T withLock(String resource, Duration timeout, Supplier&#x3C;T> task) {
    String lockKey = "lock:" + resource;
    String lockValue = UUID.randomUUID().toString(); // Unique per lock acquisition
    boolean acquired = false;

    try {
        acquired = Boolean.TRUE.equals(
            redisTemplate.opsForValue()
                .setIfAbsent(lockKey, lockValue, timeout)
        );

        if (!acquired) {
            throw new LockNotAvailableException("Resource locked: " + resource);
        }

        return task.get();
    } finally {
        if (acquired) {
            // Release only if we own the lock (Lua script ensures atomicity)
            redisTemplate.execute(
                RELEASE_LOCK_SCRIPT,
                Collections.singletonList(lockKey),
                lockValue
            );
        }
    }
}

// Lua script for atomic check-and-delete:
private static final RedisScript&#x3C;Long> RELEASE_LOCK_SCRIPT = RedisScript.of(
    "if redis.call('get', KEYS[1]) == ARGV[1] then " +
    "    return redis.call('del', KEYS[1]) " +
    "else return 0 end",
    Long.class
);
</code></pre>
<p>The Lua script is critical. Without it, two operations occur: <code>GET</code> to verify ownership, then <code>DEL</code>. A different process could acquire the lock between those two operations, and you'd delete their lock.</p>
<h2>Real World Production Issue</h2>
<p><strong>System:</strong> E-commerce product catalog service, 50,000 SKUs, Redis Cluster (6 nodes), Spring Boot.</p>
<p><strong>Incident:</strong> Flash sale launch. At 12:00:00, 50,000 concurrent users loaded the sale page. Cache hit rate: 98%. The remaining 2% ‚Äî 1,000 users ‚Äî missed on the most popular sale items because those specific keys had expired at 11:59:58 due to a batch TTL reset job.</p>
<p>All 1,000 users simultaneously queried PostgreSQL for 3 products. PostgreSQL's connection pool (20 connections) was saturated. Other queries (cart, checkout) backed up. API P99 hit 45 seconds.</p>
<p><strong>Root causes:</strong></p>
<ol>
<li>Batch job set the same TTL on all keys ‚Üí mass expiry</li>
<li>No stampede protection</li>
<li>PostgreSQL connection pool too small for burst</li>
</ol>
<p><strong>Fixes applied:</strong></p>
<ol>
<li>Jitter added to all TTL values (¬±10%)</li>
<li>Probabilistic early expiry for top-100 accessed keys</li>
<li>PostgreSQL connection pool increased to 50</li>
<li>Read replicas added; cache miss reads route to replicas</li>
</ol>
<p>Sale metrics before/after fix: Cache stampede incidents dropped from 8/month to 0.</p>
<h2>Monitoring Redis Memory and CPU</h2>
<pre><code class="language-bash"># Key memory metrics:
redis-cli INFO memory
# used_memory_human: 8.50G     ‚Üí actual data
# maxmemory_human: 12.00G      ‚Üí limit
# mem_fragmentation_ratio: 1.12 ‚Üí healthy
# used_memory_rss_human: 9.54G  ‚Üí OS-reported

# Eviction monitoring:
redis-cli INFO stats | grep evicted_keys
# Rising number = memory pressure, increase maxmemory or evict manually

# Slow query log:
redis-cli CONFIG SET slowlog-log-slower-than 10000  # 10ms threshold
redis-cli SLOWLOG GET 25  # View last 25 slow commands
</code></pre>
<p>Prometheus alert rules:</p>
<pre><code class="language-yaml"># Memory usage > 80% of maxmemory
redis_memory_used_bytes / redis_memory_max_bytes > 0.8

# High eviction rate (cache under memory pressure)
rate(redis_evicted_keys_total[5m]) > 100

# Cache hit rate dropping
redis_keyspace_hits_total / (redis_keyspace_hits_total + redis_keyspace_misses_total) &#x3C; 0.90
</code></pre>
<p>The difference between a cache that helps you and one that causes your worst production incidents is usually 3 things: TTL jitter, stampede protection, and eviction policy selection. Everything else is tuning.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">üìö</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Designing Data-Intensive Applications</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Best Seller</span></div><p class="text-xs text-gray-600">The definitive guide to building scalable, reliable distributed systems by Martin Kleppmann.</p></div><a href="https://amzn.to/3RyKzOA" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Kafka: The Definitive Guide</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Editor&#x27;s Pick</span></div><p class="text-xs text-gray-600">Real-time data and stream processing by Confluent engineers.</p></div><a href="https://amzn.to/3TpGKsI" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Apache Kafka Series on Udemy</span></div><p class="text-xs text-gray-600">Hands-on Kafka course covering producers, consumers, Kafka Streams, and Connect.</p></div><a href="https://www.udemy.com/course/apache-kafka/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> ‚Üí</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Redis%20Caching%20Strategy%20at%20Scale%3A%20Beyond%20Simple%20Key-Value&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fredis-caching-strategy-at-scale%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fredis-caching-strategy-at-scale%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#cache-aside-vs-write-through-vs-write-behind" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Cache-Aside vs Write-Through vs Write-Behind</a></li><li class=""><a href="#cache-stampede-problem" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Cache Stampede Problem</a></li><li class="ml-4"><a href="#solution-1-probabilistic-early-expiry-per" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Solution 1: Probabilistic Early Expiry (PER)</a></li><li class="ml-4"><a href="#solution-2-distributed-lock-single-recompute" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Solution 2: Distributed Lock (Single Recompute)</a></li><li class="ml-4"><a href="#solution-3-staggered-ttl" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Solution 3: Staggered TTL</a></li><li class=""><a href="#cache-penetration-and-avalanche" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Cache Penetration and Avalanche</a></li><li class=""><a href="#ttl-strategy-design" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">TTL Strategy Design</a></li><li class=""><a href="#memory-fragmentation" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Memory Fragmentation</a></li><li class=""><a href="#eviction-policy-comparison" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Eviction Policy Comparison</a></li><li class=""><a href="#redis-clustering" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Redis Clustering</a></li><li class=""><a href="#persistence-rdb-vs-aof" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Persistence: RDB vs AOF</a></li><li class=""><a href="#distributed-locks-with-redis" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Distributed Locks with Redis</a></li><li class=""><a href="#real-world-production-issue" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Real World Production Issue</a></li><li class=""><a href="#monitoring-redis-memory-and-cpu" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Monitoring Redis Memory and CPU</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/cassandra-data-modeling/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-green-100 text-green-700">Databases</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Cassandra Data Modeling: Design for Queries, Not Entities</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring ‚Äî every node is equal, there&#x27;s no primary, and data placement is determined by partit‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 18, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>9 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->cassandra</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->nosql</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->data modeling</span></div></article></a><a href="/blog/dynamodb-advanced-patterns/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-green-100 text-green-700">Databases</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">DynamoDB Advanced Patterns: Single-Table Design and Beyond</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 13, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>9 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->dynamodb</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->aws</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->nosql</span></div></article></a><a href="/blog/zero-downtime-database-migrations/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-green-100 text-green-700">Databases</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Zero-Downtime Database Migrations: Patterns for Production</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible ‚Äî rollback a bad deploy and your code is back to the previous version. Database schema changes are st‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 8, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>8 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->database</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->migrations</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->postgresql</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">‚Üê Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS ‚Äî by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">¬© <!-- -->2026<!-- --> CodeSprintPro ¬∑ Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js ¬∑ TailwindCSS ¬∑ Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Redis Caching Strategy at Scale: Beyond Simple Key-Value","description":"Cache stampede, penetration, avalanche, eviction policy selection, clustering, and persistence trade-offs for production Redis deployments. With Java examples and a real production incident walkthrough.","date":"2025-05-03","category":"Databases","tags":["redis","caching","java","spring boot","performance","distributed systems","cache stampede"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"redis-caching-strategy-at-scale","readingTime":"11 min read","excerpt":"Every senior engineer has fought a caching bug that looked simple and turned out to be a distributed systems problem. Cache stampedes, thundering herds, avalanche failures ‚Äî these happen at scale and they are expensive. ‚Ä¶","contentHtml":"\u003cp\u003eEvery senior engineer has fought a caching bug that looked simple and turned out to be a distributed systems problem. Cache stampedes, thundering herds, avalanche failures ‚Äî these happen at scale and they are expensive. This article covers the caching patterns and anti-patterns that separate a cache that works at 100 RPS from one that holds up at 100,000 RPS.\u003c/p\u003e\n\u003ch2\u003eCache-Aside vs Write-Through vs Write-Behind\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eCache-aside (lazy loading)\u003c/strong\u003e is the default pattern: check cache, miss ‚Üí fetch from DB, populate cache, return.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Product getProduct(String productId) {\n    // 1. Check cache\n    Product cached = redisTemplate.opsForValue().get(\"product:\" + productId);\n    if (cached != null) return cached;\n\n    // 2. Cache miss: fetch from DB\n    Product product = productRepository.findById(productId)\n        .orElseThrow(() -\u003e new ProductNotFoundException(productId));\n\n    // 3. Populate cache with TTL\n    redisTemplate.opsForValue().set(\"product:\" + productId, product, Duration.ofMinutes(30));\n    return product;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eProperties:\u003c/strong\u003e Reads are fast after warmup. Cache only holds data that's actually requested (space efficient). Inconsistency window = TTL (or until next write invalidates the key). Cache cold start causes DB load spike ‚Äî important after deploys.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWrite-through\u003c/strong\u003e writes to cache and DB simultaneously on every update:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Transactional\npublic Product updateProduct(String productId, ProductUpdate update) {\n    Product product = productRepository.findById(productId).orElseThrow();\n    product.apply(update);\n    productRepository.save(product);  // Write to DB\n\n    // Immediately update cache ‚Äî no stale reads\n    redisTemplate.opsForValue().set(\"product:\" + productId, product, Duration.ofMinutes(30));\n    return product;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eProperties:\u003c/strong\u003e Cache is always fresh (no inconsistency window). Every write touches both DB and cache, even for data that's never read. Cache warms up on writes, not on reads.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWrite-behind (write-back)\u003c/strong\u003e writes to cache immediately and to the DB asynchronously:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void updateProductPrice(String productId, BigDecimal newPrice) {\n    // Update cache synchronously\n    String key = \"product:\" + productId;\n    Product product = (Product) redisTemplate.opsForValue().get(key);\n    product.setPrice(newPrice);\n    redisTemplate.opsForValue().set(key, product, Duration.ofMinutes(30));\n\n    // Schedule async DB write\n    writeQueue.submit(() -\u003e productRepository.updatePrice(productId, newPrice));\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eProperties:\u003c/strong\u003e Lowest write latency. High risk ‚Äî if Redis fails before the async write, data is lost. Use only when brief data loss is acceptable (analytics counters, view counts, non-financial metrics).\u003c/p\u003e\n\u003cp\u003eFor most production services: \u003cstrong\u003ecache-aside for reads, write-through or cache invalidation on writes\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2\u003eCache Stampede Problem\u003c/h2\u003e\n\u003cp\u003eThe cache stampede (thundering herd) happens when a popular key expires and many concurrent requests all miss the cache simultaneously, all hitting the database at once:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eT=0: Key \"hot-product-123\" expires (was serving 1000 req/s)\nT=0 to T=200ms: 200 requests miss cache, all query PostgreSQL simultaneously\nPostgreSQL: 200 concurrent queries for the same row\nResult: DB CPU spike, query queue backs up, timeouts cascade\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAt 1,000 requests/second on a key with a 30-minute TTL, when the key expires, you get ~200 simultaneous DB queries in 200ms.\u003c/p\u003e\n\u003ch3\u003eSolution 1: Probabilistic Early Expiry (PER)\u003c/h3\u003e\n\u003cp\u003eRecompute the cache before it expires, with probability proportional to how close we are to expiry. Early recompute happens in one thread while others still read the valid (slightly stale) cache:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Product getProductWithPER(String productId) {\n    String key = \"product:\" + productId;\n    CachedValue\u0026#x3C;Product\u003e cached = getWithTTL(key);\n\n    if (cached == null) {\n        return fetchAndCache(productId);\n    }\n\n    // Probabilistic early expiry\n    long remainingTtlSeconds = cached.ttlSeconds();\n    double fetchTime = 0.1; // 100ms to recompute\n    double beta = 1.0;\n\n    // Recompute if: -fetchTime * beta * ln(random) \u003e= remainingTtl\n    if (-fetchTime * beta * Math.log(Math.random()) \u003e= remainingTtlSeconds) {\n        // Early refresh ‚Äî only one thread wins this race (via lock below)\n        return fetchAndCacheIfLeader(productId);\n    }\n\n    return cached.value();\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSolution 2: Distributed Lock (Single Recompute)\u003c/h3\u003e\n\u003cp\u003eOnly one thread recomputes on cache miss; others wait or serve stale:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eprivate static final String LOCK_PREFIX = \"lock:\";\nprivate static final long LOCK_TTL_MS = 5000;\n\npublic Product getProductLocked(String productId) {\n    String valueKey = \"product:\" + productId;\n    String lockKey = LOCK_PREFIX + productId;\n\n    // Fast path: cache hit\n    Product cached = redisTemplate.opsForValue().get(valueKey);\n    if (cached != null) return cached;\n\n    // Try to acquire lock (NX = only set if not exists)\n    Boolean acquired = redisTemplate.opsForValue()\n        .setIfAbsent(lockKey, \"1\", Duration.ofMillis(LOCK_TTL_MS));\n\n    if (Boolean.TRUE.equals(acquired)) {\n        try {\n            // Double-check after acquiring lock\n            cached = redisTemplate.opsForValue().get(valueKey);\n            if (cached != null) return cached;\n\n            // We are the designated recomputer\n            Product product = productRepository.findById(productId).orElseThrow();\n            redisTemplate.opsForValue().set(valueKey, product, Duration.ofMinutes(30));\n            return product;\n        } finally {\n            redisTemplate.delete(lockKey);\n        }\n    } else {\n        // Another thread is recomputing ‚Äî wait briefly and retry\n        Uninterruptibles.sleepUninterruptibly(100, TimeUnit.MILLISECONDS);\n        return getProductLocked(productId); // Retry ‚Äî will likely hit cache now\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSolution 3: Staggered TTL\u003c/h3\u003e\n\u003cp\u003eAdd random jitter to TTLs so keys in a set don't expire simultaneously:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eprivate Duration jitteredTtl(Duration baseTtl) {\n    long jitterSeconds = ThreadLocalRandom.current()\n        .nextLong(0, baseTtl.toSeconds() / 10); // ¬±10% jitter\n    return baseTtl.plusSeconds(jitterSeconds);\n}\n\nredisTemplate.opsForValue().set(key, value, jitteredTtl(Duration.ofMinutes(30)));\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eCache Penetration and Avalanche\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eCache penetration:\u003c/strong\u003e Requests for keys that never exist (e.g., \u003ccode\u003euser_id=-1\u003c/code\u003e, or IDs for deleted entities). These always miss cache and always hit the DB.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFix:\u003c/strong\u003e Cache negative results with a short TTL, or use a Bloom filter to reject impossible keys upfront:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Optional\u0026#x3C;User\u003e getUser(Long userId) {\n    if (!bloomFilter.mightContain(userId)) {\n        return Optional.empty(); // Definitely doesn't exist\n    }\n\n    String key = \"user:\" + userId;\n    Object cached = redisTemplate.opsForValue().get(key);\n\n    if (cached instanceof NullSentinel) {\n        return Optional.empty(); // Cached negative result\n    }\n\n    if (cached instanceof User user) {\n        return Optional.of(user);\n    }\n\n    // DB lookup\n    Optional\u0026#x3C;User\u003e user = userRepository.findById(userId);\n    if (user.isEmpty()) {\n        // Cache negative result for 60 seconds\n        redisTemplate.opsForValue().set(key, NullSentinel.INSTANCE, Duration.ofSeconds(60));\n    } else {\n        redisTemplate.opsForValue().set(key, user.get(), Duration.ofMinutes(30));\n    }\n    return user;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCache avalanche:\u003c/strong\u003e Many keys expire simultaneously (same TTL set in a batch job), causing a sudden DB load spike.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFix:\u003c/strong\u003e Jitter TTLs (shown above). Alternatively, warm the cache before keys expire using a background job that refreshes keys at 80% of their TTL.\u003c/p\u003e\n\u003ch2\u003eTTL Strategy Design\u003c/h2\u003e\n\u003cp\u003eTTL selection is not guesswork ‚Äî it's a trade-off between freshness and hit rate.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTTL decision matrix:\n\nData type              | Update frequency  | Staleness tolerance | Recommended TTL\n-----------------------|-------------------|--------------------|-----------------\nProduct price          | Minutes           | Low (financial)    | 5 minutes\nProduct catalog        | Hours             | Medium             | 1 hour + invalidation\nUser profile           | Daily             | Low                | 30 minutes\nStatic content (i18n)  | Weekly            | High               | 24 hours\nSearch results         | Real-time         | High               | 5 minutes\nSession data           | Per request       | None               | Session timeout\nRate limit counters    | Per request       | None               | Window size (60s)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eInvalidation over TTL for write-heavy data:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhen an entity is updated, immediately delete (or update) its cache key rather than waiting for TTL expiry. This requires write operations to know which cache keys to invalidate ‚Äî a coupling that must be managed carefully.\u003c/p\u003e\n\u003ch2\u003eMemory Fragmentation\u003c/h2\u003e\n\u003cp\u003eRedis allocates memory using jemalloc. Over time, allocating and freeing keys of varying sizes causes fragmentation ‚Äî Redis reports 2GB used but actually occupies 3GB of system memory.\u003c/p\u003e\n\u003cp\u003eMonitor fragmentation ratio:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003eredis-cli INFO memory | grep mem_fragmentation_ratio\n# \u003e 1.5 = high fragmentation, consider restart or activedefrag\n# 1.0‚Äì1.5 = normal\n# \u0026#x3C; 1.0 = Redis is using swap (severe problem)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEnable active defragmentation for long-running Redis instances:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eactivedefrag yes\nactive-defrag-ignore-bytes 100mb  # Start defrag when 100MB fragmented\nactive-defrag-threshold-lower 10  # Start defrag when fragmentation \u003e 10%\nactive-defrag-threshold-upper 25  # Use max CPU when fragmentation \u003e 25%\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eEviction Policy Comparison\u003c/h2\u003e\n\u003cp\u003eWhen Redis reaches \u003ccode\u003emaxmemory\u003c/code\u003e, it evicts keys according to the policy:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003ePolicy\u003c/th\u003e\n\u003cth\u003eBehavior\u003c/th\u003e\n\u003cth\u003eBest For\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003enoeviction\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eReturns error on writes when full\u003c/td\u003e\n\u003ctd\u003eWhen cache full = outage (unacceptable for most)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eallkeys-lru\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eEvict least recently used across all keys\u003c/td\u003e\n\u003ctd\u003eGeneral-purpose cache\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003evolatile-lru\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eEvict LRU only from keys with TTL\u003c/td\u003e\n\u003ctd\u003eMixed cache + session store\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eallkeys-lfu\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eEvict least frequently used\u003c/td\u003e\n\u003ctd\u003eWorkloads with hotspot keys\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003evolatile-ttl\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eEvict keys closest to expiry\u003c/td\u003e\n\u003ctd\u003eWhen you want expiry-driven eviction\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eallkeys-random\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eRandom eviction\u003c/td\u003e\n\u003ctd\u003eUniform access patterns\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFor a pure cache: \u003ccode\u003eallkeys-lru\u003c/code\u003e or \u003ccode\u003eallkeys-lfu\u003c/code\u003e. \u003ccode\u003eallkeys-lfu\u003c/code\u003e is better for workloads where a small set of keys is accessed constantly (product catalog, configuration) ‚Äî LFU keeps hot keys in memory longer than LRU.\u003c/p\u003e\n\u003cp\u003eFor a mixed cache + session store: \u003ccode\u003evolatile-lru\u003c/code\u003e ‚Äî only evicts keys with TTL, protecting session keys that have no TTL.\u003c/p\u003e\n\u003ch2\u003eRedis Clustering\u003c/h2\u003e\n\u003cp\u003eRedis Cluster shards data across nodes using 16,384 hash slots:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRedis Cluster (3 primary + 3 replica):\n\nKey: \"product:123\"\nhash_slot = CRC16(\"product:123\") % 16384 = 7483\n\nSlot 7483 is owned by Primary-2\n‚Üí Route request to Primary-2\n\nPrimary-1 (slots 0-5460)          Primary-2 (slots 5461-10922)        Primary-3 (slots 10923-16383)\n     ‚îÇ                                   ‚îÇ                                    ‚îÇ\n     ‚ñº                                   ‚ñº                                    ‚ñº\nReplica-1                           Replica-2                            Replica-3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCluster limitations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMulti-key operations (\u003ccode\u003eMGET\u003c/code\u003e, \u003ccode\u003eMSET\u003c/code\u003e, pipeline) only work when all keys are in the same slot\u003c/li\u003e\n\u003cli\u003eUse hash tags \u003ccode\u003e{user:123}:profile\u003c/code\u003e to force co-location: \u003ccode\u003e{user:123}\u003c/code\u003e is used for slot calculation, so all keys with the same tag go to the same slot\u003c/li\u003e\n\u003cli\u003eTransactions (\u003ccode\u003eMULTI/EXEC\u003c/code\u003e) only work on single nodes ‚Äî avoid cross-slot transactions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Hash tags for co-located keys:\nString profileKey = \"{user:\" + userId + \"}:profile\";\nString settingsKey = \"{user:\" + userId + \"}:settings\";\n// Both keys route to the same slot ‚Üí MGET works\nList\u0026#x3C;Object\u003e results = redisTemplate.opsForValue().multiGet(\n    List.of(profileKey, settingsKey));\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003ePersistence: RDB vs AOF\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eRDB (Snapshots)\u003c/th\u003e\n\u003cth\u003eAOF (Append-Only File)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eRecovery\u003c/td\u003e\n\u003ctd\u003ePoint-in-time snapshot\u003c/td\u003e\n\u003ctd\u003eReplay every write since last snapshot\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eData loss on crash\u003c/td\u003e\n\u003ctd\u003eUp to snapshot interval (minutes)\u003c/td\u003e\n\u003ctd\u003eUp to 1 second (with fsync=everysec)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ePerformance\u003c/td\u003e\n\u003ctd\u003eLow CPU overhead\u003c/td\u003e\n\u003ctd\u003efsync adds ~10% write overhead\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eRestart time\u003c/td\u003e\n\u003ctd\u003eFast (load snapshot)\u003c/td\u003e\n\u003ctd\u003eSlow (replay log, can take minutes)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eFile size\u003c/td\u003e\n\u003ctd\u003eCompact\u003c/td\u003e\n\u003ctd\u003eGrows unbounded, requires AOF rewrite\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eFor a cache:\u003c/strong\u003e RDB only. Losing cache data on crash is acceptable ‚Äî the cache warms up from the DB.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFor a cache with session data:\u003c/strong\u003e AOF with \u003ccode\u003eappendfsync everysec\u003c/code\u003e. Max 1 second of session data loss on crash.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFor Redis as primary data store (not cache):\u003c/strong\u003e AOF with \u003ccode\u003eappendfsync always\u003c/code\u003e + RDB for backup. Maximum durability, highest write overhead.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# redis.conf for production cache:\nmaxmemory 12gb\nmaxmemory-policy allkeys-lfu\nsave 900 1     # RDB snapshot if 1 key changed in 900s\nsave 300 10    # RDB snapshot if 10 keys changed in 300s\nsave 60 10000  # RDB snapshot if 10000 keys changed in 60s\nappendonly no  # No AOF for pure cache\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eDistributed Locks with Redis\u003c/h2\u003e\n\u003cp\u003eRedis Redlock is the distributed lock algorithm. For single-node Redis or Redis Cluster, a simpler approach:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic \u0026#x3C;T\u003e T withLock(String resource, Duration timeout, Supplier\u0026#x3C;T\u003e task) {\n    String lockKey = \"lock:\" + resource;\n    String lockValue = UUID.randomUUID().toString(); // Unique per lock acquisition\n    boolean acquired = false;\n\n    try {\n        acquired = Boolean.TRUE.equals(\n            redisTemplate.opsForValue()\n                .setIfAbsent(lockKey, lockValue, timeout)\n        );\n\n        if (!acquired) {\n            throw new LockNotAvailableException(\"Resource locked: \" + resource);\n        }\n\n        return task.get();\n    } finally {\n        if (acquired) {\n            // Release only if we own the lock (Lua script ensures atomicity)\n            redisTemplate.execute(\n                RELEASE_LOCK_SCRIPT,\n                Collections.singletonList(lockKey),\n                lockValue\n            );\n        }\n    }\n}\n\n// Lua script for atomic check-and-delete:\nprivate static final RedisScript\u0026#x3C;Long\u003e RELEASE_LOCK_SCRIPT = RedisScript.of(\n    \"if redis.call('get', KEYS[1]) == ARGV[1] then \" +\n    \"    return redis.call('del', KEYS[1]) \" +\n    \"else return 0 end\",\n    Long.class\n);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe Lua script is critical. Without it, two operations occur: \u003ccode\u003eGET\u003c/code\u003e to verify ownership, then \u003ccode\u003eDEL\u003c/code\u003e. A different process could acquire the lock between those two operations, and you'd delete their lock.\u003c/p\u003e\n\u003ch2\u003eReal World Production Issue\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eSystem:\u003c/strong\u003e E-commerce product catalog service, 50,000 SKUs, Redis Cluster (6 nodes), Spring Boot.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIncident:\u003c/strong\u003e Flash sale launch. At 12:00:00, 50,000 concurrent users loaded the sale page. Cache hit rate: 98%. The remaining 2% ‚Äî 1,000 users ‚Äî missed on the most popular sale items because those specific keys had expired at 11:59:58 due to a batch TTL reset job.\u003c/p\u003e\n\u003cp\u003eAll 1,000 users simultaneously queried PostgreSQL for 3 products. PostgreSQL's connection pool (20 connections) was saturated. Other queries (cart, checkout) backed up. API P99 hit 45 seconds.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRoot causes:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBatch job set the same TTL on all keys ‚Üí mass expiry\u003c/li\u003e\n\u003cli\u003eNo stampede protection\u003c/li\u003e\n\u003cli\u003ePostgreSQL connection pool too small for burst\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eFixes applied:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eJitter added to all TTL values (¬±10%)\u003c/li\u003e\n\u003cli\u003eProbabilistic early expiry for top-100 accessed keys\u003c/li\u003e\n\u003cli\u003ePostgreSQL connection pool increased to 50\u003c/li\u003e\n\u003cli\u003eRead replicas added; cache miss reads route to replicas\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSale metrics before/after fix: Cache stampede incidents dropped from 8/month to 0.\u003c/p\u003e\n\u003ch2\u003eMonitoring Redis Memory and CPU\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Key memory metrics:\nredis-cli INFO memory\n# used_memory_human: 8.50G     ‚Üí actual data\n# maxmemory_human: 12.00G      ‚Üí limit\n# mem_fragmentation_ratio: 1.12 ‚Üí healthy\n# used_memory_rss_human: 9.54G  ‚Üí OS-reported\n\n# Eviction monitoring:\nredis-cli INFO stats | grep evicted_keys\n# Rising number = memory pressure, increase maxmemory or evict manually\n\n# Slow query log:\nredis-cli CONFIG SET slowlog-log-slower-than 10000  # 10ms threshold\nredis-cli SLOWLOG GET 25  # View last 25 slow commands\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePrometheus alert rules:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003e# Memory usage \u003e 80% of maxmemory\nredis_memory_used_bytes / redis_memory_max_bytes \u003e 0.8\n\n# High eviction rate (cache under memory pressure)\nrate(redis_evicted_keys_total[5m]) \u003e 100\n\n# Cache hit rate dropping\nredis_keyspace_hits_total / (redis_keyspace_hits_total + redis_keyspace_misses_total) \u0026#x3C; 0.90\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe difference between a cache that helps you and one that causes your worst production incidents is usually 3 things: TTL jitter, stampede protection, and eviction policy selection. Everything else is tuning.\u003c/p\u003e\n","tableOfContents":[{"id":"cache-aside-vs-write-through-vs-write-behind","text":"Cache-Aside vs Write-Through vs Write-Behind","level":2},{"id":"cache-stampede-problem","text":"Cache Stampede Problem","level":2},{"id":"solution-1-probabilistic-early-expiry-per","text":"Solution 1: Probabilistic Early Expiry (PER)","level":3},{"id":"solution-2-distributed-lock-single-recompute","text":"Solution 2: Distributed Lock (Single Recompute)","level":3},{"id":"solution-3-staggered-ttl","text":"Solution 3: Staggered TTL","level":3},{"id":"cache-penetration-and-avalanche","text":"Cache Penetration and Avalanche","level":2},{"id":"ttl-strategy-design","text":"TTL Strategy Design","level":2},{"id":"memory-fragmentation","text":"Memory Fragmentation","level":2},{"id":"eviction-policy-comparison","text":"Eviction Policy Comparison","level":2},{"id":"redis-clustering","text":"Redis Clustering","level":2},{"id":"persistence-rdb-vs-aof","text":"Persistence: RDB vs AOF","level":2},{"id":"distributed-locks-with-redis","text":"Distributed Locks with Redis","level":2},{"id":"real-world-production-issue","text":"Real World Production Issue","level":2},{"id":"monitoring-redis-memory-and-cpu","text":"Monitoring Redis Memory and CPU","level":2}]},"relatedPosts":[{"title":"Cassandra Data Modeling: Design for Queries, Not Entities","description":"Apache Cassandra data modeling from first principles: partition key design, clustering columns, denormalization strategies, avoiding hot partitions, materialized views vs. manual duplication, and the anti-patterns that kill Cassandra performance.","date":"2025-06-18","category":"Databases","tags":["cassandra","nosql","data modeling","distributed databases","partition key","cql","time series"],"featured":false,"affiliateSection":"database-resources","slug":"cassandra-data-modeling","readingTime":"9 min read","excerpt":"Cassandra is a write-optimized distributed database built for linear horizontal scalability. It stores data in a distributed hash ring ‚Äî every node is equal, there's no primary, and data placement is determined by partit‚Ä¶"},{"title":"DynamoDB Advanced Patterns: Single-Table Design and Beyond","description":"Production DynamoDB: single-table design with access pattern mapping, GSI overloading, sparse indexes, adjacency lists for graph relationships, DynamoDB Streams for event-driven architectures, and the read/write capacity math that prevents bill shock.","date":"2025-06-13","category":"Databases","tags":["dynamodb","aws","nosql","single-table design","gsi","dynamodb streams","serverless"],"featured":false,"affiliateSection":"database-resources","slug":"dynamodb-advanced-patterns","readingTime":"9 min read","excerpt":"DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. It achieves this with a fundamental constraint: you must define your access patterns before you‚Ä¶"},{"title":"Zero-Downtime Database Migrations: Patterns for Production","description":"How to safely migrate production databases without downtime: expand-contract pattern, backward-compatible schema changes, rolling deployments with dual-write, column renaming strategies, and the PostgreSQL-specific techniques for large table alterations.","date":"2025-06-08","category":"Databases","tags":["database","migrations","postgresql","zero downtime","devops","schema evolution","flyway","liquibase"],"featured":false,"affiliateSection":"database-resources","slug":"zero-downtime-database-migrations","readingTime":"8 min read","excerpt":"Database migrations are the most dangerous part of a deployment. Application code changes are stateless and reversible ‚Äî rollback a bad deploy and your code is back to the previous version. Database schema changes are st‚Ä¶"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"redis-caching-strategy-at-scale"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>