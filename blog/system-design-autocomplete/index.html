<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">System Design: Search Autocomplete at Google Scale<!-- --> | CodeSprintPro</title><meta name="description" content="Design a typeahead/autocomplete system that returns relevant suggestions in under 100ms for billions of queries. Covers trie vs inverted index, ranking algorithms, and distributed architecture." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/system-design-autocomplete/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="System Design: Search Autocomplete at Google Scale" data-next-head=""/><meta property="og:description" content="Design a typeahead/autocomplete system that returns relevant suggestions in under 100ms for billions of queries. Covers trie vs inverted index, ranking algorithms, and distributed architecture." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/system-design-autocomplete/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-02-20" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="System Design" data-next-head=""/><meta property="article:tag" content="system design" data-next-head=""/><meta property="article:tag" content="search" data-next-head=""/><meta property="article:tag" content="autocomplete" data-next-head=""/><meta property="article:tag" content="trie" data-next-head=""/><meta property="article:tag" content="distributed systems" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="System Design: Search Autocomplete at Google Scale" data-next-head=""/><meta name="twitter:description" content="Design a typeahead/autocomplete system that returns relevant suggestions in under 100ms for billions of queries. Covers trie vs inverted index, ranking algorithms, and distributed architecture." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"System Design: Search Autocomplete at Google Scale","description":"Design a typeahead/autocomplete system that returns relevant suggestions in under 100ms for billions of queries. Covers trie vs inverted index, ranking algorithms, and distributed architecture.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-02-20","dateModified":"2025-02-20","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/system-design-autocomplete/"},"keywords":"system design, search, autocomplete, trie, distributed systems","articleSection":"System Design"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">System Design: Search Autocomplete at Google Scale</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">System Design</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">System Design: Search Autocomplete at Google Scale</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Design a typeahead/autocomplete system that returns relevant suggestions in under 100ms for billions of queries. Covers trie vs inverted index, ranking algorithms, and distributed architecture.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>Â·</span><span>February 20, 2025</span><span>Â·</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->system design</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->search</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->autocomplete</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->trie</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->distributed systems</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Search autocomplete â€” the dropdown that appears as you type â€” seems simple but is one of the most latency-sensitive features in any product. Google returns suggestions in under 100ms for billions of queries per day. This article designs the system behind that.</p>
<h2>Requirements</h2>
<p><strong>Functional:</strong></p>
<ul>
<li>Return top 5 suggestions as the user types (after each keystroke)</li>
<li>Suggestions ranked by historical query frequency + recency</li>
<li>Personalized suggestions (user's history)</li>
<li>Support for typo tolerance (fuzzy matching)</li>
<li>Trending queries bubble up quickly</li>
</ul>
<p><strong>Non-Functional:</strong></p>
<ul>
<li>10B daily queries â†’ 115,000 queries/sec</li>
<li>Latency p99 &#x3C; 100ms (including network round-trip)</li>
<li>High availability (99.99%)</li>
<li>Suggestions updated from query logs within 10 minutes (near real-time)</li>
</ul>
<h2>Data Model: What Are We Searching?</h2>
<p>Before choosing a data structure, you need to understand what the input and output of the system actually are. The system receives a user's partially typed query and must return the five most relevant completions. The "most relevant" part is not about text matching â€” it is about predicting what the user intends to type, which requires a scoring model built from historical behavior.</p>
<pre><code>Query log (source of truth):
  Each search query is recorded with timestamp, user_id, result_click_count.

Aggregation pipeline:
  Raw logs â†’ Count per query â†’ Filter noise â†’ Rank â†’ Index

Ranked query store:
  query: "java virtual threads"
  score: 8,432,100         (weighted: frequency Ã— recency Ã— CTR)
  updated_at: 2025-02-20

Goal: Given prefix "java v", return:
  1. java virtual threads
  2. java versions
  3. java volatile keyword
  4. java vector api
  5. java var keyword
</code></pre>
<p>The score formula <code>frequency Ã— recency Ã— CTR</code> is the key insight here: a query that was searched a million times two years ago should not outrank a query searched 100,000 times in the last hour if the recent one shows high click-through rate. Weighting these three signals together produces suggestions that feel current and useful rather than historically accurate but stale.</p>
<h2>Core Data Structure: Trie vs Inverted Index</h2>
<p>With the data model defined, the next question is how to index it so that a prefix lookup returns the top-5 completions in under 10ms. There are two fundamentally different approaches, and understanding their tradeoffs is what interviewers are really testing here.</p>
<h3>Option 1: Trie (Prefix Tree)</h3>
<p>A trie is a tree where each path from root to a leaf spells out a string. It is the most natural data structure for prefix lookups, but its real power â€” and its main limitation â€” comes from how you store suggestions at each node.</p>
<pre><code>Trie for ["java", "java virtual", "javascript"]:

root
 â””â”€ j
    â””â”€ a
       â””â”€ v
          â”œâ”€ a [end: score=8M]
          â”‚  â””â”€  [space]
          â”‚      â””â”€ v
          â”‚         â””â”€ i
          â”‚            â””â”€ r [end: score=8.4M]
          â””â”€ a
             â””â”€ s [end: score=12M]
</code></pre>
<p>Each node can store the top-K suggestions for that prefix (precomputed). Lookup: O(prefix_length). Memory: O(total characters Ã— K suggestions per node).</p>
<p>The critical insight of storing top-K suggestions at every node is what makes the trie usable for autocomplete: instead of traversing all children to find the best suggestions at query time, you precompute the answer during index build and store it directly at the node. A query for "java v" returns results in exactly as many steps as there are characters in the prefix.</p>
<pre><code class="language-java">class TrieNode {
    Map&#x3C;Character, TrieNode> children = new HashMap&#x3C;>();
    // Store top-K (e.g., 5) suggestions at this node â€” avoids tree traversal on query
    PriorityQueue&#x3C;Suggestion> topK = new PriorityQueue&#x3C;>(Comparator.comparingLong(Suggestion::getScore));
    boolean isEnd;
}

class AutocompleteTrie {

    private final TrieNode root = new TrieNode();
    private final int K = 5;

    public void insert(String query, long score) {
        TrieNode node = root;
        for (char c : query.toCharArray()) {
            node.children.putIfAbsent(c, new TrieNode());
            node = node.children.get(c);
            updateTopK(node, new Suggestion(query, score));
        }
        node.isEnd = true;
    }

    private void updateTopK(TrieNode node, Suggestion suggestion) {
        node.topK.offer(suggestion);
        if (node.topK.size() > K) {
            node.topK.poll(); // Remove lowest score
        }
    }

    public List&#x3C;String> search(String prefix) {
        TrieNode node = root;
        for (char c : prefix.toCharArray()) {
            node = node.children.get(c);
            if (node == null) return Collections.emptyList();
        }
        // Top-K already precomputed at this node
        return node.topK.stream()
            .sorted(Comparator.comparingLong(Suggestion::getScore).reversed())
            .map(Suggestion::getQuery)
            .collect(Collectors.toList());
    }
}
</code></pre>
<p>The <code>PriorityQueue</code> used as a min-heap of size K is the right choice here: when you call <code>poll()</code>, it removes the lowest-scoring suggestion, so after processing all inserts you are left with the K highest-scoring ones. This gives you O(log K) insert time at each node, which is effectively constant since K is fixed at 5.</p>
<p><strong>Trie pros/cons:</strong></p>
<ul>
<li>Pros: O(L) lookup where L = prefix length, perfect for prefix matching</li>
<li>Cons: Memory-intensive for large vocabularies, difficult to update incrementally, no fuzzy matching</li>
</ul>
<h3>Option 2: Elasticsearch with Edge N-Grams (Production Choice)</h3>
<p>The trie is elegant for teaching, but production systems at Google or LinkedIn scale choose Elasticsearch because it combines prefix search, typo tolerance, and popularity ranking in a single system that is horizontally scalable. The edge n-gram analyzer is the key configuration: it pre-indexes every prefix of every query term at index time, so a search for "java v" matches any document that contains a token starting with "java v" â€” without any trie traversal at all.</p>
<p>For production at scale, Elasticsearch handles prefix search, typo tolerance, and ranking in one system:</p>
<pre><code class="language-json">// Index mapping with edge n-gram analyzer
{
  "settings": {
    "analysis": {
      "analyzer": {
        "autocomplete_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["lowercase", "autocomplete_filter"]
        },
        "search_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["lowercase"]
        }
      },
      "filter": {
        "autocomplete_filter": {
          "type": "edge_ngram",
          "min_gram": 1,
          "max_gram": 20
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "query": {
        "type": "text",
        "analyzer": "autocomplete_analyzer",
        "search_analyzer": "search_analyzer"
      },
      "score": { "type": "long" },
      "updated_at": { "type": "date" }
    }
  }
}
</code></pre>
<p>Notice that <code>autocomplete_analyzer</code> is used at index time but <code>search_analyzer</code> (without the edge n-gram filter) is used at search time. This asymmetry is intentional: you want to store all prefixes in the index, but at search time you want to match the user's typed prefix as-is against those stored tokens.</p>
<p>With the index configured to handle prefix matching, the query below adds a second layer: it boosts documents by their historical score and applies a time-decay so that recently trending queries rank higher than equally popular but older ones.</p>
<pre><code class="language-json">// Query: prefix "java v" with boost for recency
{
  "query": {
    "function_score": {
      "query": { "match": { "query": "java v" } },
      "functions": [
        { "field_value_factor": { "field": "score", "modifier": "log1p", "factor": 1 } },
        {
          "gauss": {
            "updated_at": { "origin": "now", "scale": "7d", "decay": 0.5 }
          }
        }
      ],
      "boost_mode": "multiply"
    }
  },
  "size": 5
}
</code></pre>
<p>The <code>gauss</code> decay function is what makes trending queries surface quickly: a query updated today gets a decay score of 1.0, while a query updated 7 days ago gets a score of 0.5 (<code>decay</code> parameter), and 14 days ago about 0.25. Multiplied by the frequency score, this ensures a viral query can jump from page 3 to position 1 within hours of spiking.</p>
<h2>Distributed Architecture</h2>
<p>With the core search logic defined, the architecture adds a caching layer in front of Elasticsearch. This is not optional: at 115,000 queries per second, Elasticsearch would need hundreds of nodes to handle the full load. Caching popular prefixes in Redis and CDN reduces the load Elasticsearch actually sees to a small fraction of total traffic.</p>
<pre><code>Client                CDN                 API          Redis          Elasticsearch
  â”‚                    â”‚                   â”‚              â”‚                â”‚
  â”œâ”€ type "j" â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€ cache hit? â”€â”€â”€â”€â–ºâ”‚              â”‚                â”‚
  â”‚                    â”‚    YES: return     â”‚              â”‚                â”‚
  â”‚â—„â”€â”€â”€â”€â”€â”€ ["java"]â”€â”€â”€â”€â”¤                   â”‚              â”‚                â”‚
  â”‚                    â”‚                   â”‚              â”‚                â”‚
  â”œâ”€ type "ja" â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€ miss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€ get("ja") â”€â–ºâ”‚                â”‚
  â”‚                    â”‚                   â”‚â—„â”€ ["java"] â”€â”€â”¤                â”‚
  â”‚â—„â”€ ["java", "java"] â”¤â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚                â”‚
  â”‚                    â”‚                   â”‚              â”‚                â”‚
  â”œâ”€ type "jav" â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€ miss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€ miss â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€ search("jav")â–ºâ”‚
  â”‚                    â”‚                   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚â—„â”€ [suggestions] â”€â”€â”€â”¤â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚                â”‚
</code></pre>
<p><strong>Caching strategy:</strong></p>
<ul>
<li>CDN (CloudFront): Cache responses for common prefixes ("a", "th", "he" â€” ~80% of traffic)</li>
<li>Redis: Cache prefix â†’ suggestions with 5-minute TTL</li>
<li>Cache key: <code>suggest:{lang}:{prefix}</code> (normalize: lowercase, trim)</li>
</ul>
<p>The short 5-minute TTL in Redis is deliberate: it ensures that trending queries â€” which your pipeline updates every minute â€” propagate to users within 5 minutes of spiking, even for cached prefixes. A longer TTL would make the system more cache-efficient but less responsive to trends.</p>
<pre><code class="language-java">@Service
public class AutocompleteService {

    @Autowired
    private StringRedisTemplate redis;

    @Autowired
    private ElasticsearchClient es;

    public List&#x3C;String> suggest(String prefix, String locale) {
        String normalized = prefix.toLowerCase().trim();
        if (normalized.length() &#x3C; 2) return Collections.emptyList(); // Min 2 chars

        String cacheKey = "suggest:" + locale + ":" + normalized;
        String cached = redis.opsForValue().get(cacheKey);

        if (cached != null) {
            return objectMapper.readValue(cached, List.class);
        }

        List&#x3C;String> suggestions = searchElasticsearch(normalized, locale);

        redis.opsForValue().set(cacheKey, objectMapper.writeValueAsString(suggestions),
            Duration.ofMinutes(5));

        return suggestions;
    }
}
</code></pre>
<p>The minimum prefix length of 2 characters is a practical optimization: single-character prefixes like "a" or "t" would match millions of queries and are too ambiguous to be useful, while their cache entries would occupy disproportionate memory. By skipping them, you eliminate a class of expensive queries with low signal.</p>
<h2>Keeping Suggestions Fresh: Real-Time Updates</h2>
<p>With the serving layer in place, you need a pipeline that continuously feeds new query data back into the index. The challenge is balancing freshness (how quickly a viral query appears) against noise (a query that spikes once due to a bot should not permanently pollute the index).</p>
<p>Query logs are processed to update suggestion scores:</p>
<pre><code>Pipeline:
  User searches â†’ App logs query â†’ Kafka topic "search-queries"
      â†’ Flink/Spark aggregation (5-minute windows)
      â†’ Top queries with updated scores
      â†’ Update Elasticsearch + rebuild Redis cache

Frequency:
  Trend detection: 1-minute windows (detect viral queries immediately)
  Full re-rank: 10-minute windows (stabilize rankings)
  Full index rebuild: Daily (garbage collect dead queries)
</code></pre>
<p>The three-tier frequency schedule is the key design insight here: 1-minute windows for trend detection mean a breaking news query surfaces within 60 seconds, while the daily full rebuild prunes queries that trended briefly and are now dead weight in the index.</p>
<pre><code class="language-java">// Kafka Streams aggregation
KStream&#x3C;String, SearchEvent> searches = builder.stream("search-queries");

KTable&#x3C;String, Long> queryCounts = searches
    .groupBy((key, event) -> event.getNormalizedQuery())
    .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofMinutes(5)))
    .count();

queryCounts.toStream()
    .map((window, count) -> KeyValue.pair(window.key(), count))
    .to("query-scores", Produced.with(Serdes.String(), Serdes.Long()));
</code></pre>
<p><code>ofSizeWithNoGrace</code> is worth understanding: the "no grace period" setting means the window closes immediately at the 5-minute mark and emits its result without waiting for late-arriving events. For autocomplete scoring, this tradeoff is correct â€” a small number of late events does not materially change query rankings, and lower latency is more valuable than perfect accuracy.</p>
<h2>Personalization</h2>
<p>Global suggestions are a strong baseline, but users who have searched for "java concurrency" three times this week should see Java-related completions ranked above unrelated queries. The personalization layer blends a small number of personal suggestions into the global top-5, placing them first so they appear immediately when relevant.</p>
<p>Personal suggestions boost queries from the user's search history:</p>
<pre><code class="language-java">public List&#x3C;String> suggestPersonalized(String prefix, String userId) {
    // Blend global suggestions with personal history
    List&#x3C;String> global = suggest(prefix, "en");

    List&#x3C;String> personal = userHistoryService.getMatchingHistory(userId, prefix, 3);

    // Merge: personal first (max 2), then global (fill remaining 3)
    return Stream.concat(personal.stream(), global.stream())
        .distinct()
        .limit(5)
        .collect(Collectors.toList());
}
</code></pre>
<p>Capping personal suggestions at 2 out of 5 ensures the global ranking still dominates. If you let personalization dominate entirely, users in a narrow interest category would see increasingly narrow suggestions, a filter-bubble effect that reduces discovery of new topics.</p>
<h2>Typo Tolerance</h2>
<p>Even with perfect indexing and caching, users make typos. Without fuzzy matching, a user typing "jva virtual" would see no suggestions at all. Elasticsearch's built-in <code>fuzziness</code> setting handles this by finding documents within a configurable edit distance from the typed query.</p>
<p>Use Elasticsearch's fuzzy matching for queries with typos:</p>
<pre><code class="language-json">{
  "query": {
    "multi_match": {
      "query": "jva virtual",
      "fields": ["query"],
      "fuzziness": "AUTO",     // 0 edits for 1-2 chars, 1 for 3-5, 2 for 6+
      "prefix_length": 2,      // First 2 chars must match exactly (performance)
      "max_expansions": 50
    }
  }
}
</code></pre>
<p>The <code>prefix_length: 2</code> parameter is a critical performance guard: it tells Elasticsearch that the first two characters must match exactly, which dramatically reduces the search space for fuzzy expansion. Without this, "AUTO" fuzziness on a short query like "jv" could expand to thousands of candidate terms and make every keystroke slow.</p>
<p>The difference between a good autocomplete and a great one is the ranking function. Frequency alone gives stale results. Recency alone gives noisy trending results. The combination â€” frequency Ã— recency Ã— click-through rate â€” matches user intent. Instrument your system to measure suggestion acceptance rate and use that signal to continuously improve rankings.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">ğŸ“š</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">System Design Interview â€” Alex Xu</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Best Seller</span></div><p class="text-xs text-gray-600">Step-by-step guide to ace system design interviews with real-world examples.</p></div><a href="https://amzn.to/3TqsPRp" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> â†’</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Grokking System Design on Educative</span></div><p class="text-xs text-gray-600">Interactive course teaching system design with visual diagrams and practice problems.</p></div><a href="https://www.educative.io/courses/grokking-the-system-design-interview" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> â†’</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Designing Data-Intensive Applications</span></div><p class="text-xs text-gray-600">Martin Kleppmann&#x27;s book is essential reading for any system design role.</p></div><a href="https://amzn.to/3RyKzOA" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> â†’</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=System%20Design%3A%20Search%20Autocomplete%20at%20Google%20Scale&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fsystem-design-autocomplete%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fsystem-design-autocomplete%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#requirements" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Requirements</a></li><li class=""><a href="#data-model-what-are-we-searching" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Data Model: What Are We Searching?</a></li><li class=""><a href="#core-data-structure-trie-vs-inverted-index" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Core Data Structure: Trie vs Inverted Index</a></li><li class="ml-4"><a href="#option-1-trie-prefix-tree" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Option 1: Trie (Prefix Tree)</a></li><li class="ml-4"><a href="#option-2-elasticsearch-with-edge-n-grams-production-choice" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Option 2: Elasticsearch with Edge N-Grams (Production Choice)</a></li><li class=""><a href="#distributed-architecture" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Distributed Architecture</a></li><li class=""><a href="#keeping-suggestions-fresh-real-time-updates" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Keeping Suggestions Fresh: Real-Time Updates</a></li><li class=""><a href="#personalization" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Personalization</a></li><li class=""><a href="#typo-tolerance" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Typo Tolerance</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/observability-opentelemetry-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Building Production Observability with OpenTelemetry and Grafana Stack</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why â€” by exploring system state through metrics, traces, and logs without needing to know in advanceâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jul 3, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>6 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->observability</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->opentelemetry</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->prometheus</span></div></article></a><a href="/blog/event-sourcing-cqrs-production/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Event Sourcing and CQRS in Production: Beyond the Theory</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory â€” store events instead of state, derive state by replaying events â€” is souâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 23, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->event sourcing</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->cqrs</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->system design</span></div></article></a><a href="/blog/grpc-vs-rest-vs-graphql/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-blue-100 text-blue-700">System Design</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">gRPC vs REST vs GraphQL: Choosing the Right API Protocol</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to tâ€¦</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 18, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->grpc</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->rest</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->graphql</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">â† Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS â€” by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">Â© <!-- -->2026<!-- --> CodeSprintPro Â· Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js Â· TailwindCSS Â· Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"System Design: Search Autocomplete at Google Scale","description":"Design a typeahead/autocomplete system that returns relevant suggestions in under 100ms for billions of queries. Covers trie vs inverted index, ranking algorithms, and distributed architecture.","date":"2025-02-20","category":"System Design","tags":["system design","search","autocomplete","trie","distributed systems"],"featured":false,"affiliateSection":"system-design-courses","slug":"system-design-autocomplete","readingTime":"11 min read","excerpt":"Search autocomplete â€” the dropdown that appears as you type â€” seems simple but is one of the most latency-sensitive features in any product. Google returns suggestions in under 100ms for billions of queries per day. Thisâ€¦","contentHtml":"\u003cp\u003eSearch autocomplete â€” the dropdown that appears as you type â€” seems simple but is one of the most latency-sensitive features in any product. Google returns suggestions in under 100ms for billions of queries per day. This article designs the system behind that.\u003c/p\u003e\n\u003ch2\u003eRequirements\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eFunctional:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReturn top 5 suggestions as the user types (after each keystroke)\u003c/li\u003e\n\u003cli\u003eSuggestions ranked by historical query frequency + recency\u003c/li\u003e\n\u003cli\u003ePersonalized suggestions (user's history)\u003c/li\u003e\n\u003cli\u003eSupport for typo tolerance (fuzzy matching)\u003c/li\u003e\n\u003cli\u003eTrending queries bubble up quickly\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNon-Functional:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e10B daily queries â†’ 115,000 queries/sec\u003c/li\u003e\n\u003cli\u003eLatency p99 \u0026#x3C; 100ms (including network round-trip)\u003c/li\u003e\n\u003cli\u003eHigh availability (99.99%)\u003c/li\u003e\n\u003cli\u003eSuggestions updated from query logs within 10 minutes (near real-time)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eData Model: What Are We Searching?\u003c/h2\u003e\n\u003cp\u003eBefore choosing a data structure, you need to understand what the input and output of the system actually are. The system receives a user's partially typed query and must return the five most relevant completions. The \"most relevant\" part is not about text matching â€” it is about predicting what the user intends to type, which requires a scoring model built from historical behavior.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eQuery log (source of truth):\n  Each search query is recorded with timestamp, user_id, result_click_count.\n\nAggregation pipeline:\n  Raw logs â†’ Count per query â†’ Filter noise â†’ Rank â†’ Index\n\nRanked query store:\n  query: \"java virtual threads\"\n  score: 8,432,100         (weighted: frequency Ã— recency Ã— CTR)\n  updated_at: 2025-02-20\n\nGoal: Given prefix \"java v\", return:\n  1. java virtual threads\n  2. java versions\n  3. java volatile keyword\n  4. java vector api\n  5. java var keyword\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe score formula \u003ccode\u003efrequency Ã— recency Ã— CTR\u003c/code\u003e is the key insight here: a query that was searched a million times two years ago should not outrank a query searched 100,000 times in the last hour if the recent one shows high click-through rate. Weighting these three signals together produces suggestions that feel current and useful rather than historically accurate but stale.\u003c/p\u003e\n\u003ch2\u003eCore Data Structure: Trie vs Inverted Index\u003c/h2\u003e\n\u003cp\u003eWith the data model defined, the next question is how to index it so that a prefix lookup returns the top-5 completions in under 10ms. There are two fundamentally different approaches, and understanding their tradeoffs is what interviewers are really testing here.\u003c/p\u003e\n\u003ch3\u003eOption 1: Trie (Prefix Tree)\u003c/h3\u003e\n\u003cp\u003eA trie is a tree where each path from root to a leaf spells out a string. It is the most natural data structure for prefix lookups, but its real power â€” and its main limitation â€” comes from how you store suggestions at each node.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTrie for [\"java\", \"java virtual\", \"javascript\"]:\n\nroot\n â””â”€ j\n    â””â”€ a\n       â””â”€ v\n          â”œâ”€ a [end: score=8M]\n          â”‚  â””â”€  [space]\n          â”‚      â””â”€ v\n          â”‚         â””â”€ i\n          â”‚            â””â”€ r [end: score=8.4M]\n          â””â”€ a\n             â””â”€ s [end: score=12M]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEach node can store the top-K suggestions for that prefix (precomputed). Lookup: O(prefix_length). Memory: O(total characters Ã— K suggestions per node).\u003c/p\u003e\n\u003cp\u003eThe critical insight of storing top-K suggestions at every node is what makes the trie usable for autocomplete: instead of traversing all children to find the best suggestions at query time, you precompute the answer during index build and store it directly at the node. A query for \"java v\" returns results in exactly as many steps as there are characters in the prefix.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003eclass TrieNode {\n    Map\u0026#x3C;Character, TrieNode\u003e children = new HashMap\u0026#x3C;\u003e();\n    // Store top-K (e.g., 5) suggestions at this node â€” avoids tree traversal on query\n    PriorityQueue\u0026#x3C;Suggestion\u003e topK = new PriorityQueue\u0026#x3C;\u003e(Comparator.comparingLong(Suggestion::getScore));\n    boolean isEnd;\n}\n\nclass AutocompleteTrie {\n\n    private final TrieNode root = new TrieNode();\n    private final int K = 5;\n\n    public void insert(String query, long score) {\n        TrieNode node = root;\n        for (char c : query.toCharArray()) {\n            node.children.putIfAbsent(c, new TrieNode());\n            node = node.children.get(c);\n            updateTopK(node, new Suggestion(query, score));\n        }\n        node.isEnd = true;\n    }\n\n    private void updateTopK(TrieNode node, Suggestion suggestion) {\n        node.topK.offer(suggestion);\n        if (node.topK.size() \u003e K) {\n            node.topK.poll(); // Remove lowest score\n        }\n    }\n\n    public List\u0026#x3C;String\u003e search(String prefix) {\n        TrieNode node = root;\n        for (char c : prefix.toCharArray()) {\n            node = node.children.get(c);\n            if (node == null) return Collections.emptyList();\n        }\n        // Top-K already precomputed at this node\n        return node.topK.stream()\n            .sorted(Comparator.comparingLong(Suggestion::getScore).reversed())\n            .map(Suggestion::getQuery)\n            .collect(Collectors.toList());\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003ePriorityQueue\u003c/code\u003e used as a min-heap of size K is the right choice here: when you call \u003ccode\u003epoll()\u003c/code\u003e, it removes the lowest-scoring suggestion, so after processing all inserts you are left with the K highest-scoring ones. This gives you O(log K) insert time at each node, which is effectively constant since K is fixed at 5.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTrie pros/cons:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePros: O(L) lookup where L = prefix length, perfect for prefix matching\u003c/li\u003e\n\u003cli\u003eCons: Memory-intensive for large vocabularies, difficult to update incrementally, no fuzzy matching\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eOption 2: Elasticsearch with Edge N-Grams (Production Choice)\u003c/h3\u003e\n\u003cp\u003eThe trie is elegant for teaching, but production systems at Google or LinkedIn scale choose Elasticsearch because it combines prefix search, typo tolerance, and popularity ranking in a single system that is horizontally scalable. The edge n-gram analyzer is the key configuration: it pre-indexes every prefix of every query term at index time, so a search for \"java v\" matches any document that contains a token starting with \"java v\" â€” without any trie traversal at all.\u003c/p\u003e\n\u003cp\u003eFor production at scale, Elasticsearch handles prefix search, typo tolerance, and ranking in one system:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e// Index mapping with edge n-gram analyzer\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"autocomplete_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\"lowercase\", \"autocomplete_filter\"]\n        },\n        \"search_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\"lowercase\"]\n        }\n      },\n      \"filter\": {\n        \"autocomplete_filter\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 1,\n          \"max_gram\": 20\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"query\": {\n        \"type\": \"text\",\n        \"analyzer\": \"autocomplete_analyzer\",\n        \"search_analyzer\": \"search_analyzer\"\n      },\n      \"score\": { \"type\": \"long\" },\n      \"updated_at\": { \"type\": \"date\" }\n    }\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNotice that \u003ccode\u003eautocomplete_analyzer\u003c/code\u003e is used at index time but \u003ccode\u003esearch_analyzer\u003c/code\u003e (without the edge n-gram filter) is used at search time. This asymmetry is intentional: you want to store all prefixes in the index, but at search time you want to match the user's typed prefix as-is against those stored tokens.\u003c/p\u003e\n\u003cp\u003eWith the index configured to handle prefix matching, the query below adds a second layer: it boosts documents by their historical score and applies a time-decay so that recently trending queries rank higher than equally popular but older ones.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e// Query: prefix \"java v\" with boost for recency\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": { \"match\": { \"query\": \"java v\" } },\n      \"functions\": [\n        { \"field_value_factor\": { \"field\": \"score\", \"modifier\": \"log1p\", \"factor\": 1 } },\n        {\n          \"gauss\": {\n            \"updated_at\": { \"origin\": \"now\", \"scale\": \"7d\", \"decay\": 0.5 }\n          }\n        }\n      ],\n      \"boost_mode\": \"multiply\"\n    }\n  },\n  \"size\": 5\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003egauss\u003c/code\u003e decay function is what makes trending queries surface quickly: a query updated today gets a decay score of 1.0, while a query updated 7 days ago gets a score of 0.5 (\u003ccode\u003edecay\u003c/code\u003e parameter), and 14 days ago about 0.25. Multiplied by the frequency score, this ensures a viral query can jump from page 3 to position 1 within hours of spiking.\u003c/p\u003e\n\u003ch2\u003eDistributed Architecture\u003c/h2\u003e\n\u003cp\u003eWith the core search logic defined, the architecture adds a caching layer in front of Elasticsearch. This is not optional: at 115,000 queries per second, Elasticsearch would need hundreds of nodes to handle the full load. Caching popular prefixes in Redis and CDN reduces the load Elasticsearch actually sees to a small fraction of total traffic.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eClient                CDN                 API          Redis          Elasticsearch\n  â”‚                    â”‚                   â”‚              â”‚                â”‚\n  â”œâ”€ type \"j\" â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€ cache hit? â”€â”€â”€â”€â–ºâ”‚              â”‚                â”‚\n  â”‚                    â”‚    YES: return     â”‚              â”‚                â”‚\n  â”‚â—„â”€â”€â”€â”€â”€â”€ [\"java\"]â”€â”€â”€â”€â”¤                   â”‚              â”‚                â”‚\n  â”‚                    â”‚                   â”‚              â”‚                â”‚\n  â”œâ”€ type \"ja\" â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€ miss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€ get(\"ja\") â”€â–ºâ”‚                â”‚\n  â”‚                    â”‚                   â”‚â—„â”€ [\"java\"] â”€â”€â”¤                â”‚\n  â”‚â—„â”€ [\"java\", \"java\"] â”¤â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚                â”‚\n  â”‚                    â”‚                   â”‚              â”‚                â”‚\n  â”œâ”€ type \"jav\" â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€ miss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€ miss â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€ search(\"jav\")â–ºâ”‚\n  â”‚                    â”‚                   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  â”‚â—„â”€ [suggestions] â”€â”€â”€â”¤â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚                â”‚\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCaching strategy:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCDN (CloudFront): Cache responses for common prefixes (\"a\", \"th\", \"he\" â€” ~80% of traffic)\u003c/li\u003e\n\u003cli\u003eRedis: Cache prefix â†’ suggestions with 5-minute TTL\u003c/li\u003e\n\u003cli\u003eCache key: \u003ccode\u003esuggest:{lang}:{prefix}\u003c/code\u003e (normalize: lowercase, trim)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe short 5-minute TTL in Redis is deliberate: it ensures that trending queries â€” which your pipeline updates every minute â€” propagate to users within 5 minutes of spiking, even for cached prefixes. A longer TTL would make the system more cache-efficient but less responsive to trends.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Service\npublic class AutocompleteService {\n\n    @Autowired\n    private StringRedisTemplate redis;\n\n    @Autowired\n    private ElasticsearchClient es;\n\n    public List\u0026#x3C;String\u003e suggest(String prefix, String locale) {\n        String normalized = prefix.toLowerCase().trim();\n        if (normalized.length() \u0026#x3C; 2) return Collections.emptyList(); // Min 2 chars\n\n        String cacheKey = \"suggest:\" + locale + \":\" + normalized;\n        String cached = redis.opsForValue().get(cacheKey);\n\n        if (cached != null) {\n            return objectMapper.readValue(cached, List.class);\n        }\n\n        List\u0026#x3C;String\u003e suggestions = searchElasticsearch(normalized, locale);\n\n        redis.opsForValue().set(cacheKey, objectMapper.writeValueAsString(suggestions),\n            Duration.ofMinutes(5));\n\n        return suggestions;\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe minimum prefix length of 2 characters is a practical optimization: single-character prefixes like \"a\" or \"t\" would match millions of queries and are too ambiguous to be useful, while their cache entries would occupy disproportionate memory. By skipping them, you eliminate a class of expensive queries with low signal.\u003c/p\u003e\n\u003ch2\u003eKeeping Suggestions Fresh: Real-Time Updates\u003c/h2\u003e\n\u003cp\u003eWith the serving layer in place, you need a pipeline that continuously feeds new query data back into the index. The challenge is balancing freshness (how quickly a viral query appears) against noise (a query that spikes once due to a bot should not permanently pollute the index).\u003c/p\u003e\n\u003cp\u003eQuery logs are processed to update suggestion scores:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ePipeline:\n  User searches â†’ App logs query â†’ Kafka topic \"search-queries\"\n      â†’ Flink/Spark aggregation (5-minute windows)\n      â†’ Top queries with updated scores\n      â†’ Update Elasticsearch + rebuild Redis cache\n\nFrequency:\n  Trend detection: 1-minute windows (detect viral queries immediately)\n  Full re-rank: 10-minute windows (stabilize rankings)\n  Full index rebuild: Daily (garbage collect dead queries)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe three-tier frequency schedule is the key design insight here: 1-minute windows for trend detection mean a breaking news query surfaces within 60 seconds, while the daily full rebuild prunes queries that trended briefly and are now dead weight in the index.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Kafka Streams aggregation\nKStream\u0026#x3C;String, SearchEvent\u003e searches = builder.stream(\"search-queries\");\n\nKTable\u0026#x3C;String, Long\u003e queryCounts = searches\n    .groupBy((key, event) -\u003e event.getNormalizedQuery())\n    .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofMinutes(5)))\n    .count();\n\nqueryCounts.toStream()\n    .map((window, count) -\u003e KeyValue.pair(window.key(), count))\n    .to(\"query-scores\", Produced.with(Serdes.String(), Serdes.Long()));\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eofSizeWithNoGrace\u003c/code\u003e is worth understanding: the \"no grace period\" setting means the window closes immediately at the 5-minute mark and emits its result without waiting for late-arriving events. For autocomplete scoring, this tradeoff is correct â€” a small number of late events does not materially change query rankings, and lower latency is more valuable than perfect accuracy.\u003c/p\u003e\n\u003ch2\u003ePersonalization\u003c/h2\u003e\n\u003cp\u003eGlobal suggestions are a strong baseline, but users who have searched for \"java concurrency\" three times this week should see Java-related completions ranked above unrelated queries. The personalization layer blends a small number of personal suggestions into the global top-5, placing them first so they appear immediately when relevant.\u003c/p\u003e\n\u003cp\u003ePersonal suggestions boost queries from the user's search history:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic List\u0026#x3C;String\u003e suggestPersonalized(String prefix, String userId) {\n    // Blend global suggestions with personal history\n    List\u0026#x3C;String\u003e global = suggest(prefix, \"en\");\n\n    List\u0026#x3C;String\u003e personal = userHistoryService.getMatchingHistory(userId, prefix, 3);\n\n    // Merge: personal first (max 2), then global (fill remaining 3)\n    return Stream.concat(personal.stream(), global.stream())\n        .distinct()\n        .limit(5)\n        .collect(Collectors.toList());\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCapping personal suggestions at 2 out of 5 ensures the global ranking still dominates. If you let personalization dominate entirely, users in a narrow interest category would see increasingly narrow suggestions, a filter-bubble effect that reduces discovery of new topics.\u003c/p\u003e\n\u003ch2\u003eTypo Tolerance\u003c/h2\u003e\n\u003cp\u003eEven with perfect indexing and caching, users make typos. Without fuzzy matching, a user typing \"jva virtual\" would see no suggestions at all. Elasticsearch's built-in \u003ccode\u003efuzziness\u003c/code\u003e setting handles this by finding documents within a configurable edit distance from the typed query.\u003c/p\u003e\n\u003cp\u003eUse Elasticsearch's fuzzy matching for queries with typos:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"query\": {\n    \"multi_match\": {\n      \"query\": \"jva virtual\",\n      \"fields\": [\"query\"],\n      \"fuzziness\": \"AUTO\",     // 0 edits for 1-2 chars, 1 for 3-5, 2 for 6+\n      \"prefix_length\": 2,      // First 2 chars must match exactly (performance)\n      \"max_expansions\": 50\n    }\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eprefix_length: 2\u003c/code\u003e parameter is a critical performance guard: it tells Elasticsearch that the first two characters must match exactly, which dramatically reduces the search space for fuzzy expansion. Without this, \"AUTO\" fuzziness on a short query like \"jv\" could expand to thousands of candidate terms and make every keystroke slow.\u003c/p\u003e\n\u003cp\u003eThe difference between a good autocomplete and a great one is the ranking function. Frequency alone gives stale results. Recency alone gives noisy trending results. The combination â€” frequency Ã— recency Ã— click-through rate â€” matches user intent. Instrument your system to measure suggestion acceptance rate and use that signal to continuously improve rankings.\u003c/p\u003e\n","tableOfContents":[{"id":"requirements","text":"Requirements","level":2},{"id":"data-model-what-are-we-searching","text":"Data Model: What Are We Searching?","level":2},{"id":"core-data-structure-trie-vs-inverted-index","text":"Core Data Structure: Trie vs Inverted Index","level":2},{"id":"option-1-trie-prefix-tree","text":"Option 1: Trie (Prefix Tree)","level":3},{"id":"option-2-elasticsearch-with-edge-n-grams-production-choice","text":"Option 2: Elasticsearch with Edge N-Grams (Production Choice)","level":3},{"id":"distributed-architecture","text":"Distributed Architecture","level":2},{"id":"keeping-suggestions-fresh-real-time-updates","text":"Keeping Suggestions Fresh: Real-Time Updates","level":2},{"id":"personalization","text":"Personalization","level":2},{"id":"typo-tolerance","text":"Typo Tolerance","level":2}]},"relatedPosts":[{"title":"Building Production Observability with OpenTelemetry and Grafana Stack","description":"End-to-end observability implementation: distributed tracing with OpenTelemetry, metrics with Prometheus, structured logging with Loki, and the dashboards and alerts that actually help during incidents.","date":"2025-07-03","category":"System Design","tags":["observability","opentelemetry","prometheus","grafana","loki","tracing","spring boot","monitoring"],"featured":false,"affiliateSection":"system-design-courses","slug":"observability-opentelemetry-production","readingTime":"6 min read","excerpt":"Observability is not the same as monitoring. Monitoring tells you something is wrong. Observability lets you understand why â€” by exploring system state through metrics, traces, and logs without needing to know in advanceâ€¦"},{"title":"Event Sourcing and CQRS in Production: Beyond the Theory","description":"What event sourcing actually looks like in production Java systems: event store design, snapshot strategies, projection rebuilding, CQRS read model synchronization, and the operational challenges nobody talks about.","date":"2025-06-23","category":"System Design","tags":["event sourcing","cqrs","system design","java","distributed systems","kafka","spring boot"],"featured":false,"affiliateSection":"distributed-systems-books","slug":"event-sourcing-cqrs-production","readingTime":"7 min read","excerpt":"Event sourcing is one of those patterns that looks elegant in conference talks and becomes surprisingly complex in production systems. The theory â€” store events instead of state, derive state by replaying events â€” is souâ€¦"},{"title":"gRPC vs REST vs GraphQL: Choosing the Right API Protocol","description":"A technical comparison of REST, gRPC, and GraphQL across performance, developer experience, schema evolution, streaming, and real production use cases. When each protocol wins and where each falls short.","date":"2025-06-18","category":"System Design","tags":["grpc","rest","graphql","api design","system design","microservices","protocol buffers"],"featured":false,"affiliateSection":"system-design-courses","slug":"grpc-vs-rest-vs-graphql","readingTime":"7 min read","excerpt":"API protocol selection has a longer lifespan than almost any other technical decision. REST APIs from 2010 are still running in production. gRPC services chosen for internal communication in 2018 are tightly coupled to tâ€¦"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"system-design-autocomplete"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>