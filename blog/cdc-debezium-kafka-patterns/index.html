<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Change Data Capture with Debezium: Real-Time Data Synchronization Patterns<!-- --> | CodeSprintPro</title><meta name="description" content="CDC lets you stream every database change as an event. Learn how Debezium captures PostgreSQL WAL logs, publishes to Kafka, and powers cache invalidation, search indexing, and microservice sync." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/cdc-debezium-kafka-patterns/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Change Data Capture with Debezium: Real-Time Data Synchronization Patterns" data-next-head=""/><meta property="og:description" content="CDC lets you stream every database change as an event. Learn how Debezium captures PostgreSQL WAL logs, publishes to Kafka, and powers cache invalidation, search indexing, and microservice sync." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/cdc-debezium-kafka-patterns/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-02-01" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="Data Engineering" data-next-head=""/><meta property="article:tag" content="cdc" data-next-head=""/><meta property="article:tag" content="debezium" data-next-head=""/><meta property="article:tag" content="kafka" data-next-head=""/><meta property="article:tag" content="data engineering" data-next-head=""/><meta property="article:tag" content="postgresql" data-next-head=""/><meta property="article:tag" content="microservices" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Change Data Capture with Debezium: Real-Time Data Synchronization Patterns" data-next-head=""/><meta name="twitter:description" content="CDC lets you stream every database change as an event. Learn how Debezium captures PostgreSQL WAL logs, publishes to Kafka, and powers cache invalidation, search indexing, and microservice sync." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Change Data Capture with Debezium: Real-Time Data Synchronization Patterns","description":"CDC lets you stream every database change as an event. Learn how Debezium captures PostgreSQL WAL logs, publishes to Kafka, and powers cache invalidation, search indexing, and microservice sync.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-02-01","dateModified":"2025-02-01","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/cdc-debezium-kafka-patterns/"},"keywords":"cdc, debezium, kafka, data engineering, postgresql, microservices","articleSection":"Data Engineering"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Change Data Capture with Debezium: Real-Time Data Synchronization Patterns</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">Data Engineering</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Change Data Capture with Debezium: Real-Time Data Synchronization Patterns</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">CDC lets you stream every database change as an event. Learn how Debezium captures PostgreSQL WAL logs, publishes to Kafka, and powers cache invalidation, search indexing, and microservice sync.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>¬∑</span><span>February 1, 2025</span><span>¬∑</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>11 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->cdc</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->debezium</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->kafka</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->data engineering</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->postgresql</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->microservices</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Change Data Capture (CDC) is one of those techniques that, once you understand it, you see it everywhere. The pattern: instead of your application explicitly publishing events when data changes, let the database engine itself be the event source ‚Äî by reading its internal change log.</p>
<p>This article explains how Debezium captures PostgreSQL WAL (Write-Ahead Log) entries and streams them to Kafka, and shows the production patterns this enables.</p>
<h2>Why CDC?</h2>
<h3>The Problem: Dual-Write</h3>
<p>When a service needs to update a database AND publish an event (for cache invalidation, search indexing, microservice notification), the naive approach is dual-write. The code below looks straightforward, but it contains a race condition that will eventually corrupt your data in production ‚Äî the kind of bug that's very hard to reproduce and very hard to explain to stakeholders.</p>
<pre><code class="language-java">// PROBLEMATIC: Dual-write with a race condition
public void createOrder(Order order) {
    orderRepository.save(order);           // Step 1: DB write
    kafkaTemplate.send("orders", order);   // Step 2: Event publish
    // If the app crashes between Step 1 and Step 2:
    // ‚Üí DB has the order, Kafka doesn't ‚Üí systems are inconsistent
}
</code></pre>
<p>This is a distributed transaction problem. Two-phase commit is operationally painful. CDC solves this by making the database write the single source of truth ‚Äî the event is derived from the write, not paired with it.</p>
<h3>The Outbox Pattern (Another Solution)</h3>
<p>Before CDC was widely adopted, teams used the Outbox pattern as a more reliable alternative to dual-write. The idea is elegant: instead of writing to the database and Kafka separately, you write to two database tables in a single transaction, then a background process publishes the second table's entries to Kafka. Because both writes are in the same transaction, you eliminate the crash window entirely.</p>
<pre><code class="language-java">@Transactional
public void createOrder(Order order) {
    orderRepository.save(order);
    // Same transaction ‚Äî atomic write to both tables
    outboxRepository.save(new OutboxEvent("order.created", order.toJson()));
}

// Separate poller (less elegant, but reliable)
@Scheduled(fixedDelay = 1000)
public void publishOutboxEvents() {
    List&#x3C;OutboxEvent> events = outboxRepository.findUnpublished();
    events.forEach(e -> {
        kafkaTemplate.send(e.getType(), e.getPayload());
        outboxRepository.markPublished(e.getId());
    });
}
</code></pre>
<p>CDC with Debezium automates the outbox pattern ‚Äî Debezium reads the outbox table changes from WAL and publishes them, eliminating the polling process.</p>
<h2>How Debezium Works</h2>
<p>Understanding Debezium requires understanding PostgreSQL's Write-Ahead Log. The WAL is PostgreSQL's crash recovery mechanism ‚Äî every change is written there first before it touches the main table. Debezium acts as a logical replication client, reading those WAL entries and translating them into structured events. Think of Debezium as a translator sitting between your database's internal diary and your event streaming platform.</p>
<pre><code>PostgreSQL WAL (Write-Ahead Log):
  Every INSERT/UPDATE/DELETE is first written to the WAL before the main tables.
  WAL is append-only and durable ‚Äî used for crash recovery and replication.

Debezium's mechanism:
  1. Connects to PostgreSQL as a logical replication client
  2. PostgreSQL sends WAL entries to Debezium via a replication slot
  3. Debezium decodes WAL entries into structured change events
  4. Events published to Kafka topics (one per table by default)

PostgreSQL WAL entry for INSERT into orders:
  {
    "op": "c",          // c=create, u=update, d=delete, r=read (snapshot)
    "ts_ms": 1704153600000,
    "before": null,     // null for INSERT (no previous state)
    "after": {
      "id": "ord-123",
      "user_id": "usr-456",
      "total": 99.99,
      "status": "PENDING",
      "created_at": 1704153600000
    },
    "source": {
      "version": "2.5.0.Final",
      "connector": "postgresql",
      "name": "pg-prod",
      "ts_ms": 1704153600000,
      "snapshot": "false",
      "db": "myapp",
      "schema": "public",
      "table": "orders",
      "txId": 789,
      "lsn": 24023128   // Log Sequence Number ‚Äî WAL position
    }
  }
</code></pre>
<p>The <code>before</code> and <code>after</code> fields are especially useful ‚Äî for UPDATE operations, you get both the old and new values, enabling downstream consumers to detect exactly what changed rather than having to compare against a previous state they may have stored. The <code>lsn</code> (Log Sequence Number) is Debezium's bookmark in the WAL ‚Äî it uses this to resume from exactly the right position after a restart.</p>
<h2>PostgreSQL Configuration</h2>
<p>Before Debezium can connect, PostgreSQL needs to be configured to support logical replication. The default WAL level (<code>replica</code>) only supports physical replication for read replicas ‚Äî you need to change it to <code>logical</code> to allow Debezium to decode the WAL entries into structured events.</p>
<pre><code class="language-bash"># postgresql.conf ‚Äî requires PostgreSQL restart
 wal_level = logical                    # Default is 'replica' ‚Äî change to 'logical'
max_replication_slots = 5              # Debezium uses one slot per connector
max_wal_senders = 5                    # Max concurrent replication connections

# Retention: don't let WAL grow unbounded if Debezium falls behind
 wal_keep_size = 1024                   # Keep at least 1GB of WAL segments
</code></pre>
<p>With the server configured, create a dedicated replication user with the minimum necessary permissions ‚Äî this follows the principle of least privilege and limits blast radius if the credentials are ever compromised.</p>
<pre><code class="language-sql">-- Create a replication user with minimal permissions
CREATE ROLE debezium REPLICATION LOGIN PASSWORD 'strong_password';
GRANT SELECT ON ALL TABLES IN SCHEMA public TO debezium;
GRANT CREATE ON DATABASE myapp TO debezium;  -- For creating replication slots

-- Verify replication slots (Debezium creates these automatically)
SELECT slot_name, plugin, slot_type, active, restart_lsn FROM pg_replication_slots;
-- slot_name: debezium_pg_prod
-- plugin: pgoutput (or decoderbufs)
-- active: true (Debezium is connected)
</code></pre>
<h2>Debezium Connector Configuration</h2>
<p>Debezium runs as a Kafka Connect plugin, which means you deploy it by submitting a JSON configuration to the Kafka Connect REST API. The configuration below sets up capture for three tables and uses Avro serialization with a Schema Registry ‚Äî this is the production-grade setup that handles schema evolution safely.</p>
<pre><code class="language-json">{
  "name": "postgres-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "plugin.name": "pgoutput",
    "tasks.max": "1",

    "database.hostname": "postgres.internal",
    "database.port": "5432",
    "database.user": "debezium",
    "database.password": "${file:/opt/kafka/connect/secrets.properties:DB_PASSWORD}",
    "database.dbname": "myapp",
    "database.server.name": "pg-prod",

    "table.include.list": "public.orders,public.products,public.users",

    "slot.name": "debezium_pg_prod",
    "publication.name": "debezium_publication",
    "publication.autocreate.mode": "filtered",

    "snapshot.mode": "initial",
    "snapshot.locking.mode": "none",

    "topic.prefix": "pg-prod",
    "topic.creation.default.replication.factor": 3,
    "topic.creation.default.partitions": 6,

    "key.converter": "io.confluent.kafka.serializers.KafkaAvroSerializer",
    "key.converter.schema.registry.url": "http://schema-registry:8081",
    "value.converter": "io.confluent.kafka.serializers.KafkaAvroSerializer",
    "value.converter.schema.registry.url": "http://schema-registry:8081",

    "transforms": "unwrap",
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
    "transforms.unwrap.drop.tombstones": "false",
    "transforms.unwrap.delete.handling.mode": "rewrite"
  }
}
</code></pre>
<p>The <code>transforms.unwrap</code> section applies the <code>ExtractNewRecordState</code> Single Message Transform (SMT), which strips the Debezium metadata envelope and gives consumers a clean, flat event with just the row data. Without this transform, consumers would need to parse the nested <code>before</code>/<code>after</code> structure on every event.</p>
<p>This produces Kafka topics:</p>
<ul>
<li><code>pg-prod.public.orders</code> ‚Äî all order changes</li>
<li><code>pg-prod.public.products</code> ‚Äî all product changes</li>
<li><code>pg-prod.public.users</code> ‚Äî all user changes</li>
</ul>
<h2>Consumer Patterns</h2>
<p>Now that changes are flowing into Kafka topics, you can attach multiple independent consumers ‚Äî each solving a different downstream problem without any coupling between them. This fan-out is CDC's key architectural benefit: one data source, many consumers, zero application changes required to add a new one.</p>
<h3>Pattern 1: Cache Invalidation</h3>
<p>Cache invalidation is one of the hardest problems in distributed systems. CDC makes it straightforward: every time a row changes in the database, Kafka delivers the event to your cache invalidator, which removes the stale entry. The cache and database can never diverge for more than the Kafka propagation latency (typically under a second).</p>
<pre><code class="language-java">@Component
public class OrderCacheInvalidator {

    @Autowired
    private RedisTemplate&#x3C;String, Object> redis;

    @KafkaListener(topics = "pg-prod.public.orders", groupId = "cache-invalidator")
    public void handleOrderChange(ConsumerRecord&#x3C;String, OrderChangeEvent> record) {
        OrderChangeEvent event = record.value();

        // The ExtractNewRecordState transform extracts the "after" state
        // event.getId() is the order ID (set as Kafka message key)
        String cacheKey = "order:" + event.getId();

        if (event.getOp().equals("d")) {
            // DELETE: remove from cache
            redis.delete(cacheKey);
        } else {
            // INSERT or UPDATE: invalidate so next read fetches fresh data
            // (or write-through: set the new value directly)
            redis.delete(cacheKey);
        }

        log.debug("Invalidated cache for order {}, op={}", event.getId(), event.getOp());
    }
}
</code></pre>
<h3>Pattern 2: Elasticsearch Indexing</h3>
<p>Keeping your search index in sync with your database is a problem CDC solves cleanly. Previously this required either synchronous writes to Elasticsearch in your application code (coupling your service to your search infrastructure) or a scheduled batch job that lagged hours behind. With CDC, your search index stays near-real-time automatically.</p>
<pre><code class="language-java">@Component
public class ProductSearchIndexer {

    @Autowired
    private ElasticsearchClient esClient;

    @KafkaListener(topics = "pg-prod.public.products", groupId = "search-indexer")
    public void handleProductChange(ConsumerRecord&#x3C;String, ProductChangeEvent> record) {
        ProductChangeEvent event = record.value();

        if (event.getOp().equals("d")) {
            // Delete from search index
            esClient.delete(d -> d
                .index("products")
                .id(event.getId().toString())
            );
        } else {
            // Upsert into search index
            ProductSearchDocument doc = ProductSearchDocument.from(event);
            esClient.index(i -> i
                .index("products")
                .id(event.getId().toString())
                .document(doc)
            );
        }
    }
}
</code></pre>
<h3>Pattern 3: Materialized View Maintenance (CQRS Read Model)</h3>
<p>The CQRS (Command Query Responsibility Segregation) pattern separates the write model from the read model. CDC is the most natural way to keep the read model up-to-date ‚Äî when the write model changes, Debezium publishes the event, and the read model updater below enriches and denormalizes it into a form optimized for fast queries. This eliminates expensive JOINs at read time by paying the cost once at write time.</p>
<pre><code class="language-java">// Separate read model: orders enriched with user info, denormalized for fast reads
@Component
public class OrderReadModelUpdater {

    @Autowired
    private OrderReadRepository readRepository;

    @Autowired
    private UserService userService;

    @KafkaListener(topics = "pg-prod.public.orders", groupId = "read-model-updater")
    public void handleOrderChange(ConsumerRecord&#x3C;String, OrderChangeEvent> record) {
        OrderChangeEvent event = record.value();

        if (event.getOp().equals("d")) {
            readRepository.deleteById(event.getId());
            return;
        }

        // Enrich with user data (from another service or local cache)
        UserInfo user = userService.getUser(event.getUserId());

        OrderReadModel readModel = OrderReadModel.builder()
            .id(event.getId())
            .userId(event.getUserId())
            .userName(user.getName())         // Denormalized
            .userEmail(user.getEmail())       // Denormalized
            .total(event.getTotal())
            .status(event.getStatus())
            .updatedAt(Instant.now())
            .build();

        readRepository.upsert(readModel);
    }
}
</code></pre>
<p>The <code>userName</code> and <code>userEmail</code> fields being stored directly in the order read model means your order list queries never need to JOIN against the users table ‚Äî a significant performance win at scale. The trade-off is that if a user updates their name, you need a separate process to backfill affected order read models.</p>
<h2>The Outbox Pattern with Debezium</h2>
<p>Building on the outbox pattern introduced earlier, you can combine it with Debezium to achieve the most reliable event publishing architecture available. The application writes to its domain table and an outbox table in a single transaction; Debezium reads the outbox changes and publishes them ‚Äî no polling thread, no risk of missed events.</p>
<pre><code class="language-java">// Domain service: writes to domain table + outbox atomically
@Service
@Transactional
public class OrderService {

    @Autowired
    private OrderRepository orderRepository;

    @Autowired
    private OutboxRepository outboxRepository;

    public Order createOrder(CreateOrderRequest request) {
        Order order = orderRepository.save(buildOrder(request));

        // Outbox entry: same transaction ‚Üí guaranteed to be written
        outboxRepository.save(OutboxEvent.builder()
            .id(UUID.randomUUID())
            .aggregateType("Order")
            .aggregateId(order.getId())
            .type("order.created")
            .payload(objectMapper.writeValueAsString(OrderCreatedEvent.from(order)))
            .build());

        return order;
    }
}

// Debezium watches the outbox table
// When outbox row is inserted ‚Üí WAL entry ‚Üí Debezium reads ‚Üí publishes to Kafka
// No polling thread, no duplicate publish risk, no dual-write race condition
</code></pre>
<p>The beauty of this pattern is that your application code is simple and transactional ‚Äî it writes to two tables and returns. All the complexity of reliable event delivery is handled by Debezium at the infrastructure layer, not scattered through your application code. The Kafka Connect EventRouter SMT routes each outbox event to the appropriate topic based on the <code>aggregate_type</code> field.</p>
<pre><code class="language-json">{
  "transforms": "outbox",
  "transforms.outbox.type": "io.debezium.transforms.outbox.EventRouter",
  "transforms.outbox.table.field.event.id": "id",
  "transforms.outbox.table.field.event.type": "type",
  "transforms.outbox.table.field.event.payload": "payload",
  "transforms.outbox.route.by.field": "aggregate_type",
  "transforms.outbox.route.topic.replacement": "outbox.${routedByValue}"
}
</code></pre>
<h2>Operational Considerations</h2>
<h3>Replication Slot Lag</h3>
<p>The most critical production concern: if Debezium is down or slow, PostgreSQL <strong>cannot clean up WAL</strong> until the replication slot reads it. WAL can grow to fill your disk.</p>
<p>This is the one operational risk you must monitor closely. If your Debezium connector goes offline for hours during a busy period, your PostgreSQL disk can fill up entirely ‚Äî which takes down your entire database, not just the CDC pipeline. Set up this query as an alert in your monitoring system, and treat it with the same urgency as disk space alerts.</p>
<pre><code class="language-bash"># Monitor replication slot lag (in bytes)
SELECT slot_name,
       pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) AS lag_bytes,
       active
FROM pg_replication_slots;

# Alert if lag_bytes > 10GB (customize based on disk space and write rate)
</code></pre>
<p>Set a WAL disk limit and drop the slot if Debezium is offline too long:</p>
<pre><code class="language-bash"># Drop stalled replication slot (Debezium will re-snapshot on reconnect)
SELECT pg_drop_replication_slot('debezium_pg_prod');
</code></pre>
<p>Dropping the slot is a drastic action ‚Äî Debezium will need to perform a full snapshot on reconnect ‚Äî but it's better than losing your primary database to disk exhaustion. The right approach is to automate slot removal after a configurable lag threshold.</p>
<h3>Schema Evolution</h3>
<p>When your database schema changes (new column, renamed column), Debezium handles this through Schema Registry versioning. Use <code>ALTER TABLE ... ADD COLUMN</code> safely ‚Äî Debezium handles new columns gracefully. Renaming/dropping columns requires coordination with consumers.</p>
<h3>Snapshot Mode</h3>
<p>On initial deployment, Debezium can snapshot existing data:</p>
<ul>
<li><code>initial</code>: Snapshot all existing rows, then stream new changes (default ‚Äî use for most cases)</li>
<li><code>never</code>: Skip snapshot, only stream changes from now (use when data is already migrated)</li>
<li><code>schema_only</code>: Only capture the schema, not existing data</li>
</ul>
<p>CDC with Debezium turns your database into a reliable event bus without changing application code. The WAL is already there ‚Äî Debezium just makes it readable. For any microservice architecture where services need to react to data changes in other services' databases, CDC is the most reliable and operationally simple solution available.</p>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">üìö</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Fundamentals of Data Engineering</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">New</span></div><p class="text-xs text-gray-600">Joe Reis&#x27;s book on data pipelines, architectures, and the modern data stack.</p></div><a href="https://amzn.to/3Vmf5sX" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">Apache Kafka for Data Engineers ‚Äî Udemy</span></div><p class="text-xs text-gray-600">Learn Kafka Connect, Kafka Streams, and CDC with Debezium for data pipelines.</p></div><a href="https://www.udemy.com/course/kafka-streams-real-time-stream-processing-master-class/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> ‚Üí</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Change%20Data%20Capture%20with%20Debezium%3A%20Real-Time%20Data%20Synchronization%20Patterns&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fcdc-debezium-kafka-patterns%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fcdc-debezium-kafka-patterns%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#why-cdc" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Why CDC?</a></li><li class="ml-4"><a href="#the-problem-dual-write" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">The Problem: Dual-Write</a></li><li class="ml-4"><a href="#the-outbox-pattern-another-solution" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">The Outbox Pattern (Another Solution)</a></li><li class=""><a href="#how-debezium-works" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">How Debezium Works</a></li><li class=""><a href="#postgresql-configuration" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">PostgreSQL Configuration</a></li><li class=""><a href="#debezium-connector-configuration" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Debezium Connector Configuration</a></li><li class=""><a href="#consumer-patterns" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Consumer Patterns</a></li><li class="ml-4"><a href="#pattern-1-cache-invalidation" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Pattern 1: Cache Invalidation</a></li><li class="ml-4"><a href="#pattern-2-elasticsearch-indexing" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Pattern 2: Elasticsearch Indexing</a></li><li class="ml-4"><a href="#pattern-3-materialized-view-maintenance-cqrs-read-model" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Pattern 3: Materialized View Maintenance (CQRS Read Model)</a></li><li class=""><a href="#the-outbox-pattern-with-debezium" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">The Outbox Pattern with Debezium</a></li><li class=""><a href="#operational-considerations" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Operational Considerations</a></li><li class="ml-4"><a href="#replication-slot-lag" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Replication Slot Lag</a></li><li class="ml-4"><a href="#schema-evolution" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Schema Evolution</a></li><li class="ml-4"><a href="#snapshot-mode" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 text-xs">Snapshot Mode</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/kafka-streams-real-time-processing/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-teal-100 text-teal-700">Data Engineering</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Kafka Streams: Real-Time Stream Processing Without a Separate Cluster</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Kafka Streams is a Java library for building real-time stream processing applications. Unlike Flink or Spark Streaming, it has no separate cluster ‚Äî it runs as a library inside your Java application. Each instance of you‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Apr 24, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>6 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->kafka</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->kafka streams</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->stream processing</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">‚Üê Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS ‚Äî by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">¬© <!-- -->2026<!-- --> CodeSprintPro ¬∑ Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js ¬∑ TailwindCSS ¬∑ Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Change Data Capture with Debezium: Real-Time Data Synchronization Patterns","description":"CDC lets you stream every database change as an event. Learn how Debezium captures PostgreSQL WAL logs, publishes to Kafka, and powers cache invalidation, search indexing, and microservice sync.","date":"2025-02-01","category":"Data Engineering","tags":["cdc","debezium","kafka","data engineering","postgresql","microservices"],"featured":false,"affiliateSection":"data-engineering-resources","slug":"cdc-debezium-kafka-patterns","readingTime":"11 min read","excerpt":"Change Data Capture (CDC) is one of those techniques that, once you understand it, you see it everywhere. The pattern: instead of your application explicitly publishing events when data changes, let the database engine i‚Ä¶","contentHtml":"\u003cp\u003eChange Data Capture (CDC) is one of those techniques that, once you understand it, you see it everywhere. The pattern: instead of your application explicitly publishing events when data changes, let the database engine itself be the event source ‚Äî by reading its internal change log.\u003c/p\u003e\n\u003cp\u003eThis article explains how Debezium captures PostgreSQL WAL (Write-Ahead Log) entries and streams them to Kafka, and shows the production patterns this enables.\u003c/p\u003e\n\u003ch2\u003eWhy CDC?\u003c/h2\u003e\n\u003ch3\u003eThe Problem: Dual-Write\u003c/h3\u003e\n\u003cp\u003eWhen a service needs to update a database AND publish an event (for cache invalidation, search indexing, microservice notification), the naive approach is dual-write. The code below looks straightforward, but it contains a race condition that will eventually corrupt your data in production ‚Äî the kind of bug that's very hard to reproduce and very hard to explain to stakeholders.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// PROBLEMATIC: Dual-write with a race condition\npublic void createOrder(Order order) {\n    orderRepository.save(order);           // Step 1: DB write\n    kafkaTemplate.send(\"orders\", order);   // Step 2: Event publish\n    // If the app crashes between Step 1 and Step 2:\n    // ‚Üí DB has the order, Kafka doesn't ‚Üí systems are inconsistent\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is a distributed transaction problem. Two-phase commit is operationally painful. CDC solves this by making the database write the single source of truth ‚Äî the event is derived from the write, not paired with it.\u003c/p\u003e\n\u003ch3\u003eThe Outbox Pattern (Another Solution)\u003c/h3\u003e\n\u003cp\u003eBefore CDC was widely adopted, teams used the Outbox pattern as a more reliable alternative to dual-write. The idea is elegant: instead of writing to the database and Kafka separately, you write to two database tables in a single transaction, then a background process publishes the second table's entries to Kafka. Because both writes are in the same transaction, you eliminate the crash window entirely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Transactional\npublic void createOrder(Order order) {\n    orderRepository.save(order);\n    // Same transaction ‚Äî atomic write to both tables\n    outboxRepository.save(new OutboxEvent(\"order.created\", order.toJson()));\n}\n\n// Separate poller (less elegant, but reliable)\n@Scheduled(fixedDelay = 1000)\npublic void publishOutboxEvents() {\n    List\u0026#x3C;OutboxEvent\u003e events = outboxRepository.findUnpublished();\n    events.forEach(e -\u003e {\n        kafkaTemplate.send(e.getType(), e.getPayload());\n        outboxRepository.markPublished(e.getId());\n    });\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCDC with Debezium automates the outbox pattern ‚Äî Debezium reads the outbox table changes from WAL and publishes them, eliminating the polling process.\u003c/p\u003e\n\u003ch2\u003eHow Debezium Works\u003c/h2\u003e\n\u003cp\u003eUnderstanding Debezium requires understanding PostgreSQL's Write-Ahead Log. The WAL is PostgreSQL's crash recovery mechanism ‚Äî every change is written there first before it touches the main table. Debezium acts as a logical replication client, reading those WAL entries and translating them into structured events. Think of Debezium as a translator sitting between your database's internal diary and your event streaming platform.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ePostgreSQL WAL (Write-Ahead Log):\n  Every INSERT/UPDATE/DELETE is first written to the WAL before the main tables.\n  WAL is append-only and durable ‚Äî used for crash recovery and replication.\n\nDebezium's mechanism:\n  1. Connects to PostgreSQL as a logical replication client\n  2. PostgreSQL sends WAL entries to Debezium via a replication slot\n  3. Debezium decodes WAL entries into structured change events\n  4. Events published to Kafka topics (one per table by default)\n\nPostgreSQL WAL entry for INSERT into orders:\n  {\n    \"op\": \"c\",          // c=create, u=update, d=delete, r=read (snapshot)\n    \"ts_ms\": 1704153600000,\n    \"before\": null,     // null for INSERT (no previous state)\n    \"after\": {\n      \"id\": \"ord-123\",\n      \"user_id\": \"usr-456\",\n      \"total\": 99.99,\n      \"status\": \"PENDING\",\n      \"created_at\": 1704153600000\n    },\n    \"source\": {\n      \"version\": \"2.5.0.Final\",\n      \"connector\": \"postgresql\",\n      \"name\": \"pg-prod\",\n      \"ts_ms\": 1704153600000,\n      \"snapshot\": \"false\",\n      \"db\": \"myapp\",\n      \"schema\": \"public\",\n      \"table\": \"orders\",\n      \"txId\": 789,\n      \"lsn\": 24023128   // Log Sequence Number ‚Äî WAL position\n    }\n  }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003ebefore\u003c/code\u003e and \u003ccode\u003eafter\u003c/code\u003e fields are especially useful ‚Äî for UPDATE operations, you get both the old and new values, enabling downstream consumers to detect exactly what changed rather than having to compare against a previous state they may have stored. The \u003ccode\u003elsn\u003c/code\u003e (Log Sequence Number) is Debezium's bookmark in the WAL ‚Äî it uses this to resume from exactly the right position after a restart.\u003c/p\u003e\n\u003ch2\u003ePostgreSQL Configuration\u003c/h2\u003e\n\u003cp\u003eBefore Debezium can connect, PostgreSQL needs to be configured to support logical replication. The default WAL level (\u003ccode\u003ereplica\u003c/code\u003e) only supports physical replication for read replicas ‚Äî you need to change it to \u003ccode\u003elogical\u003c/code\u003e to allow Debezium to decode the WAL entries into structured events.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# postgresql.conf ‚Äî requires PostgreSQL restart\n wal_level = logical                    # Default is 'replica' ‚Äî change to 'logical'\nmax_replication_slots = 5              # Debezium uses one slot per connector\nmax_wal_senders = 5                    # Max concurrent replication connections\n\n# Retention: don't let WAL grow unbounded if Debezium falls behind\n wal_keep_size = 1024                   # Keep at least 1GB of WAL segments\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWith the server configured, create a dedicated replication user with the minimum necessary permissions ‚Äî this follows the principle of least privilege and limits blast radius if the credentials are ever compromised.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sql\"\u003e-- Create a replication user with minimal permissions\nCREATE ROLE debezium REPLICATION LOGIN PASSWORD 'strong_password';\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO debezium;\nGRANT CREATE ON DATABASE myapp TO debezium;  -- For creating replication slots\n\n-- Verify replication slots (Debezium creates these automatically)\nSELECT slot_name, plugin, slot_type, active, restart_lsn FROM pg_replication_slots;\n-- slot_name: debezium_pg_prod\n-- plugin: pgoutput (or decoderbufs)\n-- active: true (Debezium is connected)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eDebezium Connector Configuration\u003c/h2\u003e\n\u003cp\u003eDebezium runs as a Kafka Connect plugin, which means you deploy it by submitting a JSON configuration to the Kafka Connect REST API. The configuration below sets up capture for three tables and uses Avro serialization with a Schema Registry ‚Äî this is the production-grade setup that handles schema evolution safely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"name\": \"postgres-connector\",\n  \"config\": {\n    \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n    \"plugin.name\": \"pgoutput\",\n    \"tasks.max\": \"1\",\n\n    \"database.hostname\": \"postgres.internal\",\n    \"database.port\": \"5432\",\n    \"database.user\": \"debezium\",\n    \"database.password\": \"${file:/opt/kafka/connect/secrets.properties:DB_PASSWORD}\",\n    \"database.dbname\": \"myapp\",\n    \"database.server.name\": \"pg-prod\",\n\n    \"table.include.list\": \"public.orders,public.products,public.users\",\n\n    \"slot.name\": \"debezium_pg_prod\",\n    \"publication.name\": \"debezium_publication\",\n    \"publication.autocreate.mode\": \"filtered\",\n\n    \"snapshot.mode\": \"initial\",\n    \"snapshot.locking.mode\": \"none\",\n\n    \"topic.prefix\": \"pg-prod\",\n    \"topic.creation.default.replication.factor\": 3,\n    \"topic.creation.default.partitions\": 6,\n\n    \"key.converter\": \"io.confluent.kafka.serializers.KafkaAvroSerializer\",\n    \"key.converter.schema.registry.url\": \"http://schema-registry:8081\",\n    \"value.converter\": \"io.confluent.kafka.serializers.KafkaAvroSerializer\",\n    \"value.converter.schema.registry.url\": \"http://schema-registry:8081\",\n\n    \"transforms\": \"unwrap\",\n    \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n    \"transforms.unwrap.drop.tombstones\": \"false\",\n    \"transforms.unwrap.delete.handling.mode\": \"rewrite\"\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003etransforms.unwrap\u003c/code\u003e section applies the \u003ccode\u003eExtractNewRecordState\u003c/code\u003e Single Message Transform (SMT), which strips the Debezium metadata envelope and gives consumers a clean, flat event with just the row data. Without this transform, consumers would need to parse the nested \u003ccode\u003ebefore\u003c/code\u003e/\u003ccode\u003eafter\u003c/code\u003e structure on every event.\u003c/p\u003e\n\u003cp\u003eThis produces Kafka topics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003epg-prod.public.orders\u003c/code\u003e ‚Äî all order changes\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epg-prod.public.products\u003c/code\u003e ‚Äî all product changes\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epg-prod.public.users\u003c/code\u003e ‚Äî all user changes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eConsumer Patterns\u003c/h2\u003e\n\u003cp\u003eNow that changes are flowing into Kafka topics, you can attach multiple independent consumers ‚Äî each solving a different downstream problem without any coupling between them. This fan-out is CDC's key architectural benefit: one data source, many consumers, zero application changes required to add a new one.\u003c/p\u003e\n\u003ch3\u003ePattern 1: Cache Invalidation\u003c/h3\u003e\n\u003cp\u003eCache invalidation is one of the hardest problems in distributed systems. CDC makes it straightforward: every time a row changes in the database, Kafka delivers the event to your cache invalidator, which removes the stale entry. The cache and database can never diverge for more than the Kafka propagation latency (typically under a second).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Component\npublic class OrderCacheInvalidator {\n\n    @Autowired\n    private RedisTemplate\u0026#x3C;String, Object\u003e redis;\n\n    @KafkaListener(topics = \"pg-prod.public.orders\", groupId = \"cache-invalidator\")\n    public void handleOrderChange(ConsumerRecord\u0026#x3C;String, OrderChangeEvent\u003e record) {\n        OrderChangeEvent event = record.value();\n\n        // The ExtractNewRecordState transform extracts the \"after\" state\n        // event.getId() is the order ID (set as Kafka message key)\n        String cacheKey = \"order:\" + event.getId();\n\n        if (event.getOp().equals(\"d\")) {\n            // DELETE: remove from cache\n            redis.delete(cacheKey);\n        } else {\n            // INSERT or UPDATE: invalidate so next read fetches fresh data\n            // (or write-through: set the new value directly)\n            redis.delete(cacheKey);\n        }\n\n        log.debug(\"Invalidated cache for order {}, op={}\", event.getId(), event.getOp());\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003ePattern 2: Elasticsearch Indexing\u003c/h3\u003e\n\u003cp\u003eKeeping your search index in sync with your database is a problem CDC solves cleanly. Previously this required either synchronous writes to Elasticsearch in your application code (coupling your service to your search infrastructure) or a scheduled batch job that lagged hours behind. With CDC, your search index stays near-real-time automatically.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e@Component\npublic class ProductSearchIndexer {\n\n    @Autowired\n    private ElasticsearchClient esClient;\n\n    @KafkaListener(topics = \"pg-prod.public.products\", groupId = \"search-indexer\")\n    public void handleProductChange(ConsumerRecord\u0026#x3C;String, ProductChangeEvent\u003e record) {\n        ProductChangeEvent event = record.value();\n\n        if (event.getOp().equals(\"d\")) {\n            // Delete from search index\n            esClient.delete(d -\u003e d\n                .index(\"products\")\n                .id(event.getId().toString())\n            );\n        } else {\n            // Upsert into search index\n            ProductSearchDocument doc = ProductSearchDocument.from(event);\n            esClient.index(i -\u003e i\n                .index(\"products\")\n                .id(event.getId().toString())\n                .document(doc)\n            );\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003ePattern 3: Materialized View Maintenance (CQRS Read Model)\u003c/h3\u003e\n\u003cp\u003eThe CQRS (Command Query Responsibility Segregation) pattern separates the write model from the read model. CDC is the most natural way to keep the read model up-to-date ‚Äî when the write model changes, Debezium publishes the event, and the read model updater below enriches and denormalizes it into a form optimized for fast queries. This eliminates expensive JOINs at read time by paying the cost once at write time.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Separate read model: orders enriched with user info, denormalized for fast reads\n@Component\npublic class OrderReadModelUpdater {\n\n    @Autowired\n    private OrderReadRepository readRepository;\n\n    @Autowired\n    private UserService userService;\n\n    @KafkaListener(topics = \"pg-prod.public.orders\", groupId = \"read-model-updater\")\n    public void handleOrderChange(ConsumerRecord\u0026#x3C;String, OrderChangeEvent\u003e record) {\n        OrderChangeEvent event = record.value();\n\n        if (event.getOp().equals(\"d\")) {\n            readRepository.deleteById(event.getId());\n            return;\n        }\n\n        // Enrich with user data (from another service or local cache)\n        UserInfo user = userService.getUser(event.getUserId());\n\n        OrderReadModel readModel = OrderReadModel.builder()\n            .id(event.getId())\n            .userId(event.getUserId())\n            .userName(user.getName())         // Denormalized\n            .userEmail(user.getEmail())       // Denormalized\n            .total(event.getTotal())\n            .status(event.getStatus())\n            .updatedAt(Instant.now())\n            .build();\n\n        readRepository.upsert(readModel);\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003euserName\u003c/code\u003e and \u003ccode\u003euserEmail\u003c/code\u003e fields being stored directly in the order read model means your order list queries never need to JOIN against the users table ‚Äî a significant performance win at scale. The trade-off is that if a user updates their name, you need a separate process to backfill affected order read models.\u003c/p\u003e\n\u003ch2\u003eThe Outbox Pattern with Debezium\u003c/h2\u003e\n\u003cp\u003eBuilding on the outbox pattern introduced earlier, you can combine it with Debezium to achieve the most reliable event publishing architecture available. The application writes to its domain table and an outbox table in a single transaction; Debezium reads the outbox changes and publishes them ‚Äî no polling thread, no risk of missed events.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Domain service: writes to domain table + outbox atomically\n@Service\n@Transactional\npublic class OrderService {\n\n    @Autowired\n    private OrderRepository orderRepository;\n\n    @Autowired\n    private OutboxRepository outboxRepository;\n\n    public Order createOrder(CreateOrderRequest request) {\n        Order order = orderRepository.save(buildOrder(request));\n\n        // Outbox entry: same transaction ‚Üí guaranteed to be written\n        outboxRepository.save(OutboxEvent.builder()\n            .id(UUID.randomUUID())\n            .aggregateType(\"Order\")\n            .aggregateId(order.getId())\n            .type(\"order.created\")\n            .payload(objectMapper.writeValueAsString(OrderCreatedEvent.from(order)))\n            .build());\n\n        return order;\n    }\n}\n\n// Debezium watches the outbox table\n// When outbox row is inserted ‚Üí WAL entry ‚Üí Debezium reads ‚Üí publishes to Kafka\n// No polling thread, no duplicate publish risk, no dual-write race condition\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe beauty of this pattern is that your application code is simple and transactional ‚Äî it writes to two tables and returns. All the complexity of reliable event delivery is handled by Debezium at the infrastructure layer, not scattered through your application code. The Kafka Connect EventRouter SMT routes each outbox event to the appropriate topic based on the \u003ccode\u003eaggregate_type\u003c/code\u003e field.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n  \"transforms\": \"outbox\",\n  \"transforms.outbox.type\": \"io.debezium.transforms.outbox.EventRouter\",\n  \"transforms.outbox.table.field.event.id\": \"id\",\n  \"transforms.outbox.table.field.event.type\": \"type\",\n  \"transforms.outbox.table.field.event.payload\": \"payload\",\n  \"transforms.outbox.route.by.field\": \"aggregate_type\",\n  \"transforms.outbox.route.topic.replacement\": \"outbox.${routedByValue}\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eOperational Considerations\u003c/h2\u003e\n\u003ch3\u003eReplication Slot Lag\u003c/h3\u003e\n\u003cp\u003eThe most critical production concern: if Debezium is down or slow, PostgreSQL \u003cstrong\u003ecannot clean up WAL\u003c/strong\u003e until the replication slot reads it. WAL can grow to fill your disk.\u003c/p\u003e\n\u003cp\u003eThis is the one operational risk you must monitor closely. If your Debezium connector goes offline for hours during a busy period, your PostgreSQL disk can fill up entirely ‚Äî which takes down your entire database, not just the CDC pipeline. Set up this query as an alert in your monitoring system, and treat it with the same urgency as disk space alerts.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Monitor replication slot lag (in bytes)\nSELECT slot_name,\n       pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) AS lag_bytes,\n       active\nFROM pg_replication_slots;\n\n# Alert if lag_bytes \u003e 10GB (customize based on disk space and write rate)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSet a WAL disk limit and drop the slot if Debezium is offline too long:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Drop stalled replication slot (Debezium will re-snapshot on reconnect)\nSELECT pg_drop_replication_slot('debezium_pg_prod');\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDropping the slot is a drastic action ‚Äî Debezium will need to perform a full snapshot on reconnect ‚Äî but it's better than losing your primary database to disk exhaustion. The right approach is to automate slot removal after a configurable lag threshold.\u003c/p\u003e\n\u003ch3\u003eSchema Evolution\u003c/h3\u003e\n\u003cp\u003eWhen your database schema changes (new column, renamed column), Debezium handles this through Schema Registry versioning. Use \u003ccode\u003eALTER TABLE ... ADD COLUMN\u003c/code\u003e safely ‚Äî Debezium handles new columns gracefully. Renaming/dropping columns requires coordination with consumers.\u003c/p\u003e\n\u003ch3\u003eSnapshot Mode\u003c/h3\u003e\n\u003cp\u003eOn initial deployment, Debezium can snapshot existing data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003einitial\u003c/code\u003e: Snapshot all existing rows, then stream new changes (default ‚Äî use for most cases)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enever\u003c/code\u003e: Skip snapshot, only stream changes from now (use when data is already migrated)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eschema_only\u003c/code\u003e: Only capture the schema, not existing data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCDC with Debezium turns your database into a reliable event bus without changing application code. The WAL is already there ‚Äî Debezium just makes it readable. For any microservice architecture where services need to react to data changes in other services' databases, CDC is the most reliable and operationally simple solution available.\u003c/p\u003e\n","tableOfContents":[{"id":"why-cdc","text":"Why CDC?","level":2},{"id":"the-problem-dual-write","text":"The Problem: Dual-Write","level":3},{"id":"the-outbox-pattern-another-solution","text":"The Outbox Pattern (Another Solution)","level":3},{"id":"how-debezium-works","text":"How Debezium Works","level":2},{"id":"postgresql-configuration","text":"PostgreSQL Configuration","level":2},{"id":"debezium-connector-configuration","text":"Debezium Connector Configuration","level":2},{"id":"consumer-patterns","text":"Consumer Patterns","level":2},{"id":"pattern-1-cache-invalidation","text":"Pattern 1: Cache Invalidation","level":3},{"id":"pattern-2-elasticsearch-indexing","text":"Pattern 2: Elasticsearch Indexing","level":3},{"id":"pattern-3-materialized-view-maintenance-cqrs-read-model","text":"Pattern 3: Materialized View Maintenance (CQRS Read Model)","level":3},{"id":"the-outbox-pattern-with-debezium","text":"The Outbox Pattern with Debezium","level":2},{"id":"operational-considerations","text":"Operational Considerations","level":2},{"id":"replication-slot-lag","text":"Replication Slot Lag","level":3},{"id":"schema-evolution","text":"Schema Evolution","level":3},{"id":"snapshot-mode","text":"Snapshot Mode","level":3}]},"relatedPosts":[{"title":"Kafka Streams: Real-Time Stream Processing Without a Separate Cluster","description":"Production Kafka Streams: KStream vs KTable semantics, stateful transformations with RocksDB state stores, windowed aggregations, stream-table joins, topology design, changelog topics, and the operational patterns for running Kafka Streams in production.","date":"2025-04-24","category":"Data Engineering","tags":["kafka","kafka streams","stream processing","real-time","java","rocksdb","windowing","data engineering"],"featured":false,"affiliateSection":"data-engineering-resources","slug":"kafka-streams-real-time-processing","readingTime":"6 min read","excerpt":"Kafka Streams is a Java library for building real-time stream processing applications. Unlike Flink or Spark Streaming, it has no separate cluster ‚Äî it runs as a library inside your Java application. Each instance of you‚Ä¶"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"cdc-debezium-kafka-patterns"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>