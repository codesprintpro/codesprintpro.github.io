<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Kubernetes in Production: Patterns Every Backend Engineer Must Know<!-- --> | CodeSprintPro</title><meta name="description" content="Resource requests and limits, liveness vs readiness probes, rolling deployments, HPA configuration, pod disruption budgets, and the mistakes that cause production outages in Kubernetes." data-next-head=""/><meta name="author" content="Sachin Sarawgi" data-next-head=""/><link rel="canonical" href="https://codesprintpro.com/blog/kubernetes-production-best-practices/" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:title" content="Kubernetes in Production: Patterns Every Backend Engineer Must Know" data-next-head=""/><meta property="og:description" content="Resource requests and limits, liveness vs readiness probes, rolling deployments, HPA configuration, pod disruption budgets, and the mistakes that cause production outages in Kubernetes." data-next-head=""/><meta property="og:url" content="https://codesprintpro.com/blog/kubernetes-production-best-practices/" data-next-head=""/><meta property="og:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><meta property="og:site_name" content="CodeSprintPro" data-next-head=""/><meta property="article:published_time" content="2025-06-08" data-next-head=""/><meta property="article:author" content="Sachin Sarawgi" data-next-head=""/><meta property="article:section" content="AWS" data-next-head=""/><meta property="article:tag" content="kubernetes" data-next-head=""/><meta property="article:tag" content="k8s" data-next-head=""/><meta property="article:tag" content="devops" data-next-head=""/><meta property="article:tag" content="containers" data-next-head=""/><meta property="article:tag" content="deployment" data-next-head=""/><meta property="article:tag" content="aws" data-next-head=""/><meta property="article:tag" content="eks" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Kubernetes in Production: Patterns Every Backend Engineer Must Know" data-next-head=""/><meta name="twitter:description" content="Resource requests and limits, liveness vs readiness probes, rolling deployments, HPA configuration, pod disruption budgets, and the mistakes that cause production outages in Kubernetes." data-next-head=""/><meta name="twitter:image" content="https://codesprintpro.com/images/og-default.jpg" data-next-head=""/><script type="application/ld+json" data-next-head="">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes in Production: Patterns Every Backend Engineer Must Know","description":"Resource requests and limits, liveness vs readiness probes, rolling deployments, HPA configuration, pod disruption budgets, and the mistakes that cause production outages in Kubernetes.","image":"https://codesprintpro.com/images/og-default.jpg","datePublished":"2025-06-08","dateModified":"2025-06-08","author":{"@type":"Person","name":"Sachin Sarawgi","url":"https://codesprintpro.com","sameAs":["https://www.linkedin.com/in/sachin-sarawgi/","https://github.com/codesprintpro","https://medium.com/@codesprintpro"]},"publisher":{"@type":"Organization","name":"CodeSprintPro","url":"https://codesprintpro.com","logo":{"@type":"ImageObject","url":"https://codesprintpro.com/favicon/favicon-96x96.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://codesprintpro.com/blog/kubernetes-production-best-practices/"},"keywords":"kubernetes, k8s, devops, containers, deployment, aws, eks","articleSection":"AWS"}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXXX" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/735d4373f93910ca.css" as="style"/><link rel="stylesheet" href="/_next/static/css/735d4373f93910ca.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-11a4a2dad04962bb.js" defer=""></script><script src="/_next/static/chunks/framework-1ce91eb6f9ecda85.js" defer=""></script><script src="/_next/static/chunks/main-c7f4109f032d7b6e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1a852bd9e38c70ab.js" defer=""></script><script src="/_next/static/chunks/730-db7827467817f501.js" defer=""></script><script src="/_next/static/chunks/90-1cd4611704c3dbe0.js" defer=""></script><script src="/_next/static/chunks/440-29f4b99a44e744b5.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0c1b14a33d5e05ee.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_buildManifest.js" defer=""></script><script src="/_next/static/rpFn_jslWZ9qPkJwor7RD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_f367f3"><div class="min-h-screen bg-white"><nav class="fixed w-full z-50 transition-all duration-300 bg-white shadow-md" style="transform:translateY(-100px) translateZ(0)"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><a class="text-xl font-bold transition-colors text-blue-600" href="/">CodeSprintPro</a><div class="hidden md:flex items-center space-x-8"><a class="text-sm font-medium transition-colors text-blue-600" href="/blog/">Blog</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#about">About</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#portfolio">Portfolio</a><a class="text-sm font-medium transition-colors text-gray-600 hover:text-blue-600" href="/#contact">Contact</a></div><button class="md:hidden p-2 rounded-lg transition-colors hover:bg-gray-100 text-gray-600" aria-label="Toggle mobile menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><div class="pt-20"><div class="bg-gradient-to-br from-gray-900 to-blue-950 py-16"><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl"><nav class="flex items-center gap-2 text-sm text-gray-400 mb-6"><a class="hover:text-white transition-colors" href="/">Home</a><span>/</span><a class="hover:text-white transition-colors" href="/blog/">Blog</a><span>/</span><span class="text-gray-300 truncate max-w-xs">Kubernetes in Production: Patterns Every Backend Engineer Must Know</span></nav><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full bg-blue-100 text-blue-700 mb-4">AWS</span><h1 class="text-3xl md:text-5xl font-bold text-white mb-4 leading-tight">Kubernetes in Production: Patterns Every Backend Engineer Must Know</h1><p class="text-gray-300 text-lg mb-6 max-w-3xl">Resource requests and limits, liveness vs readiness probes, rolling deployments, HPA configuration, pod disruption budgets, and the mistakes that cause production outages in Kubernetes.</p><div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm"><span class="flex items-center gap-1.5"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z"></path></svg>Sachin Sarawgi</span><span>¬∑</span><span>June 8, 2025</span><span>¬∑</span><span class="flex items-center gap-1"><svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>6 min read</span></div><div class="flex flex-wrap gap-2 mt-4"><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->kubernetes</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->k8s</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->devops</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->containers</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->deployment</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->aws</span><span class="text-xs text-gray-400 bg-gray-800 px-2 py-1 rounded">#<!-- -->eks</span></div></div></div><div class="container mx-auto px-4 sm:px-6 lg:px-8 max-w-5xl py-12"><div class="flex gap-12"><div class="flex-1 min-w-0"><article class="prose prose-lg max-w-none prose-headings:text-gray-900 prose-headings:font-bold prose-h2:text-2xl prose-h2:mt-10 prose-h2:mb-4 prose-h3:text-xl prose-h3:mt-8 prose-h3:mb-3 prose-p:text-gray-700 prose-p:leading-relaxed prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline prose-strong:text-gray-900 prose-blockquote:border-blue-600 prose-blockquote:text-gray-600 prose-code:text-blue-700 prose-code:bg-blue-50 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sm prose-code:before:content-none prose-code:after:content-none prose-pre:rounded-xl prose-img:rounded-xl prose-img:shadow-md prose-table:text-sm prose-th:bg-gray-100 prose-th:text-gray-700"><p>Running a container in Kubernetes and running a production workload in Kubernetes are different disciplines. The gap between <code>kubectl apply -f deployment.yaml</code> and a service that survives node failures, deployment rollouts, and traffic spikes without user-visible downtime is filled with configuration that doesn't exist in most tutorials.</p>
<h2>Resource Requests and Limits: The Foundation</h2>
<p>Every production pod must have resource requests and limits. Without them, Kubernetes cannot make scheduling decisions and nodes become dangerously overloaded.</p>
<pre><code class="language-yaml">resources:
  requests:
    memory: "512Mi"    # Scheduler uses this for placement decisions
    cpu: "250m"        # 250 millicores = 25% of one CPU core
  limits:
    memory: "1Gi"      # Container is OOMKilled if it exceeds this
    cpu: "1000m"       # Container is CPU-throttled (not killed) if it exceeds this
</code></pre>
<p><strong>CPU throttling vs OOM Kill:</strong> CPU limits throttle ‚Äî the container is slowed but kept running. Memory limits kill ‚Äî the container is OOMKilled and restarted. This distinction matters: a CPU limit that's too low causes latency spikes; a memory limit that's too low causes crashes.</p>
<p><strong>Requests vs Limits ratio:</strong> Kubernetes allows "overcommitting" ‚Äî requesting 500m but limiting at 2000m. This is valid for bursty workloads but creates a risk: if all pods burst simultaneously, the node runs out of resources. For critical services, set requests = limits (Guaranteed QoS class) to prevent eviction.</p>
<p><strong>Setting the right values:</strong></p>
<pre><code class="language-bash"># Check actual usage in production:
kubectl top pod -l app=api-service --containers
# Use P95 of observed memory as request, P99 + 20% headroom as limit

# For CPU: set request = P50 usage, limit = 2-4√ó request
</code></pre>
<h2>Liveness vs Readiness vs Startup Probes</h2>
<p>These three probes are distinct and frequently misconfigured:</p>
<pre><code class="language-yaml">livenessProbe:
  # Is the application alive? If not, restart the container.
  # Use this ONLY for deadlock detection ‚Äî processes that are running but stuck.
  httpGet:
    path: /actuator/health/liveness
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  failureThreshold: 3         # Restart after 3 failures
  timeoutSeconds: 5

readinessProbe:
  # Can the application serve traffic? If not, remove from Service endpoints.
  # Use this to signal when the app is ready and when it's temporarily busy.
  httpGet:
    path: /actuator/health/readiness
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 3
  successThreshold: 1

startupProbe:
  # Overrides liveness during startup ‚Äî prevents premature restarts for slow-starting apps.
  # Only needed when app takes > 30s to start.
  httpGet:
    path: /actuator/health/liveness
    port: 8080
  failureThreshold: 30        # Allow up to 30 √ó 10s = 300s to start
  periodSeconds: 10
</code></pre>
<p><strong>Spring Boot actuator separation:</strong></p>
<pre><code class="language-java">// application.properties
management.endpoint.health.group.liveness.include=livenessState
management.endpoint.health.group.readiness.include=readinessState,db,redis
</code></pre>
<p>Readiness probe fails ‚Üí pod removed from load balancer (no new traffic) ‚Üí existing connections drain. This is correct behavior during DB connection issues ‚Äî the pod stays alive but stops receiving traffic.</p>
<p>Liveness probe fails ‚Üí pod restarted. <strong>Do not include DB/external checks in liveness probes.</strong> If your DB is down and liveness probes fail, Kubernetes restarts all pods. Now you have all pods simultaneously in restart loops. The DB comes back but pods are thrashing. Always keep liveness probes lightweight.</p>
<h2>Rolling Deployments Without Downtime</h2>
<p>Default rolling update configuration is too aggressive:</p>
<pre><code class="language-yaml">strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1          # Default: 25% ‚Äî create at most 1 extra pod
    maxUnavailable: 0    # Never have fewer than replicas running
                         # This ensures zero-downtime: new pod must be Ready before old is terminated
</code></pre>
<p>For a service with 10 replicas:</p>
<ul>
<li><code>maxUnavailable: 0, maxSurge: 1</code> ‚Üí 1 new pod created, 1 old pod terminated when new is Ready. Linear, predictable.</li>
<li><code>maxUnavailable: 25%, maxSurge: 25%</code> ‚Üí up to 2 old pods removed before new pods are Ready ‚Üí brief 80% capacity.</li>
</ul>
<p><strong>Graceful shutdown:</strong> When Kubernetes terminates a pod, it sends <code>SIGTERM</code>, waits <code>terminationGracePeriodSeconds</code>, then sends <code>SIGKILL</code>. Your application must handle <code>SIGTERM</code> gracefully ‚Äî stop accepting new connections, finish in-flight requests, then exit.</p>
<pre><code class="language-java">// Spring Boot graceful shutdown:
// application.properties:
server.shutdown=graceful
spring.lifecycle.timeout-per-shutdown-phase=30s
</code></pre>
<pre><code class="language-yaml"># Pod spec:
terminationGracePeriodSeconds: 60  # Must be > your slowest request timeout
lifecycle:
  preStop:
    exec:
      command: ["sh", "-c", "sleep 5"]
      # 5-second sleep before SIGTERM gives the load balancer time to
      # deregister the pod before it stops accepting connections
</code></pre>
<h2>Horizontal Pod Autoscaler Configuration</h2>
<pre><code class="language-yaml">apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-service
  minReplicas: 3          # Never go below 3 ‚Äî one per AZ for HA
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60    # Scale at 60%, not 80% ‚Äî headroom for spikes
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0      # Scale up immediately
      policies:
      - type: Percent
        value: 100                        # Can double pod count per 15s
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300    # Wait 5 minutes before scaling down
</code></pre>
<p><strong>The HPA + JVM problem:</strong> JVM heap is counted against memory limits. During startup, JVM allocates max heap upfront. If <code>maxHeap > memory.request</code>, every new pod immediately looks memory-heavy. HPA sees average memory at 90% and scales up before the JVM has warmed up. Fix: set <code>Xmx</code> to <code>memory.limit √ó 0.75</code>, and set <code>memory.request = memory.limit</code> (Guaranteed QoS).</p>
<h2>Pod Disruption Budgets</h2>
<p>PDBs prevent Kubernetes from simultaneously evicting too many pods during node drains or cluster upgrades:</p>
<pre><code class="language-yaml">apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-service-pdb
spec:
  minAvailable: 2     # Always keep at least 2 pods running
  # OR:
  maxUnavailable: 1   # Never disrupt more than 1 pod at a time
  selector:
    matchLabels:
      app: api-service
</code></pre>
<p>Without a PDB, <code>kubectl drain node-1</code> removes all pods on that node simultaneously. With <code>minAvailable: 2</code> on a 3-replica deployment, the drain can only proceed one pod at a time ‚Äî safe.</p>
<h2>ConfigMaps and Secrets: Common Mistakes</h2>
<pre><code class="language-yaml"># DO: Use envFrom for cleaner pod specs
envFrom:
- configMapRef:
    name: api-config
- secretRef:
    name: api-secrets

# DON'T: Mount secrets as env vars for sensitive data that rotates ‚Äî
# env vars require pod restart to pick up new values.
# DO: Mount as files for secrets that rotate:
volumeMounts:
- name: db-credentials
  mountPath: /etc/credentials
  readOnly: true
volumes:
- name: db-credentials
  secret:
    secretName: db-credentials
    # Updates to the secret propagate to the file within ~1 minute
    # No pod restart needed
</code></pre>
<h2>Resource Quotas Per Namespace</h2>
<pre><code class="language-yaml">apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    requests.cpu: "50"          # Total CPU requests across all pods
    requests.memory: 100Gi
    limits.cpu: "100"
    limits.memory: 200Gi
    pods: "200"
    services: "20"
    persistentvolumeclaims: "50"
</code></pre>
<p>Quotas prevent a single team's misconfigured deployment from consuming all cluster resources.</p>
<h2>Production Checklist</h2>
<p>Before any service goes to production on Kubernetes:</p>
<pre><code>‚ñ° Resource requests AND limits set on all containers
‚ñ° Liveness probe (lightweight, no external deps)
‚ñ° Readiness probe (includes DB/cache connectivity)
‚ñ° Graceful shutdown configured (SIGTERM handler + preStop sleep)
‚ñ° terminationGracePeriodSeconds > max request duration
‚ñ° PodDisruptionBudget configured (minAvailable ‚â• 2 for critical services)
‚ñ° HPA configured with appropriate min/max replicas
‚ñ° Anti-affinity rules for HA (pods spread across AZs)
‚ñ° Network policies limiting ingress/egress
‚ñ° Image tag pinned (never use :latest in production)
‚ñ° Resource quotas on namespace
</code></pre>
<p>Anti-affinity for AZ spread:</p>
<pre><code class="language-yaml">affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchLabels:
          app: api-service
      topologyKey: topology.kubernetes.io/zone
      # Required: pods MUST be in different AZs
      # If only 1 AZ available, pod stays Pending (fail safe)
</code></pre>
</article><div class="my-10 rounded-xl border border-yellow-200 bg-yellow-50 p-6"><div class="flex items-center gap-2 mb-1"><span class="text-lg">üìö</span><h3 class="text-base font-bold text-gray-900">Recommended Resources</h3></div><div class="space-y-4"><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">AWS Solutions Architect Associate ‚Äî Udemy</span><span class="text-xs bg-blue-100 text-blue-700 px-2 py-0.5 rounded-full font-medium">Best Seller</span></div><p class="text-xs text-gray-600">Most popular AWS certification course by Stephane Maarek.</p></div><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View Course<!-- --> ‚Üí</a></div><div class="flex items-start gap-3 bg-white rounded-lg p-4 border border-yellow-100"><div class="flex-1 min-w-0"><div class="flex items-center gap-2 flex-wrap mb-1"><span class="font-semibold text-gray-900 text-sm">AWS in Action, 3rd Edition</span></div><p class="text-xs text-gray-600">Hands-on guide to building cloud applications on AWS.</p></div><a href="https://amzn.to/3Vmf49E" target="_blank" rel="noopener noreferrer" class="flex-shrink-0 text-xs bg-blue-600 text-white px-3 py-2 rounded-lg hover:bg-blue-700 transition-colors font-medium whitespace-nowrap">View on Amazon<!-- --> ‚Üí</a></div></div></div><div class="mt-10 pt-8 border-t border-gray-100"><p class="text-sm text-gray-500 mb-3">Found this useful? Share it:</p><div class="flex gap-3"><a href="https://twitter.com/intent/tweet?text=Kubernetes%20in%20Production%3A%20Patterns%20Every%20Backend%20Engineer%20Must%20Know&amp;url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fkubernetes-production-best-practices%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-gray-900 text-white px-4 py-2 rounded-lg hover:bg-gray-700 transition-colors">Share on X/Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fcodesprintpro.com%2Fblog%2Fkubernetes-production-best-practices%2F" target="_blank" rel="noopener noreferrer" class="text-xs bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">Share on LinkedIn</a></div></div></div><aside class="hidden lg:block w-64 flex-shrink-0"><nav class="hidden lg:block sticky top-24"><h4 class="text-xs font-semibold uppercase tracking-widest text-gray-400 mb-3">On This Page</h4><ul class="space-y-1"><li class=""><a href="#resource-requests-and-limits-the-foundation" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Resource Requests and Limits: The Foundation</a></li><li class=""><a href="#liveness-vs-readiness-vs-startup-probes" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Liveness vs Readiness vs Startup Probes</a></li><li class=""><a href="#rolling-deployments-without-downtime" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Rolling Deployments Without Downtime</a></li><li class=""><a href="#horizontal-pod-autoscaler-configuration" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Horizontal Pod Autoscaler Configuration</a></li><li class=""><a href="#pod-disruption-budgets" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Pod Disruption Budgets</a></li><li class=""><a href="#configmaps-and-secrets-common-mistakes" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">ConfigMaps and Secrets: Common Mistakes</a></li><li class=""><a href="#resource-quotas-per-namespace" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Resource Quotas Per Namespace</a></li><li class=""><a href="#production-checklist" class="block text-sm py-1 border-l-2 pl-3 transition-all border-transparent text-gray-500 hover:text-gray-900 hover:border-gray-300 ">Production Checklist</a></li></ul></nav></aside></div><div class="mt-16 pt-10 border-t border-gray-100"><h2 class="text-2xl font-bold text-gray-900 mb-6">Related Articles</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-6"><a href="/blog/aws-lambda-production-patterns/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-yellow-100 text-yellow-700">AWS</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">AWS Lambda in Production: Cold Starts, Concurrency, and Cost Optimization</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Lambda&#x27;s value proposition is compelling: run code without managing servers, pay per invocation, scale from zero to 10,000 concurrent executions without configuration. The reality is a set of execution model nuances that‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Jun 28, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->aws</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->lambda</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->serverless</span></div></article></a><a href="/blog/terraform-infrastructure-as-code/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-yellow-100 text-yellow-700">AWS</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Terraform Infrastructure as Code: Production Patterns and Pitfalls</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Terraform is the industry-standard tool for Infrastructure as Code (IaC) ‚Äî defining cloud infrastructure as declarative HCL configuration that can be version-controlled, reviewed, and applied reproducibly. The value prop‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>May 14, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->terraform</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->infrastructure as code</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->aws</span></div></article></a><a href="/blog/cloud-cost-optimization/"><article class="group bg-white rounded-xl border border-gray-200 p-6 h-full cursor-pointer hover:border-blue-300 hover:shadow-lg transition-all"><span class="inline-block text-xs font-semibold px-3 py-1 rounded-full mb-3 bg-yellow-100 text-yellow-700">AWS</span><h3 class="text-lg font-bold text-gray-900 mb-2 group-hover:text-blue-600 transition-colors leading-snug">Cloud Cost Optimization: Engineering Practices That Cut AWS Bills by 50%</h3><p class="text-gray-600 text-sm mb-4 line-clamp-3">Cloud bills scale with usage ‚Äî but they also scale with inattention. Most teams that haven&#x27;t deliberately optimized their AWS spend are 30-50% over what they need to pay for the same workload. The savings come from a pre‚Ä¶</p><div class="flex items-center justify-between text-xs text-gray-400"><span>Apr 29, 2025</span><span class="flex items-center gap-1"><svg class="w-3 h-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>7 min read</span></div><div class="mt-3 flex flex-wrap gap-1"><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->aws</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->cost optimization</span><span class="text-xs text-gray-500 bg-gray-50 px-2 py-0.5 rounded border border-gray-100">#<!-- -->ec2</span></div></article></a></div></div><div class="mt-12 text-center"><a class="inline-flex items-center gap-2 text-blue-600 hover:text-blue-700 font-medium transition-colors" href="/blog/">‚Üê Back to all articles</a></div></div></div><footer class="bg-gray-900 text-white py-12"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="grid grid-cols-1 md:grid-cols-4 gap-8 mb-8"><div class="col-span-1 md:col-span-1"><div><a class="text-xl font-bold block mb-3 text-white hover:text-blue-400 transition-colors" href="/">CodeSprintPro</a></div><p class="text-gray-400 text-sm mb-4 leading-relaxed">Deep-dive technical content on System Design, Java, Databases, AI/ML, and AWS ‚Äî by Sachin Sarawgi.</p><div class="flex space-x-4"><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit GitHub profile"><i class="fab fa-github text-xl"></i></a><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit LinkedIn profile"><i class="fab fa-linkedin text-xl"></i></a><a href="https://medium.com/@codesprintpro" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-white transition-colors" aria-label="Visit Medium profile"><i class="fab fa-medium text-xl"></i></a></div></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Quick Links</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Blog</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#about">About</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#portfolio">Portfolio</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/#contact">Contact</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Categories</h3><ul class="space-y-2"><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">System Design</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Java</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Databases</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AI/ML</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">AWS</a></div></li><li><div><a class="text-gray-400 hover:text-white transition-colors text-sm" href="/blog/">Messaging</a></div></li></ul></div><div><h3 class="text-sm font-semibold uppercase tracking-widest text-gray-400 mb-4">Contact</h3><ul class="space-y-2 text-gray-400 text-sm"><li><a href="mailto:sachinsarawgi201143@gmail.com" class="hover:text-white transition-colors flex items-center gap-2"><i class="fas fa-envelope"></i> Email</a></li><li><a href="https://www.linkedin.com/in/sachin-sarawgi/" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-linkedin"></i> LinkedIn</a></li><li><a href="https://github.com/codesprintpro" target="_blank" rel="noopener noreferrer" class="hover:text-white transition-colors flex items-center gap-2"><i class="fab fa-github"></i> GitHub</a></li></ul></div></div><div class="border-t border-gray-800 pt-6 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">¬© <!-- -->2026<!-- --> CodeSprintPro ¬∑ Sachin Sarawgi. All rights reserved.</p><p class="text-gray-600 text-xs">Built with Next.js ¬∑ TailwindCSS ¬∑ Deployed on GitHub Pages</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Kubernetes in Production: Patterns Every Backend Engineer Must Know","description":"Resource requests and limits, liveness vs readiness probes, rolling deployments, HPA configuration, pod disruption budgets, and the mistakes that cause production outages in Kubernetes.","date":"2025-06-08","category":"AWS","tags":["kubernetes","k8s","devops","containers","deployment","aws","eks"],"featured":false,"affiliateSection":"aws-resources","slug":"kubernetes-production-best-practices","readingTime":"6 min read","excerpt":"Running a container in Kubernetes and running a production workload in Kubernetes are different disciplines. The gap between  and a service that survives node failures, deployment rollouts, and traffic spikes without use‚Ä¶","contentHtml":"\u003cp\u003eRunning a container in Kubernetes and running a production workload in Kubernetes are different disciplines. The gap between \u003ccode\u003ekubectl apply -f deployment.yaml\u003c/code\u003e and a service that survives node failures, deployment rollouts, and traffic spikes without user-visible downtime is filled with configuration that doesn't exist in most tutorials.\u003c/p\u003e\n\u003ch2\u003eResource Requests and Limits: The Foundation\u003c/h2\u003e\n\u003cp\u003eEvery production pod must have resource requests and limits. Without them, Kubernetes cannot make scheduling decisions and nodes become dangerously overloaded.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003eresources:\n  requests:\n    memory: \"512Mi\"    # Scheduler uses this for placement decisions\n    cpu: \"250m\"        # 250 millicores = 25% of one CPU core\n  limits:\n    memory: \"1Gi\"      # Container is OOMKilled if it exceeds this\n    cpu: \"1000m\"       # Container is CPU-throttled (not killed) if it exceeds this\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCPU throttling vs OOM Kill:\u003c/strong\u003e CPU limits throttle ‚Äî the container is slowed but kept running. Memory limits kill ‚Äî the container is OOMKilled and restarted. This distinction matters: a CPU limit that's too low causes latency spikes; a memory limit that's too low causes crashes.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRequests vs Limits ratio:\u003c/strong\u003e Kubernetes allows \"overcommitting\" ‚Äî requesting 500m but limiting at 2000m. This is valid for bursty workloads but creates a risk: if all pods burst simultaneously, the node runs out of resources. For critical services, set requests = limits (Guaranteed QoS class) to prevent eviction.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSetting the right values:\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Check actual usage in production:\nkubectl top pod -l app=api-service --containers\n# Use P95 of observed memory as request, P99 + 20% headroom as limit\n\n# For CPU: set request = P50 usage, limit = 2-4√ó request\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLiveness vs Readiness vs Startup Probes\u003c/h2\u003e\n\u003cp\u003eThese three probes are distinct and frequently misconfigured:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003elivenessProbe:\n  # Is the application alive? If not, restart the container.\n  # Use this ONLY for deadlock detection ‚Äî processes that are running but stuck.\n  httpGet:\n    path: /actuator/health/liveness\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  failureThreshold: 3         # Restart after 3 failures\n  timeoutSeconds: 5\n\nreadinessProbe:\n  # Can the application serve traffic? If not, remove from Service endpoints.\n  # Use this to signal when the app is ready and when it's temporarily busy.\n  httpGet:\n    path: /actuator/health/readiness\n    port: 8080\n  initialDelaySeconds: 10\n  periodSeconds: 5\n  failureThreshold: 3\n  successThreshold: 1\n\nstartupProbe:\n  # Overrides liveness during startup ‚Äî prevents premature restarts for slow-starting apps.\n  # Only needed when app takes \u003e 30s to start.\n  httpGet:\n    path: /actuator/health/liveness\n    port: 8080\n  failureThreshold: 30        # Allow up to 30 √ó 10s = 300s to start\n  periodSeconds: 10\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSpring Boot actuator separation:\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// application.properties\nmanagement.endpoint.health.group.liveness.include=livenessState\nmanagement.endpoint.health.group.readiness.include=readinessState,db,redis\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReadiness probe fails ‚Üí pod removed from load balancer (no new traffic) ‚Üí existing connections drain. This is correct behavior during DB connection issues ‚Äî the pod stays alive but stops receiving traffic.\u003c/p\u003e\n\u003cp\u003eLiveness probe fails ‚Üí pod restarted. \u003cstrong\u003eDo not include DB/external checks in liveness probes.\u003c/strong\u003e If your DB is down and liveness probes fail, Kubernetes restarts all pods. Now you have all pods simultaneously in restart loops. The DB comes back but pods are thrashing. Always keep liveness probes lightweight.\u003c/p\u003e\n\u003ch2\u003eRolling Deployments Without Downtime\u003c/h2\u003e\n\u003cp\u003eDefault rolling update configuration is too aggressive:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003estrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: 1          # Default: 25% ‚Äî create at most 1 extra pod\n    maxUnavailable: 0    # Never have fewer than replicas running\n                         # This ensures zero-downtime: new pod must be Ready before old is terminated\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor a service with 10 replicas:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003emaxUnavailable: 0, maxSurge: 1\u003c/code\u003e ‚Üí 1 new pod created, 1 old pod terminated when new is Ready. Linear, predictable.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emaxUnavailable: 25%, maxSurge: 25%\u003c/code\u003e ‚Üí up to 2 old pods removed before new pods are Ready ‚Üí brief 80% capacity.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eGraceful shutdown:\u003c/strong\u003e When Kubernetes terminates a pod, it sends \u003ccode\u003eSIGTERM\u003c/code\u003e, waits \u003ccode\u003eterminationGracePeriodSeconds\u003c/code\u003e, then sends \u003ccode\u003eSIGKILL\u003c/code\u003e. Your application must handle \u003ccode\u003eSIGTERM\u003c/code\u003e gracefully ‚Äî stop accepting new connections, finish in-flight requests, then exit.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e// Spring Boot graceful shutdown:\n// application.properties:\nserver.shutdown=graceful\nspring.lifecycle.timeout-per-shutdown-phase=30s\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003e# Pod spec:\nterminationGracePeriodSeconds: 60  # Must be \u003e your slowest request timeout\nlifecycle:\n  preStop:\n    exec:\n      command: [\"sh\", \"-c\", \"sleep 5\"]\n      # 5-second sleep before SIGTERM gives the load balancer time to\n      # deregister the pod before it stops accepting connections\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eHorizontal Pod Autoscaler Configuration\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003eapiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-service\n  minReplicas: 3          # Never go below 3 ‚Äî one per AZ for HA\n  maxReplicas: 50\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 60    # Scale at 60%, not 80% ‚Äî headroom for spikes\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 70\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 0      # Scale up immediately\n      policies:\n      - type: Percent\n        value: 100                        # Can double pod count per 15s\n        periodSeconds: 15\n    scaleDown:\n      stabilizationWindowSeconds: 300    # Wait 5 minutes before scaling down\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eThe HPA + JVM problem:\u003c/strong\u003e JVM heap is counted against memory limits. During startup, JVM allocates max heap upfront. If \u003ccode\u003emaxHeap \u003e memory.request\u003c/code\u003e, every new pod immediately looks memory-heavy. HPA sees average memory at 90% and scales up before the JVM has warmed up. Fix: set \u003ccode\u003eXmx\u003c/code\u003e to \u003ccode\u003ememory.limit √ó 0.75\u003c/code\u003e, and set \u003ccode\u003ememory.request = memory.limit\u003c/code\u003e (Guaranteed QoS).\u003c/p\u003e\n\u003ch2\u003ePod Disruption Budgets\u003c/h2\u003e\n\u003cp\u003ePDBs prevent Kubernetes from simultaneously evicting too many pods during node drains or cluster upgrades:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003eapiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: api-service-pdb\nspec:\n  minAvailable: 2     # Always keep at least 2 pods running\n  # OR:\n  maxUnavailable: 1   # Never disrupt more than 1 pod at a time\n  selector:\n    matchLabels:\n      app: api-service\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWithout a PDB, \u003ccode\u003ekubectl drain node-1\u003c/code\u003e removes all pods on that node simultaneously. With \u003ccode\u003eminAvailable: 2\u003c/code\u003e on a 3-replica deployment, the drain can only proceed one pod at a time ‚Äî safe.\u003c/p\u003e\n\u003ch2\u003eConfigMaps and Secrets: Common Mistakes\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003e# DO: Use envFrom for cleaner pod specs\nenvFrom:\n- configMapRef:\n    name: api-config\n- secretRef:\n    name: api-secrets\n\n# DON'T: Mount secrets as env vars for sensitive data that rotates ‚Äî\n# env vars require pod restart to pick up new values.\n# DO: Mount as files for secrets that rotate:\nvolumeMounts:\n- name: db-credentials\n  mountPath: /etc/credentials\n  readOnly: true\nvolumes:\n- name: db-credentials\n  secret:\n    secretName: db-credentials\n    # Updates to the secret propagate to the file within ~1 minute\n    # No pod restart needed\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eResource Quotas Per Namespace\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003eapiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: production-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"50\"          # Total CPU requests across all pods\n    requests.memory: 100Gi\n    limits.cpu: \"100\"\n    limits.memory: 200Gi\n    pods: \"200\"\n    services: \"20\"\n    persistentvolumeclaims: \"50\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eQuotas prevent a single team's misconfigured deployment from consuming all cluster resources.\u003c/p\u003e\n\u003ch2\u003eProduction Checklist\u003c/h2\u003e\n\u003cp\u003eBefore any service goes to production on Kubernetes:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e‚ñ° Resource requests AND limits set on all containers\n‚ñ° Liveness probe (lightweight, no external deps)\n‚ñ° Readiness probe (includes DB/cache connectivity)\n‚ñ° Graceful shutdown configured (SIGTERM handler + preStop sleep)\n‚ñ° terminationGracePeriodSeconds \u003e max request duration\n‚ñ° PodDisruptionBudget configured (minAvailable ‚â• 2 for critical services)\n‚ñ° HPA configured with appropriate min/max replicas\n‚ñ° Anti-affinity rules for HA (pods spread across AZs)\n‚ñ° Network policies limiting ingress/egress\n‚ñ° Image tag pinned (never use :latest in production)\n‚ñ° Resource quotas on namespace\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnti-affinity for AZ spread:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003eaffinity:\n  podAntiAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n    - labelSelector:\n        matchLabels:\n          app: api-service\n      topologyKey: topology.kubernetes.io/zone\n      # Required: pods MUST be in different AZs\n      # If only 1 AZ available, pod stays Pending (fail safe)\n\u003c/code\u003e\u003c/pre\u003e\n","tableOfContents":[{"id":"resource-requests-and-limits-the-foundation","text":"Resource Requests and Limits: The Foundation","level":2},{"id":"liveness-vs-readiness-vs-startup-probes","text":"Liveness vs Readiness vs Startup Probes","level":2},{"id":"rolling-deployments-without-downtime","text":"Rolling Deployments Without Downtime","level":2},{"id":"horizontal-pod-autoscaler-configuration","text":"Horizontal Pod Autoscaler Configuration","level":2},{"id":"pod-disruption-budgets","text":"Pod Disruption Budgets","level":2},{"id":"configmaps-and-secrets-common-mistakes","text":"ConfigMaps and Secrets: Common Mistakes","level":2},{"id":"resource-quotas-per-namespace","text":"Resource Quotas Per Namespace","level":2},{"id":"production-checklist","text":"Production Checklist","level":2}]},"relatedPosts":[{"title":"AWS Lambda in Production: Cold Starts, Concurrency, and Cost Optimization","description":"How Lambda execution environments work, cold start mitigation strategies, concurrency limits and throttling, Lambda power tuning, VPC networking costs, and when Lambda is the wrong tool.","date":"2025-06-28","category":"AWS","tags":["aws","lambda","serverless","java","cold start","performance","cost optimization"],"featured":false,"affiliateSection":"aws-resources","slug":"aws-lambda-production-patterns","readingTime":"7 min read","excerpt":"Lambda's value proposition is compelling: run code without managing servers, pay per invocation, scale from zero to 10,000 concurrent executions without configuration. The reality is a set of execution model nuances that‚Ä¶"},{"title":"Terraform Infrastructure as Code: Production Patterns and Pitfalls","description":"Production Terraform: module design, state management with S3 and DynamoDB locking, workspace strategies for multi-environment deployments, sensitive variable handling, drift detection, and the Terraform anti-patterns that cause outages.","date":"2025-05-14","category":"AWS","tags":["terraform","infrastructure as code","aws","devops","s3","modules","ci/cd"],"featured":false,"affiliateSection":"aws-resources","slug":"terraform-infrastructure-as-code","readingTime":"7 min read","excerpt":"Terraform is the industry-standard tool for Infrastructure as Code (IaC) ‚Äî defining cloud infrastructure as declarative HCL configuration that can be version-controlled, reviewed, and applied reproducibly. The value prop‚Ä¶"},{"title":"Cloud Cost Optimization: Engineering Practices That Cut AWS Bills by 50%","description":"Systematic AWS cost reduction: right-sizing EC2 and RDS instances, Savings Plans vs Reserved Instances, S3 lifecycle policies, data transfer cost elimination, EKS node optimization, RDS read replicas vs caching, and the observability stack for cost monitoring.","date":"2025-04-29","category":"AWS","tags":["aws","cost optimization","ec2","rds","s3","eks","savings plans","finops"],"featured":false,"affiliateSection":"aws-resources","slug":"cloud-cost-optimization","readingTime":"7 min read","excerpt":"Cloud bills scale with usage ‚Äî but they also scale with inattention. Most teams that haven't deliberately optimized their AWS spend are 30-50% over what they need to pay for the same workload. The savings come from a pre‚Ä¶"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"kubernetes-production-best-practices"},"buildId":"rpFn_jslWZ9qPkJwor7RD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>